[{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/reference/api/","tags":"","title":"API"},{"body":"Overview Developing and deploying an application to Verrazzano consists of:\n Packaging the application as a Docker image. Publishing the application’s Docker image to a container registry. Applying the application’s Verrazzano components to the cluster. Applying the application’s Verrazzano applications to the cluster.  This guide does not provide the full details for the first two steps. An existing example application Docker image has been packaged and published for use.\nVerrazzano supports application definition using Open Application Model (OAM). Verrrazzano applications are composed of components and application configurations. This document demonstrates creating OAM resources that define an application as well as the steps required to deploy those resources.\nWhat you need   About 10 minutes.\n  Access to an existing Kubernetes cluster with Verrazzano installed.\n  Access to the application’s image in GitHub Container Registry.\nConfirm access using this command to pull the example’s Docker image:\n$ docker pull ghcr.io/verrazzano/example-helidon-greet-app-v1:0.1.12-1-20210218160249-d8db8f3   Application development This guide uses an example application which was written with Java and Helidon. For the implementation details, see the Helidon MP tutorial. See the application source code in the Verrazzano examples repository.\nThe example application is a JAX-RS service and implements the following REST endpoints:\n /greet - Returns a default greeting message that is stored in memory. This endpoint accepts the GET HTTP request method. /greet/{name} - Returns a greeting message including the name provided in the path parameter. This endpoint accepts the GET HTTP request method. /greet/greeting - Changes the greeting message to be used in future calls to the other endpoints. This endpoint accepts the PUT HTTP request method and a JSON payload.  The following code shows a portion of the application’s implementation. The Verrazzano examples repository contains the complete implementation. An important detail here is that the application contains a single resource exposed on path /greet.\npackage io.helidon.examples.quickstart.mp; ... @Path(\"/greet\") @RequestScoped public class GreetResource {   @GET  @Produces(MediaType.APPLICATION_JSON)  public JsonObject getDefaultMessage() {  ...  }   @Path(\"/{name}\")  @GET  @Produces(MediaType.APPLICATION_JSON)  public JsonObject getMessage(@PathParam(\"name\") String name) {  ...  }   @Path(\"/greeting\")  @PUT  @Consumes(MediaType.APPLICATION_JSON)  ...  public Response updateGreeting(JsonObject jsonObject) {  ...  }  } A Dockerfile is used to package the completed application JAR file into a Docker image. The following code shows a portion of the Dockerfile. The Verrazzano examples repository contains the complete Dockerfile. Note that the Docker container exposes a single port 8080.\nFROMghcr.io/oracle/oraclelinux:7-slim...CMD java -cp /app/helidon-quickstart-mp.jar:/app/* io.helidon.examples.quickstart.mp.MainEXPOSE8080Application deployment When you deploy applications with Verrazzano, the platform sets up connections, network policies, and ingresses in the service mesh, and wires up a monitoring stack to capture the metrics, logs, and traces. Verrazzano employs OAM Components to define the functional units of a system that are then assembled and configured by defining associated application configurations.\nVerrazzano components A Verrazzano OAM Component is a Kubernetes Custom Resource describing an application’s general composition and environment requirements. The following code shows the component for the example application used in this guide. This resource describes a component which is implemented by a single Docker image containing a Helidon application exposing a single endpoint.\napiVersion:core.oam.dev/v1alpha2kind:Componentmetadata:name:hello-helidon-componentnamespace:hello-helidonspec:workload:apiVersion:oam.verrazzano.io/v1alpha1kind:VerrazzanoHelidonWorkloadmetadata:name:hello-helidon-workloadlabels:app:hello-helidonspec:deploymentTemplate:metadata:name:hello-helidon-deploymentpodSpec:containers:- name:hello-helidon-containerimage:\"ghcr.io/verrazzano/example-helidon-greet-app-v1:0.1.10-3-20201016220428-56fb4d4\"ports:- containerPort:8080name:httpA brief description of each field of the component:\n apiVersion - Version of the component custom resource definition kind - Standard name of the component custom resource definition metadata.name - The name used to create the component’s custom resource metadata.namespace - The namespace used to create this component’s custom resource spec.workload.kind - VerrazzanoHelidonWorkload defines a stateless workload of Kubernetes spec.workload.spec.deploymentTemplate.podSpec.metadata.name - The name used to create the stateless workload of Kubernetes spec.workload.spec.deploymentTemplate.podSpec.containers - The implementation containers spec.workload.spec.deploymentTemplate.podSpec.containers.ports - Ports exposed by the container  Verrazzano application configurations A Verrazzano application configuration is a Kubernetes Custom Resource which provides environment specific customizations. The following code shows the application configuration for the example used in this guide. This resource specifies the deployment of the application to the hello-helidon namespace. Additional runtime features are specified using traits, or runtime overlays that augment the workload. For example, the ingress trait specifies the ingress host and path, while the metrics trait provides the Prometheus scraper used to obtain the application related metrics.\napiVersion:core.oam.dev/v1alpha2kind:ApplicationConfigurationmetadata:name:hello-helidon-appconfnamespace:hello-helidonannotations:version:v1.0.0description:\"Hello Helidon application\"spec:components:- componentName:hello-helidon-componenttraits:- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:MetricsTraitspec:scraper:verrazzano-system/vmi-system-prometheus-0- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:IngressTraitmetadata:name:hello-helidon-ingressspec:rules:- paths:- path:\"/greet\"pathType:PrefixA brief description of each field in the application configuration:\n apiVersion - Version of the ApplicationConfiguration custom resource definition kind - Standard name of the application configuration custom resource definition metadata.name - The name used to create this application configuration resource metadata.namespace - The namespace used for this application configuration custom resource spec.components - Reference to the application’s components leveraged to specify runtime configuration spec.components[].traits - The traits specified for the application’s components  To explore traits, we can examine the fields of an ingress trait:\n apiVersion - Version of the OAM trait custom resource definition kind - IngressTrait is the name of the OAM application ingress trait custom resource definition spec.rules.paths - The context paths for accessing the application  Deploy the application The following steps are required to deploy the example application. Steps similar to the apply steps would be used to deploy any application to Verrazzano.\n  Create a namespace for the example application and add labels identifying the namespace as managed by Verrazzano and enabled for Istio.\n$ kubectl create namespace hello-helidon $ kubectl label namespace hello-helidon verrazzano-managed=true istio-injection=enabled   Apply the application’s component.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/hello-helidon/hello-helidon-comp.yaml This step causes the validation and creation of the Component resource. No other resources or objects are created as a result. Application configurations applied in the future may reference this Component resource.\n  Apply the application configuration.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/hello-helidon/hello-helidon-app.yaml This step causes the validation and creation of the application configuration resource. This operation triggers the activation of a number of Verrazzano operators. These operators create other Kubernetes objects (for example, Deployments, ReplicaSets, Pods, Services, Ingresses) that collectively provide and support the application.\n  Configure the application’s DNS resolution.\nAfter deploying the application, configure DNS to resolve the application’s ingress DNS name to the application’s load balancer IP address. The generated host name is obtained by querying Kubernetes for the gateway:\n$ kubectl get gateways.networking.istio.io hello-helidon-hello-helidon-appconf-gw \\ -n hello-helidon \\ -o jsonpath='{.spec.servers[0].hosts[0]}' The load balancer IP is obtained by querying Kubernetes for the Istio ingress gateway status:\n$ kubectl get service \\ -n istio-system istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}' DNS configuration steps are outside the scope of this guide. For DNS infrastructure that can be configured and used, see the Oracle Cloud Infrastructure DNS documentation. In some small non-production scenarios, DNS configuration using /etc/hosts or an equivalent may be sufficient.\n  Verify the deployment Applying the application configuration initiates the creation of several Kubernetes objects. Actual creation and initialization of these objects occurs asynchronously. The following steps provide commands for determining when these objects are ready for use.\nNote: Many other Kubernetes objects unrelated to the example application may also exist. Those have been omitted from the lists.\n  Verify the Helidon application pod is running.\n$ kubectl get pods -n hello-helidon -l app=hello-helidon # Sample output NAME READY STATUS RESTARTS AGE hello-helidon-deployment-8664954995-wcb9d 2/2 Running 0 5m5s   Verify that the Verrazzano application operator pod is running.\n$ kubectl get pod -n verrazzano-system -l app=verrazzano-application-operator # Sample output NAME READY STATUS RESTARTS AGE verrazzano-application-operator-79849b89ff-lr9w6 1/1 Running 0 13m The namespace verrazzano-system is used by Verrazzano for non-application objects managed by Verrazzano. A single verrazzano-application-operator manages the life cycle of all OAM based applications within the cluster.\n  Verify the Verrazzano monitoring infrastructure is running.\n$ kubectl get pods -n verrazzano-system | grep '^NAME\\|vmi-system' # Sample output NAME READY STATUS RESTARTS AGE vmi-system-es-master-0 2/2 Running 0 47m vmi-system-grafana-799d79648d-wsdp4 2/2 Running 0 47m vmi-system-kiali-574c6dd94d-f49jv 2/2 Running 0 51m vmi-system-kibana-77f8d998f4-zzvqr 2/2 Running 0 47m vmi-system-prometheus-0-7f89d54fbf-brg6x 3/3 Running 0 45m These pods in the verrazzano-system namespace constitute a monitoring stack created by Verrazzano for the deployed applications.\nThe monitoring infrastructure comprises several components:\n vmi-system-es - OpenSearch for log collection vmi-system-grafana - Grafana for metric visualization vms-system-kiali - Kiali for management console of istio service mesh vmi-system-kibana - OpenSearch Dashboards for log visualization vmi-system-prometheus - Prometheus for metric collection     Diagnose failures.\nView the event logs of any pod not entering the Running state within a reasonable length of time, such as five minutes.\n$ kubectl describe pod -n hello-helidon -l app=hello-helidon Use the specific namespace and name for the pod being investigated.\n  Explore the application Follow these steps to explore the application’s functionality. If DNS was not configured, then use the alternative commands.\n  Save the host name and IP address of the load balancer exposing the application’s REST service endpoints for later.\n$ HOST=$(kubectl get gateways.networking.istio.io hello-helidon-hello-helidon-appconf-gw \\ -n hello-helidon \\ -o jsonpath='{.spec.servers[0].hosts[0]}') $ ADDRESS=$(kubectl get service \\ -n istio-system istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') NOTE:\n The value of ADDRESS is used only if DNS has not been configured. The following alternative commands may not work in conjunction with firewalls that validate HTTP Host headers.    Get the default message.\n$ curl -sk \\ -X GET \\ \"https://${HOST}/greet\" # Expected response {\"message\":\"Hello World!\"} If DNS has not been configured, then use this command.\n$ curl -sk \\ -X GET \\ \"https://${HOST}/greet\" \\ --resolve ${HOST}:443:${ADDRESS}   Get a message for Robert.\n$ curl -sk \\ -X GET \\ \"https://${HOST}/greet/Robert\" # Expected response {\"message\":\"Hello Robert!\"} If DNS has not been configured, then use this command.\n$ curl -sk \\ -X GET \"https://${HOST}/greet/Robert\" \\ --resolve ${HOST}:443:${ADDRESS}   Update the default greeting.\n$ curl -sk \\ -X PUT \\ \"https://${HOST}/greet/greeting\" \\ -H 'Content-Type: application/json' \\ -d '{\"greeting\" : \"Greetings\"}' If DNS has not been configured, then use this command.\n$ curl -sk \\ -X PUT \\ \"https://${HOST}/greet/greeting\" \\ -H 'Content-Type: application/json' \\ -d '{\"greeting\" : \"Greetings\"}' \\ --resolve ${HOST}:443:${ADDRESS}   Get the new message for Robert.\n$ curl -sk \\ -X GET \\ \"https://${HOST}/greet/Robert\" # Expected response {\"message\":\"Greetings Robert!\"} If DNS has not been configured, then use this command.\n$ curl -sk \\ -X GET \\ \"https://${HOST}/greet/Robert\" \\ --resolve ${HOST}:443:${ADDRESS}   Access the application’s logs Deployed applications have log collection enabled. These logs are collected using OpenSearch and can be accessed using OpenSearch Dashboards. OpenSearch and OpenSearch Dashboards are examples of infrastructure Verrazzano creates in support of an application as a result of applying an application configuration. For more information on creating an index pattern and visualizing the log data collected in OpenSearch, see OpenSearch Dashboards.\nDetermine the URL to access OpenSearch Dashboards:\n$ OSD_HOST=$(kubectl get ingress \\ -n verrazzano-system vmi-system-kibana \\ -o jsonpath='{.spec.rules[0].host}') $ OSD_URL=\"https://${OSD_HOST}\" $ echo \"${OSD_URL}\" $ open \"${OSD_URL}\" The user name to access OpenSearch Dashboards defaults to verrazzano during the Verrazzano installation.\nDetermine the password to access OpenSearch Dashboards:\n$ echo $(kubectl get secret \\ -n verrazzano-system verrazzano \\ -o jsonpath={.data.password} | base64 \\ --decode) Access the application’s metrics Deployed applications have metric collection enabled. Grafana can be used to access these metrics collected by Prometheus. Prometheus and Grafana are additional components Verrazzano creates as a result of applying an application configuration. For more information on visualizing Prometheus metrics data, see Grafana.\nDetermine the URL to access Grafana:\n$ GRAFANA_HOST=$(kubectl get ingress \\ -n verrazzano-system vmi-system-grafana \\ -o jsonpath='{.spec.rules[0].host}') $ GRAFANA_URL=\"https://${GRAFANA_HOST}\" $ echo \"${GRAFANA_URL}\" $ open \"${GRAFANA_URL}\" The user name to access Grafana is set to the default value verrazzano during the Verrazzano installation.\nDetermine the password to access Grafana:\n$ echo $(kubectl get secret \\ -n verrazzano-system verrazzano \\ -o jsonpath={.data.password} | base64 \\ --decode) Alternatively, metrics can be accessed directly using Prometheus. Determine the URL for this access:\n$ PROMETHEUS_HOST=$(kubectl get ingress \\ -n verrazzano-system vmi-system-prometheus \\ -o jsonpath='{.spec.rules[0].host}') $ PROMETHEUS_URL=\"https://${PROMETHEUS_HOST}\" $ echo \"${PROMETHEUS_URL}\" $ open \"${PROMETHEUS_URL}\" The user name and password for both Prometheus and Grafana are the same.\nRemove the application Run the following commands to delete the application configuration, and optionally the component and namespace.\n  Delete the application configuration.\n$ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/hello-helidon/hello-helidon-app.yaml The deletion of the application configuration will result in the destruction of all application-specific Kubernetes objects.\n  (Optional) Delete the application’s component.\n$ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/hello-helidon/hello-helidon-comp.yaml Note: This step is not required if other application configurations for this component will be applied in the future.\n  (Optional) Delete the namespace.\n$ kubectl delete namespace hello-helidon   ","categories":"","description":"A guide for deploying an example application on Verrazzano","excerpt":"A guide for deploying an example application on Verrazzano","ref":"/docs/guides/app-deployment/application-deployment-guide/","tags":"","title":"Application Deployment Guide"},{"body":"During application deployment, the oam-kubernetes-runtime and verrazzano-application-operator cooperate through the generation and update of Kubernetes resources. The oam-kubernetes-runtime processes the ApplicationConfiguration and Component resources provided by the user and generates workload and Trait resources. The verrazzano-application-operator processes Verrazzano specific workload and Trait resources. These are then used to generate additional child and related resources.\nTroubleshooting application deployments should follow three general steps:\n Review the status of the oam-kubernetes-runtime and verrazzano-application-operator operator pods. Review the logs of the oam-kubernetes-runtime and verrazzano-application-operator operator pods. Review the resources generated by the oam-kubernetes-runtime and the verrazzano-application-operator.  Review oam-kubernetes-runtime operator status For application deployment to succeed, the oam-kubernetes-runtime pod must have a status of Running.\nUse the following command to get the pod status.\n$ kubectl get pods \\ -n verrazzano-system \\ -l app.kubernetes.io/name=oam-kubernetes-runtime If the pod status is not Running, then see the following instructions for reviewing the oam-kubernetes-runtime pod logs.\nReview verrazzano-application-operator operator status For application deployment to succeed, the verrazzano-application-operator pod must have a status of Running.\nUse the following command to get the pod status.\n$ kubectl get pods \\ -n verrazzano-system \\ -l app=verrazzano-application-operator If the pod status is not Running, then see the following instructions for reviewing the verrazzano-application-operator logs.\nReview oam-kubernetes-runtime operator logs Review the oam-kubernetes-runtime pod logs for any indication that pod startup or the generation of workloads or traits has failed.\nUse the following command to get the logs.\n$ kubectl logs \\ -n verrazzano-system \\ -l app.kubernetes.io/name=oam-kubernetes-runtime Review verrazzano-application-operator logs Review the verrazzano-application-operator logs for any indication that pod startup or resource generation has failed.\nUse the following command to get the logs.\n$ kubectl logs \\ -n verrazzano-system \\ -l app=verrazzano-application-operator Review generated workload resources The processing of a Component reference within an ApplicationConfiguration results in the generation of workloads. For example, a referenced Component might result in the generation of a VerrazzanoHelidonWorkload workload resource. In turn, the VerrazzanoHelidonWorkload workload resource will be processed and result in the generation of related Deployment and Service resources.\nIf the expected workload resource, for example VerrazzanoHelidonWorkload, is missing, then review the oam-kubernetes-runtime logs. If the expected related resources, for example Deployment or Service, are missing, then review the verrazzano-application-operator logs.\nThe following commands are examples of checking for the resources related to a VerrazzanoHelidonWorkload deployment.\n$ kubectl get -n hello-helidon verrazzanohelidonworkload hello-helidon-workload $ kubectl get -n hello-helidon deployment hello-helidon-deployment $ kubectl get -n hello-helidon service hello-helidon-deployment Review generated Trait resources The processing of traits embedded with an ApplicationConfiguration results in the generation of Trait resources. For example, an IngressTrait embedded within an ApplicationConfiguration will result in the generation of an IngressTrait resource. In turn, the IngressTrait resource will be processed and result in the generation of related Certificate, Gateway, and VirtualService resources.\nIf the expected Trait resource, for example IngressTrait, is missing, then review the oam-kubernetes-runtime logs. If the expected related resources, for example Certificate, Gateway, and VirtualService, are missing, then review the verrazzano-application-operator logs.\nThe following commands are examples of checking for the resources related to an IngressTrait.\n$ kubectl get -n hello-helidon ingresstrait hello-helidon-ingress $ kubectl get -n istio-system Certificate hello-helidon-hello-helidon-appconf-cert $ kubectl get -n hello-helidon gateway hello-helidon-hello-helidon-appconf-gw $ kubectl get -n hello-helidon virtualservice hello-helidon-ingress-rule-0-vs Check for RBAC privilege issues The use of generic Kubernetes resources as workloads and traits can result in deployment failures if privileges are insufficient. In this case, the oam-kubernetes-runtime logs will contain errors containing the term forbidden.\nThe following command shows how to query for this type of failure message.\n$ kubectl logs \\ -n verrazzano-system \\ -l app.kubernetes.io/name=oam-kubernetes-runtime | grep forbidden Check resource owners Kubernetes maintains the child to parent relationship within metadata fields.\nThe following example returns the parent of the IngressTrait, named hello-helidon-ingress, in the hello-helidon namespace.\n$ kubectl get IngressTrait \\ -n hello-helidon hello-helidon-ingress \\ -o jsonpath='{range .metadata.ownerReferences[*]}{.name}{\"\\n\"}{end}' The results of this command can help identify the lineage of a given resource.\nCheck related resources Some resources also record the related resources affected during their processing. For example, when processed, an IngressTrait will create related Gateway, VirtualService, and Certificate resources.\nThe following command is an example of how to obtain the related resources of an IngressTraits.\n$ kubectl get IngressTrait \\ -n hello-helidon hello-helidon-ingress \\ -o jsonpath='{range .status.resources[*]}{.kind}: {.name}{\"\\n\"}{end}' The results of this command can help identify which other resources, the given resource affected.\n","categories":"","description":"Troubleshoot issues deploying applications on Verrazzano","excerpt":"Troubleshoot issues deploying applications on Verrazzano","ref":"/docs/troubleshooting/troubleshooting-application-deployment/","tags":"","title":"Application Deployment"},{"body":"","categories":"","description":"Learn about Verrazzano","excerpt":"Learn about Verrazzano","ref":"/docs/concepts/","tags":"","title":"Concepts"},{"body":"Verrazzano supports three DNS choices for Verrazzano services and applications:\n Free wildcard DNS services (nip.io and sslip.io) Oracle Cloud Infrastructure DNS managed by Verrazzano Custom (user-managed) DNS  How Verrazzano constructs a DNS domain Regardless of which DNS management you use, the value in the spec.environmentName field in your installation will be prepended to the configured domain in the spec.components.dns section of the custom resource, to form the full DNS domain name used to access Verrazzano endpoints.\nFor example, if spec.environmentName is set to sales and the domain is configured in spec.components.dns as us.example.com, Verrazzano will create sales.us.example.com as the DNS domain for the installation.\n      Verrazzano can be configured to use either the nip.io or sslip.io free wildcard DNS services. When queried with a hostname with an embedded IP address, wildcard DNS services return that IP address.\nFor example, using the nip.io service, the following DNS names all map to the IP address 10.0.0.1:\n10.0.0.1.nip.io app.10.0.0.1.nip.io customer1.app.10.0.0.1.nip.io To configure Verrazzano to use one of these services, set the spec.wildcard.domain field in the Verrazzano custom resource to either nip.io or sslip.io; the default is nip.io.\nFor example, the following configuration uses sslip.io, instead of nip.io, for wildcard DNS with a dev installation profile:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: example-verrazzano spec: profile: dev environmentName: default components: dns: wildcard: domain: sslip.io   Verrazzano can directly manage records in Oracle Oracle Cloud Infrastructure DNS when configured to use the spec.components.dns.oci field. This is achieved through the External DNS Service, which is a component that is conditionally installed when Oracle Cloud Infrastructure DNS is configured for DNS management in Verrazzano.\nPrerequisites The following prerequisites must be met before using Oracle Cloud Infrastructure DNS with Verrazzano:\n  You must have control of a DNS domain.\n  You must have an Oracle Cloud Infrastructure DNS Service Zone that is configured to manage records for that domain. Verrazzano also supports the use of both GLOBAL and PRIVATE Oracle Cloud Infrastructure DNS zones.\nA DNS Service Zone is a distinct portion of a domain namespace. You must ensure that the zone is appropriately associated with a parent domain. For example, an appropriate zone name for parent domain example.com is us.example.com.\nTo create an Oracle Cloud Infrastructure DNS zone using the Oracle Cloud Infrastructure CLI:\n$ oci dns zone create \\ -c \u003ccompartment ocid\u003e \\ --name \u003czone-name-prefix\u003e.example.com \\ --zone-type PRIMARY To create an Oracle Cloud Infrastructure DNS zone using the Oracle Cloud Infrastructure Console, see Managing DNS Service Zones.\n  You must have a valid Oracle Cloud Infrastructure API signing key that can be used to communicate with Oracle Cloud Infrastructure DNS in your tenancy.\nFor example, you can create an API signing key using the Oracle Cloud Infrastructure CLI:\n $ oci setup keys --key-name myapikey Enter a passphrase for your private key (empty for no passphrase): Public key written to: /Users/jdoe/.oci/myapikey_public.pem Private key written to: /Users/jdoe/.oci/myapikey.pem Public key fingerprint: 39:08:44:69:9f:f5:73:86:7a:46:d8:ad:34:4f:95:29 If you haven't already uploaded your API signing public key through the console, follow the instructions on the page linked below in the section 'How to upload the public key': https://docs.cloud.oracle.com/Content/API/Concepts/apisigningkey.htm#How2 After the key pair has been created, you must upload the public key to your account in your Oracle Cloud Infrastructure tenancy. For details, see the Oracle Cloud Infrastructure documentation, Required Keys and OCIDs.\n  Create an Oracle Cloud Infrastructure API secret in the target cluster To communicate with Oracle Cloud Infrastructure DNS to manage DNS records, Verrazzano needs to be made aware of the necessary API credentials.\nA generic Kubernetes secret must be created in the cluster’s verrazzano-install namespace with the required credentials. That secret must then be referenced by the custom resource that is used to install Verrazzano.\nAfter you have an Oracle Cloud Infrastructure API key ready for use, create a YAML file, oci.yaml, with the API credentials in the form:\nauth: region: \u003coci-region\u003e tenancy: \u003coci-tenancy-ocid\u003e user: \u003coci-user-ocid\u003e key: | \u003coci-api-private-key-file-contents\u003e fingerprint: \u003coci-api-private-key-fingerprint\u003e This information typically can be found in your Oracle Cloud Infrastructure CLI config file or in the Oracle Cloud Infrastructure Console. The \u003coci-api-private-key-file-contents\u003e contents are the PEM-encoded contents of the key_file value within the Oracle Cloud Infrastructure CLI configuration profile.\nFor example, your oci.yaml file will look similar to the following:\nauth: region: us-ashburn-1 tenancy: ocid1.tenancy.oc1..... user: ocid1.user.oc1..... key: | -----BEGIN RSA PRIVATE KEY----- ... -----END RSA PRIVATE KEY----- fingerprint: 12:d3:4c:gh:fd:9e:27:g8:b9:0d:9f:00:22:33:c3:gg Verrazzano also supports the use of instance principals to communicate with Oracle Cloud Infrastructure in order to create or update Oracle Cloud Infrastructure DNS records. Instance principal requires some prerequisites that can be found here.\nWhen using instance principals, your oci.yaml file will look as follows:\nauth: authtype: instance_principal Then, you can create a generic Kubernetes secret in the cluster’s verrazzano-install namespace using kubectl.\n$ kubectl create secret generic -n verrazzano-install \u003csecret-name\u003e --from-file=\u003cpath-to-oci-yaml-file\u003e For example, to create a secret named oci from a file oci.yaml, do the following:\n$ kubectl create secret generic -n verrazzano-install oci --from-file=oci.yaml This secret will later be referenced from the Verrazzano custom resource used during installation.\nUse a Verrazzano helper script to create an Oracle Cloud Infrastructure secret Verrazzano also provides a helper script to create the necessary Kubernetes secret based on your Oracle Cloud Infrastructure CLI config file, assuming that you have the Oracle Cloud Infrastructure CLI installed and a valid Oracle Cloud Infrastructure CLI profile with the required API key information. The script create_oci_config_secret.sh reads your Oracle Cloud Infrastructure CLI configuration file to create the secret.\nFirst, download the create_oci_config_secret.sh script:\n$ curl \\ -o ./create_oci_config_secret.sh \\ https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/platform-operator/scripts/install/create_oci_config_secret.sh Next, set your KUBECONFIG environment variable to point to your cluster and run create_oci_config_secret.sh -h to display the script options:\n$ chmod +x create_oci_config_secret.sh $ export KUBECONFIG=\u003ckubeconfig-file\u003e $ ./create_oci_config_secret.sh -h usage: ./create_oci_config_secret.sh [-o oci_config_file] [-s config_file_section] -o oci_config_file The full path to the Oracle Cloud Infrastructure configuration file (default ~/.oci/config) -s config_file_section The properties section within the Oracle Cloud Infrastructure configuration file. Default is DEFAULT -k secret_name The secret name containing the Oracle Cloud Infrastructure configuration. Default is oci -c context_name The kubectl context to use -a auth_type The auth_type to be used to access Oracle Cloud Infrastructure. Valid values are user_principal/instance_principal. Default is user_principal. -h Help For example, to have the script create the YAML file using your [DEFAULT] Oracle Cloud Infrastructure CLI profile and then create a Kubernetes secret named oci, you can run the script with no arguments, as follows:\n$ ./create_oci_config_secret.sh secret/oci created The following example creates a secret myoci using an Oracle Cloud Infrastructure CLI profile named [dev]:\n$ ./create_oci_config_secret.sh -s dev -k myoci secret/myoci created When using instance principals all other parameters will be ignored automatically. The following example creates a secret myoci using Oracle Cloud Infrastructure instance principal:\n$ ./create_oci_config_secret.sh -a instance_principal secret/myoci created Installation After the Oracle Cloud Infrastructure API secret is created, create a Verrazzano custom resource for the installation that is configured to use Oracle Cloud Infrastructure DNS, and reference the secret you created.\nAs a starting point, download the sample Verrazzano custom resource install-oci.yaml file for Oracle Cloud Infrastructure DNS:\n$ curl \\ -o ./install-oci.yaml \\ https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/platform-operator/config/samples/install-oci.yaml Edit the install-oci.yaml file to provide values for the following configuration settings in the custom resource spec:\n spec.environmentName spec.components.dns.oci.ociConfigSecret spec.components.dns.oci.dnsZoneCompartmentOCID spec.components.dns.oci.dnsZoneOCID spec.components.dns.oci.dnsZoneName spec.components.dns.oci.dnsScope  The field spec.components.dns.oci.ociConfigSecret should reference the secret created earlier. For details on the Oracle Cloud Infrastructure DNS configuration settings, see spec.components.dns.oci.\nFor example, a custom resource for a prod installation profile using Oracle Cloud Infrastructure DNS might look as follows, yielding a domain of myenv.example.com (Oracle Cloud Infrastructure identifiers redacted):\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: example-verrazzano spec: profile: prod environmentName: myenv components: dns: oci: ociConfigSecret: oci dnsZoneCompartmentOCID: ocid1.compartment.oc1..compartment-ocid dnsZoneOCID: ocid1.dns-zone.oc1..zone-ocid dnsZoneName: example.com If using a private DNS zone, then the same prod installation profile using Oracle Cloud Infrastructure DNS will look as follows:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: my-verrazzano spec: profile: prod environmentName: myenv components: dns: oci: ociConfigSecret: oci dnsZoneCompartmentOCID: ocid1.compartment.oc1..compartment-ocid dnsZoneOCID: ocid1.dns-zone.oc1..zone-ocid dnsZoneName: example.com dnsScope: PRIVATE After the custom resource is ready, apply it using kubectl apply -f \u003cpath-to-custom-resource-file\u003e.\n You can specify your own externally managed, custom DNS domain. In this scenario, you manage your own DNS domain and all DNS records in that domain.\nAn externally managed DNS domain is specified in the spec.components.dns.external.suffix field of the Verrazzano custom resource.\nWhen using an externally managed DNS domain, you are responsible for:\n Configuring A records for Verrazzano ingress points (load balancers) Configuring CNAME records for hostnames in the domain that point to the A records, as needed  The Verrazzano installer searches the DNS zone you provide for two specific A records.\nThese are used to configure the cluster and should refer to external addresses of the load balancers provisioned by the user.\nThe A records need to be created manually.\n   Record Use     ingress-mgmt Set as the .spec.externalIPs value of the ingress-controller-nginx-ingress-controller service.   ingress-verrazzano Set as the .spec.externalIPs value of the istio-ingressgateway service.    For example, if spec.environmentName is set to myenv, and spec.components.dns.external.suffix is set to example.com, the A records would need to be set up as follows:\n198.51.100.10 A ingress-mgmt.myenv.example.com. 203.0.113.10 A ingress-verrazzano.myenv.example.com. This example assumes that load balancers exist for ingress-mgmt on 198.51.100.10 and for ingress-verrazzano on 203.0.113.10.\nFor a more complete example, see the documentation for setting up Verrazzano on the Oracle Cloud Native Environment Platform.\n  ","categories":"","description":"Customize DNS configurations for Verrazzano system and application endpoints","excerpt":"Customize DNS configurations for Verrazzano system and application endpoints","ref":"/docs/setup/customizing/dns/","tags":"","title":"Customize DNS"},{"body":"The Hello World Helidon example is a Helidon-based service that returns a “Hello World” response when invoked. The example application is specified using Open Application Model (OAM) component and application configuration YAML files, and then deployed by applying those files. This example shows how to deploy the Hello World Helidon application in a multicluster environment.\nBefore you begin Create a multicluster Verrazzano installation with one admin and one managed cluster, and register the managed cluster, by following the instructions here.\nSet up the following environment variables to point to the kubeconfig for the admin and managed clusters.\n$ export KUBECONFIG_ADMIN=/path/to/your/adminclusterkubeconfig $ export KUBECONFIG_MANAGED1=/path/to/your/managedclusterkubeconfig NOTE: The Hello World Helidon application deployment files are contained in the Verrazzano project located at \u003cVERRAZZANO_HOME\u003e/examples/multicluster/hello-helidon, where \u003cVERRAZZANO_HOME\u003e is the root of the Verrazzano project.\nCreate the application namespace Apply the VerrazzanoProject resource on the admin cluster that defines the namespace for the application. The namespaces defined in the VerrazzanoProject resource will be created on the admin cluster and all the managed clusters.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN apply \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/hello-helidon/verrazzano-project.yaml Deploy the Hello World Helidon application   Apply the hello-helidon multicluster application configuration resource to deploy the application. The multicluster resource is an envelope that contains an OAM resource and a list of clusters to which to deploy.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN apply \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/hello-helidon/hello-helidon-comp.yaml $ kubectl --kubeconfig $KUBECONFIG_ADMIN apply \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/hello-helidon/mc-hello-helidon-app.yaml   Wait for the application to be ready on the managed cluster.\n$ kubectl --kubeconfig $KUBECONFIG_MANAGED1 wait \\ --for=condition=Ready pods \\ --all -n hello-helidon \\ --timeout=300s   Explore the example application Follow the instructions for exploring the Hello World Helidon application in a single cluster use case. Use the managed cluster kubeconfig for testing the example application.\nTroubleshooting Follow the instructions for troubleshooting the Hello World Helidon application in a single cluster use case. Use the managed cluster kubeconfig for troubleshooting the example application.\n  Verify that the application namespace exists on the managed cluster.\n$ kubectl --kubeconfig $KUBECONFIG_MANAGED1 get namespace hello-helidon   Verify that the multicluster resource for the application exists.\n$ kubectl --kubeconfig $KUBECONFIG_MANAGED1 get MultiClusterApplicationConfiguration -n hello-helidon   Locating the application on a different cluster By default, the application is located on the managed cluster called managed1. You can change the application’s location to be on a different cluster, which can be the admin cluster or a different managed cluster. In this example, you change the placement of the application to the admin cluster by patching the multicluster resources.\n  To change the application’s location to the admin cluster, specify the change placement patch file.\n# To change the placement to the admin cluster $ export CHANGE_PLACEMENT_PATCH_FILE=\"https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/hello-helidon/patch-change-placement-to-admin.yaml\" This environment variable is used in subsequent steps.\n  To change their placement, patch the hello-helidon multicluster application configuration.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN patch mcappconf hello-helidon-appconf \\ -n hello-helidon \\ --type merge \\ --patch \"$(curl -s $CHANGE_PLACEMENT_PATCH_FILE)\" # Expected response multiclusterapplicationconfiguration.clusters.verrazzano.io/hello-helidon-appconf patched   To verify that its placement has changed, view the multicluster resource.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN get mcappconf hello-helidon-appconf \\ -n hello-helidon \\ -o jsonpath='{.spec.placement}';echo # Expected response {\"clusters\":[{\"name\":\"local\"}]} The cluster name, local, indicates placement in the admin cluster.\n  To change its placement, patch the VerrazzanoProject.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN patch vp hello-helidon \\ -n verrazzano-mc \\ --type merge \\ --patch \"$(curl -s $CHANGE_PLACEMENT_PATCH_FILE)\" # Expected response verrazzanoproject.clusters.verrazzano.io/hello-helidon patched   Wait for the application to be ready on the admin cluster.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN wait \\ --for=condition=Ready pods \\ --all -n hello-helidon \\ --timeout=300s Note: If you are returning the application to the managed cluster, then instead, wait for the application to be ready on the managed cluster.\n$ kubectl --kubeconfig $KUBECONFIG_MANAGED1 wait \\ --for=condition=Ready pods \\ --all -n hello-helidon \\ --timeout=300s   Now, you can test the example application running in its new location.\nTo return the application to the managed cluster named managed1, set the value of the CHANGE_PLACEMENT_PATCH_FILE environment variable to the patch file provided for that purpose, then repeat the previous numbered steps.\n# To change the placement back to the managed cluster named managed1 $ export CHANGE_PLACEMENT_PATCH_FILE=\"https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/hello-helidon/patch-return-placement-to-managed1.yaml\"   Undeploy the Hello World Helidon application Regardless of its location, to undeploy the application, delete the application resources and the project from the admin cluster. Undeploy affects all clusters in which the application is located.\n  To undeploy the application, delete the Hello World Helidon OAM resources.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN delete \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/hello-helidon/mc-hello-helidon-app.yaml $ kubectl --kubeconfig $KUBECONFIG_ADMIN delete \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/hello-helidon/hello-helidon-comp.yaml   Delete the project.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN delete \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/hello-helidon/verrazzano-project.yaml   Delete the namespace hello-helidon after the application pod is terminated.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN delete namespace hello-helidon $ kubectl --kubeconfig $KUBECONFIG_MANAGED1 delete namespace hello-helidon   ","categories":"","description":"Hello World Helidon example deployed to a multicluster environment. The example also demonstrates how to change the placement of the application to a different cluster.","excerpt":"Hello World Helidon example deployed to a multicluster environment. The example also demonstrates how to change the placement of the application to a different cluster.","ref":"/docs/samples/multicluster/hello-helidon/","tags":"","title":"Multicluster Hello World Helidon"},{"body":"The following instructions show you how to install Verrazzano in a single Kubernetes cluster.\nPrerequisites  Find the Verrazzano prerequisite requirements here. Review the list of the software versions supported and installed by Verrazzano.  Prepare for the install Before installing Verrazzano, see instructions on preparing Kubernetes platforms.\nNOTE: Verrazzano can create network policies that can be used to limit the ports and protocols that pods use for network communication. Network policies provide additional security but they are enforced only if you install a Kubernetes Container Network Interface (CNI) plug-in that enforces them, such as Calico. For instructions on how to install a CNI plug-in, see the documentation for your Kubernetes cluster.\nInstall the Verrazzano platform operator Verrazzano provides a platform operator to manage the life cycle of Verrazzano installations. Using the Verrazzano custom resource, you can install, uninstall, and upgrade Verrazzano installations.\nTo install the Verrazzano platform operator:\n  Deploy the Verrazzano platform operator.\n$ kubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/v1.2.2/operator.yaml   Wait for the deployment to complete.\n$ kubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator # Expected response deployment \"verrazzano-platform-operator\" successfully rolled out   Confirm that the operator pod is correctly defined and running.\n$ kubectl -n verrazzano-install get pods # Sample output NAME READY STATUS RESTARTS AGE verrazzano-platform-operator-59d5c585fd-lwhsx 1/1 Running 0 114s   Perform the install Verrazzano supports the following installation profiles: development (dev), production (prod), and managed cluster (managed-cluster). For more information on profiles, see Installation Profiles.\nThis page shows how to create a basic Verrazzano installation using:\n The development (dev) installation profile Wildcard-DNS, where DNS is provided by nip.io (the default)  NOTE Because the dev profile installs self-signed certificates, when installing Verrazzano on macOS, you might see: Your connection is not private. For a workaround, see this FAQ.  For a complete description of Verrazzano configuration options, see the Verrazzano Custom Resource Definition.\nTo use other DNS options, see the Customzing DNS page for more details.\nInstall Verrazzano To create a Verrazzano installation as described in the previous section, run the following commands:\n$ kubectl apply -f - \u003c\u003cEOF apiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: example-verrazzano spec: profile: ${VZ_PROFILE:-dev} EOF $ kubectl wait \\ --timeout=20m \\ --for=condition=InstallComplete verrazzano/example-verrazzano To use a different profile with the above example, set the VZ_PROFILE environment variable to the name of the profile you want to install.\nIf an error occurs, check the log output of the installation. You can view the logs with the following command:\n$ kubectl logs -n verrazzano-install \\ -f $(kubectl get pod \\ -n verrazzano-install \\ -l app=verrazzano-platform-operator \\ -o jsonpath=\"{.items[0].metadata.name}\") | grep '^{.*}$' \\ | jq -r '.\"@timestamp\" as $timestamp | \"\\($timestamp) \\(.level) \\(.message)\"' For more help troubleshooting the installation, see Analysis Advice.\nAfter the installation is complete, you can use the console URLs. For more information on how to access the Verrazzano consoles, see Access Verrazzano.\nVerify the install Verrazzano installs multiple objects in multiple namespaces. In the verrazzano-system namespaces, all the pods in the Running state, does not guarantee, but likely indicates that Verrazzano is up and running.\n$ kubectl get pods -n verrazzano-system # Sample output coherence-operator-dcfb446df-24djp 1/1 Running 1 49m fluentd-h65xf 2/2 Running 1 45m oam-kubernetes-runtime-6645df49cd-6q96c 1/1 Running 0 49m verrazzano-application-operator-85ffd7f77b-rhwk7 1/1 Running 0 48m verrazzano-authproxy-58db5b9484-nhnql 2/2 Running 0 45m verrazzano-console-5dbdc579bd-hm4rh 2/2 Running 0 45m verrazzano-monitoring-operator-599654889d-lbb4z 1/1 Running 0 45m verrazzano-operator-7b6fd64dd5-8j9h8 1/1 Running 0 45m vmi-system-es-master-0 2/2 Running 0 45m vmi-system-grafana-5558d65b46-pxg78 2/2 Running 0 45m vmi-system-kiali-5949966fb8-465s8 2/2 Running 0 48m vmi-system-kibana-86b894d8f6-q4vb5 2/2 Running 0 45m vmi-system-prometheus-0-859fcd87dc-m5ws9 3/3 Running 0 44m weblogic-operator-646756c75c-hgz6j 2/2 Running 0 49m (Optional) Run the example applications Example applications are located here.\nTo get the consoles URLs and credentials, see Access Verrazzano. ","categories":"","description":"How to install Verrazzano","excerpt":"How to install Verrazzano","ref":"/docs/setup/install/installation/","tags":"","title":"Install Guide"},{"body":"The Verrazzano logging stack consists of Fluentd, OpenSearch, and OpenSearch Dashboards components.\n Fluentd: a log aggregator that collects, processes, and formats logs from Kubernetes clusters. OpenSearch: a scalable search and analytics engine for storing Kubernetes logs. OpenSearch Dashboards: a visualization layer that provides a user interface to query and visualize collected logs.  As shown in the following diagram, logs written to stdout by a container running on Kubernetes are picked up by the kubelet service running on that node and written to /var/log/containers.\nFluentd sidecar For components with multiple log streams or that cannot log to stdout, Verrazzano deploys a Fluentd sidecar which parses and translates the log stream. The resulting log is sent to stdout of the sidecar container and then written to /var/log/containers by the kubelet service.\nFor example, in a WebLogic deployment, AdminServer.log is consumed, translated, and written to stdout by the Fluentd sidecar. You can view these logs using kubectl on the container named fluentd-stdout-sidecar.\n$ kubectl logs tododomain-adminserver \\ -n todo-list \\ -c fluentd-stdout-sidecar The Verrazzano Fluentd Docker image comes with these plug-ins:\n fluent-plugin-concat fluent-plugin-dedot_filter fluent-plugin-detect-exceptions  fluent-plugin-elasticsearch fluent-plugin-grok-parser fluent-plugin-json-in-json-2 fluent-plugin-kubernetes_metadata_filter fluent-plugin-multi-format-parser fluent-plugin-parser-cri fluent-plugin-prometheus fluent-plugin-record-modifier fluent-plugin-rewrite-tag-filter fluent-plugin-systemd fluent-plugin-oci-logging  The Verrazzano Fluentd Docker image also has two local default plug-ins, kubernetes_parser and kubernetes_multiline_parser. These plug-ins help to parse Kubernetes management log files.\nHere are example use cases for these plug-ins:\n# ---- fluentd.conf ---- # kubernetes parser \u003csource\u003e @type tail path ./kubelet.log read_from_head yes tag kubelet \u003cparse\u003e @type multiline_kubernetes \u003c/parse\u003e \u003c/source\u003e # kubernetes multi-line parser \u003csource\u003e @type tail path ./kubelet.log read_from_head yes tag kubelet \u003cparse\u003e @type multiline_kubernetes \u003c/parse\u003e \u003c/source\u003e # ---- EOF ---- For more details, see the Fluentd plugins folder.\nFluentd DaemonSet Verrazzano deploys a Fluentd DaemonSet which runs one Fluentd replica per node in the verrazzano-system namespace. Each instance pulls logs from the node’s /var/log/containers directory and writes them to the target OpenSearch index. The index name is based on the namespace associated with the record, using this format: verrazzano-namespace-\u003crecord namespace\u003e.\nFor example, vmi-system-kibana logs written to /var/log/containers will be pulled by Fluentd and written to OpenSearch. The index used is named verrazzano-namespace-verrazzano-system because the VMI runs in the verrazzano-system namespace.\nThe same approach is used for both system and application logs.\nOpenSearch Verrazzano creates an OpenSearch deployment as the store and search engine for the logs processed by Fluentd. Records written by Fluentd can be queried using the OpenSearch REST API.\nFor example, you can use curl to get all of the OpenSearch indexes. First, you must get the password for the verrazzano user and the host for the VMI OpenSearch.\n$ PASS=$(kubectl get secret \\ --namespace verrazzano-system verrazzano \\ -o jsonpath={.data.password} | base64 \\ --decode; echo) $ HOST=$(kubectl get ingress \\ -n verrazzano-system vmi-system-es-ingest \\ -o jsonpath={.spec.rules[0].host}) $ curl -ik \\ --user verrazzano:$PASS https://$HOST/_cat/indices To see all of the records for a specific index, do the following:\n$ INDEX=verrazzano-namespace-todo-list $ curl -ik \\ --user verrazzano:$PASS https://$HOST/$INDEX/_doc/_search?q=message:* Verrazzano provides support for Installation Profiles. The production profile (prod), which is the default, provides a 3-node OpenSearch and persistent storage for the Verrazzano Monitoring Instance (VMI). The development profile (dev) provides a single node OpenSearch and no persistent storage for the VMI. The managed-cluster profile does not install OpenSearch or OpenSearch Dashboards in the local cluster; all logs are forwarded to the admin cluster’s OpenSearch instance.\nIf you want the logs sent to an external OpenSearch, instead of the default VMI OpenSearch, specify elasticsearchURL and elasticsearchSecret in the Fluentd Component configuration in your Verrazzano custom resource.\nThe following is an example of a Verrazzano custom resource to send the logs to the OpenSearch endpoint https://external-es.default.172.18.0.231.nip.io.\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: default spec: components: fluentd: elasticsearchURL: https://external-es.default.172.18.0.231.nip.io elasticsearchSecret: external-es-secret OpenSearch Dashboards OpenSearch Dashboards is a visualization dashboard for the content indexed on an OpenSearch cluster. Verrazzano creates a OpenSearch Dashboards deployment to provide a user interface for querying and visualizing the log data collected in OpenSearch.\nTo access the OpenSearch Dashboards console, read Access Verrazzano.\nTo see the records of an OpenSearch index through OpenSearch Dashboards, create an index pattern to filter for records under the desired index.\nFor example, to see the log records of a WebLogic application deployed to the todo-list namespace, create an index pattern of verrazzano-namespace-todo-*.\nLog rotation We recommend configuring log rotation for OpenSearch using a periodic job to purge or snapshot old records. A basic implementation of log rotation is provided in the following example, implemented using a Kubernetes CronJob. To install the log rotation example on your cluster, save the snippet into a file and make the following modifications:\n Substitue the value of OPENSEARCH_HOST with your specific OpenSearch HTTPS endpoint. By default, the CronJob deletes the last 7 days of data. You may customize this by modifying the query in the ConfigMap.  apiVersion: batch/v1beta1 kind: CronJob metadata: name: log-rotate namespace: verrazzano-system labels: app: log-rotate spec: # Rotate logs every day at midnight schedule: \"0 0 * * *\" jobTemplate: spec: template: metadata: labels: app: log-rotate annotations: sidecar.istio.io/inject: \"false\" spec: containers: - name: log-rotate args: - /bin/sh - -c - /opt/script/rotate env: - name: \"OPENSEARCH_HOST\" value: \"https://elasticsearch.vmi.system.default.172.18.0.151.nip.io\" - name: OPENSEARCH_USER valueFrom: secretKeyRef: key: username name: verrazzano optional: true - name: OPENSEARCH_PASSWORD valueFrom: secretKeyRef: key: password name: verrazzano optional: true image: ghcr.io/oracle/oraclelinux:7-slim imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /opt/script name: log-rotate restartPolicy: OnFailure volumes: - configMap: defaultMode: 0777 name: log-rotate name: log-rotate --- apiVersion: v1 kind: ConfigMap metadata: name: log-rotate namespace: verrazzano-system labels: app: log-rotate data: rotate: | #!/bin/bash curl -v --silent -k -u \"$OPENSEARCH_USER:$OPENSEARCH_PASSWORD\" -X POST \"$OPENSEARCH_HOST/verrazzano-*/_delete_by_query\" -H 'Content-Type: application/json' -d' { \"query\": { \"bool\": { \"filter\": [ { \"range\": { \"@timestamp\": { \"lt\": \"now-7d\" } } } ] } } } ' ","categories":"","description":"Learn about Verrazzano log collection and viewing","excerpt":"Learn about Verrazzano log collection and viewing","ref":"/docs/monitoring/logs/","tags":"","title":"Logging"},{"body":"Verrazzano may be installed in a multicluster environment, consisting of an admin cluster and optionally, one or more managed clusters.\n The admin cluster is a central point from which Verrazzano applications in managed clusters can be deployed and monitored. Managed clusters are registered with an admin cluster. Verrazzano multicluster resources are used to target applications to any cluster in a multicluster Verrazzano environment.  The following diagram shows a high-level overview of how multicluster Verrazzano works. For a more detailed view, see the diagram here.\nAdmin cluster A Verrazzano admin cluster is a central management point for:\n Deploying and undeploying applications to the managed clusters registered with the admin cluster. Viewing logs and metrics for both Verrazzano Components and applications that reside in the managed clusters.  You may register one or more managed clusters with the admin cluster by creating a VerrazzanoManagedCluster resource in the verrazzano-mc namespace of an admin cluster.\nNote: The admin cluster has a fully functional Verrazzano installation. You can locate applications on the admin cluster as well as on managed clusters.\nManaged clusters A Verrazzano managed cluster has a minimal footprint of Verrazzano, installed using the managed-cluster installation profile. A managed cluster has the following additional characteristics:\n It is registered with an admin cluster with a unique name. Logs for Verrazzano system Components and Verrazzano multicluster applications are sent to OpenSearch running on the admin cluster, and are viewable from that cluster. A Verrazzano multicluster Kubernetes resource, created on the admin cluster, will be retrieved and deployed to a managed cluster if all of the following are true:  The resource is in a namespace governed by a VerrazzanoProject. The VerrazzanoProject has a placement value that includes this managed cluster. The resource itself has a placement value that includes this managed cluster.    Verrazzano multicluster resources Verrazzano includes several multicluster resource definitions for resources that may be targeted for placement in one or more clusters: MultiClusterApplicationConfiguration, MultiClusterComponent, MultiClusterConfigMap, and MultiClusterSecret.\n Each multicluster resource type serves as a wrapper for an underlying resource type. A multicluster resource additionally allows the placement of the underlying resource to be specified as a list of names of the clusters in which the resource must be placed. Multicluster resources are created in the admin cluster, in a namespace that is part of a VerrazzanoProject, and targeted for placement in either the local admin cluster or a remote managed cluster. A multicluster resource is said to be part of a VerrazzanoProject if it is in a namespace that is governed by that VerrazzanoProject.  Managed cluster registration A managed cluster may be registered with an admin cluster using a two-step process:\nStep 1: Create a VerrazzanoManagedCluster resource in the verrazzano-mc namespace of the admin cluster.\nStep 2: Retrieve the Kubernetes manifest file generated in the VerrazzanoManagedCluster resource and apply it on the managed cluster to complete the registration.\nWhen a managed cluster is registered, the following will happen:\n After both steps of the registration are complete, the managed cluster begins polling the admin cluster for VerrazzanoProject resources and multicluster resources, which specify a placement in this managed cluster.  Any VerrazzanoProject resources placed in this managed cluster are retrieved, and the corresponding namespaces and security permissions (RoleBindings) are created in the managed cluster. Any multicluster resources that are placed in this managed cluster, and are in a VerrazzanoProject that is also placed in this managed cluster, are retrieved, and created or updated on the managed cluster. The underlying resource represented by the multicluster resource is unwrapped, and created or updated on the managed cluster. The managed cluster namespace of the multicluster resource and its underlying resource matches the admin cluster namespace of the multicluster resource.   When the managed cluster connects to the admin cluster, it updates the VerrazzanoManagedCluster resource for this managed cluster with:  The endpoint URL that the admin cluster should use to scrape Prometheus metrics from the managed cluster. The date and time of the most recent successful connection from the managed cluster to the admin cluster.   For MultiClusterApplicationConfigurations retrieved and unwrapped on a managed cluster, the application logs are sent to OpenSearch on the admin cluster, and may be viewed from the Verrazzano-installed OpenSearch Dashboards UI on the admin cluster. Likewise, application metrics will be scraped by the admin cluster and available from Verrazzano-installed Prometheus on the admin cluster.  Detailed view of multicluster Verrazzano This diagram shows a detailed view of how multicluster Verrazzano works.\nTry out multicluster Verrazzano For more information, see the API Documentation for the resources described here.\nTo try out multicluster Verrazzano, see the Multicluster examples.\n","categories":"","description":"Learn about Verrazzano in a multicluster environment","excerpt":"Learn about Verrazzano in a multicluster environment","ref":"/docs/applications/multicluster/","tags":"","title":"Multicluster"},{"body":"This document describes some common problems you might encounter when using multicluster Verrazzano, and how to troubleshoot them.\nIf you created multicluster resources in the admin cluster, and specified a placement value in a managed cluster, then those resources will get created in that managed cluster. If they do not get created in the managed cluster, then use the following steps to troubleshoot:\n Verify that the managed cluster is registered correctly and can connect to the admin cluster. Verify that the VerrazzanoProject for the resource’s namespace, also has a placement in that managed cluster. Check the multicluster resource’s status field on the admin cluster to know what the status of that resource is on each managed cluster to which it is targeted.  Verify managed cluster registration and connectivity You can verify that a managed cluster was successfully registered with an admin cluster by viewing the corresponding VerrazzanoManagedCluster (VMC) resource on the admin cluster. For example, to verify that a managed cluster named managed1 was successfully registered:\n# on the admin cluster $ kubectl get verrazzanomanagedcluster managed1 \\ -n verrazzano-mc \\ -o yaml Partial sample output from the previous command:\n status: conditions: - lastTransitionTime: \"2021-06-22T21:03:27Z\" message: Ready status: \"True\" type: Ready lastAgentConnectTime: \"2021-06-22T21:06:04Z\" ... other fields ... Check the lastAgentConnectTime in the status of the VMC resource. This is the last time at which the managed cluster connected to the admin cluster. If this value is not present, then the managed cluster named managed1 never successfully connected to the admin cluster. This could be due to several reasons:\n  The managed cluster registration process step of applying the registration YAML on the managed cluster, was not completed. For the complete setup instructions, see here.\n  The managed cluster does not have network connectivity to the admin cluster. The managed cluster will attempt to connect to the admin cluster at regular intervals, and any errors will be reported in the verrazzano-application-operator pod’s log on the managed cluster. View the logs using the following command.\n  # on the managed cluster $ kubectl logs \\ -n verrazzano-system \\ -l app=verrazzano-application-operator If these logs reveal that there is a connectivity issue, check the admin cluster Kubernetes server address that you provided during registration and ensure that it is correct, and that it is reachable from the managed cluster. If it is incorrect, then you will need to repeat the managed cluster registration process described in the setup instructions here.\nVerify VerrazzanoProject placement For Verrazzano to create an application namespace in a managed cluster, that namespace must be part of a VerrazzanoProject that:\n Includes that namespace. Has a placement value that includes that managed cluster.  View the details of the project that corresponds to your application’s namespace. In the example command that follows, the project name is assumed to be myproject. All projects are expected to be created in the verrazzano-mc namespace.\n# on the admin cluster $ kubectl get verrazzanoproject myproject \\ -n verrazzano-mc \\ -o yaml The following partial sample output is for a project that will result in the namespace mynamespace being created on the managed cluster managed1.\nspec: placement: clusters: - name: managed1 template: namespaces: - metadata: name: mynamespace ....other fields.... Check the multicluster resource status On the admin cluster, each multicluster resource’s status field is updated with the status of the underlying resource on each managed cluster in which it is placed.\nThe following example command shows how to view the status of a MultiClusterApplicationConfiguration named myapp, in the namespace mynamespace, that has a placement value that includes the managed cluster managed1\n$ kubectl get multiclusterapplicationconfiguration myapp \\ -n mynamespace \\ -o yaml The status of the underlying resource in each cluster specified in the placement is shown in the following partial sample output:\n status: clusters: - lastUpdateTime: \"2021-06-22T21:05:04Z\" message: OAM Application Configuration created name: managed1 state: Succeeded conditions: - lastTransitionTime: \"2021-06-22T21:03:58Z\" message: OAM Application Configuration created status: \"True\" type: DeployComplete state: Succeeded The status message contains additional information on the operation’s success or failure.\n","categories":"","description":"Troubleshoot issues with multicluster setup and applications","excerpt":"Troubleshoot issues with multicluster setup and applications","ref":"/docs/troubleshooting/troubleshooting-multicluster/","tags":"","title":"Multicluster Verrazzano"},{"body":"This document describes how to recover an OpenSearch cluster’s health after it becomes unhealthy due to unassigned shards or disk pressure.\nIt also describes how to scale up the cluster’s data nodes and increase the size of the volumes. Because the volume size change in the Verrazzano operator also affects the master nodes volume size, you must take additional steps to address the volume resizing of a StatefulSet.\nFirst:\n# Edit the Verrazzano operator $ kubectl -n verrazzano-system edit deploy verrazzano-operator Then, change the following portion by increasing the number of ES_DATA_NODE_REPLICAS to 3, and the ES_DATA_STORAGE to 200Gi:\n- name: ES_DATA_NODE_REPLICAS value: \"3\" - name: ES_DATA_STORAGE value: \"200\" Scaling OpenSearch data nodes Follow this procedure to scale the OpenSearch data nodes.\n Wait for the new data node pod to become ready and then check the health of the cluster: $ kubectl -n verrazzano-system exec -it vmi-system-es-master-0 -- curl http://127.0.0.1:9200/_cat/health $ kubectl -n verrazzano-system exec -it vmi-system-es-master-0 -- curl http://127.0.0.1:9200/_cat/indices  When you have a green state, replace the original data node -0 pods: $ kubectl -n verrazzano-system scale deploy verrazzano-operator --replicas=0 $ kubectl -n verrazzano-system scale deploy verrazzano-monitoring-operator --replicas=0 $ kubectl -n verrazzano-system delete pod/vmi-system-es-data-0-xxxxxxxxx-xxxx pvc/vmi-system-es-data-0 $ kubectl -n verrazzano-system scale deploy verrazzano-operator --replicas=1 $ kubectl -n verrazzano-system scale deploy verrazzano-monitoring-operator --replicas=1  Wait for the new data node pod to become ready and then check the health of the cluster: $ kubectl -n verrazzano-system exec -it vmi-system-es-master-0 -- curl http://127.0.0.1:9200/_cat/health $ kubectl -n verrazzano-system exec -it vmi-system-es-master-0 -- curl http://127.0.0.1:9200/_cat/indices  When you have a green state, replace the original data node -1 pods: $ kubectl -n verrazzano-system scale deploy verrazzano-operator --replicas=0 $ kubectl -n verrazzano-system scale deploy verrazzano-monitoring-operator --replicas=0 $ kubectl -n verrazzano-system delete pod/vmi-system-es-data-1-xxxxxxxxx-xxxx pvc/vmi-system-es-data-1 $ kubectl -n verrazzano-system scale deploy verrazzano-operator --replicas=1 $ kubectl -n verrazzano-system scale deploy verrazzano-monitoring-operator --replicas=1  Wait for the new data node pod to become ready and then check the health of the cluster: $ kubectl -n verrazzano-system exec -it vmi-system-es-master-0 -- curl http://127.0.0.1:9200/_cat/health $ kubectl -n verrazzano-system exec -it vmi-system-es-master-0 -- curl http://127.0.0.1:9200/_cat/indices   You should now have three data nodes that are healthy and at 200GB volumes.\nAddress the master nodes’ StatefulSet Now to address the master nodes. Because you cannot directly change the size of the volume associated with a volume template in a StatefulSet, you must follow this procedure:\n  First:\n$ kubectl -n verrazzano-system scale deploy verrazzano-operator --replicas=0 $ kubectl -n verrazzano-system scale deploy verrazzano-monitoring-operator --replicas=0 $ kubectl -n verrazzano-system get sts vmi-system-es-master -o yaml \u003e vmi-system-es-master.yaml   Edit the file created in the previous command, vmi-system-es-master.yaml\na. Remove the lines starting with:\ncreationTimestamp: generation: resourceVersion: selfLink: uid: status: b. Remove every line below status:\nc. Edit the section to increase the storage to the same value that you used for the Verrazzano operator:\nstorage: 200Gi d. Save that file.\n  The following command will delete the StatefulSet, but allow the associated pods to continue to run.\n$ kubectl -n verrazzano-system delete sts vmi-system-es-master --cascade=orphan   Then run this command to recreate the StatefulSet with the new volume size defined:\n$ kubectl -n verrazzano-system apply -f vmi-system-es-master.yaml   The next steps are to delete the existing master node pods, one at a time, allowing the cluster to become healthy before moving on to the next node:\n$ kubectl -n verrazzano-system delete pod/vmi-system-es-master-0 pvc/elasticsearch-master-vmi-system-es-master-0   Wait for the new master node pod to become ready and then check the health of the cluster:\n$ kubectl -n verrazzano-system exec -it vmi-system-es-master-0 -- curl http://127.0.0.1:9200/_cat/health $ kubectl -n verrazzano-system exec -it vmi-system-es-master-0 -- curl http://127.0.0.1:9200/_cat/indices   When the cluster is healthy, continue to the next master node:\n$ kubectl -n verrazzano-system delete pod/vmi-system-es-master-1 pvc/elasticsearch-master-vmi-system-es-master-1   Wait for the new master node pod to become ready and then check the health of the cluster:\n$ kubectl -n verrazzano-system exec -it vmi-system-es-master-0 -- curl http://127.0.0.1:9200/_cat/health $ kubectl -n verrazzano-system exec -it vmi-system-es-master-0 -- curl http://127.0.0.1:9200/_cat/indices   When the cluster is healthy, continue to the next master node:\n$ kubectl -n verrazzano-system delete pod/vmi-system-es-master-2 pvc/elasticsearch-master-vmi-system-es-master-2   Wait for the new master node pod to become ready and then check the health of the cluster:\n$ kubectl -n verrazzano-system exec -it vmi-system-es-master-0 -- curl http://127.0.0.1:9200/_cat/health $ kubectl -n verrazzano-system exec -it vmi-system-es-master-0 -- curl http://127.0.0.1:9200/_cat/indices   When the cluster is healthy rescale the operators:\n$ kubectl -n verrazzano-system scale deploy verrazzano-operator --replicas=1 $ kubectl -n verrazzano-system scale deploy verrazzano-monitoring-operator --replicas=1   Update the Verrazzano Custom Resource Now you will edit the Verrazzano CR, so that when you upgrade, the above changes will not be overwritten. Follow this procedure:\n Get the namespace and name of the Verrazzano CR: $ kubectl get vz -A  Edit the Verrazzano CR: $ kubectl -n \u003cnamespace\u003e edit vz \u003cname\u003e a. Alter to include the following: spec: components: elasticsearch: installArgs: - name: nodes.data.replicas value: \"3\" - name: nodes.data.requests.storage value: 200Gi b. Save the changes.  This completes the process.\n","categories":"","description":"Scaling and resizing OpenSearch to restore healthy status","excerpt":"Scaling and resizing OpenSearch to restore healthy status","ref":"/docs/troubleshooting/troubleshooting-elasticsearch/","tags":"","title":"OpenSearch Scaling and Resizing"},{"body":"The Oracle Cloud Infrastructure Logging service is a highly scalable and fully managed single pane of glass for all the logs in your tenancy. You can configure Verrazzano to send logs to Oracle Cloud Infrastructure Logging instead of OpenSearch. For general information, see Oracle Cloud Infrastructure Logging Overview.\nSet up custom logs Verrazzano can send its logs to Oracle Cloud Infrastructure custom logs. You will need to provide two Oracle Cloud Infrastructure Log identifiers in your Verrazzano installation resource - one for Verrazzano system logs and one for application logs. Follow the steps in Creating Custom Logs to create two custom logs. Do not create an agent configuration when creating a custom log, otherwise the log records will be duplicated.\nConfigure credentials The Fluentd plug-in included with Verrazzano will use Oracle Cloud Infrastructure instance principal authentication by default. Optionally, you can configure Verrazzano with a user API signing key. API signing key authentication is required to send logs to Oracle Cloud Infrastructure Logging if the cluster is running outside of Oracle Cloud Infrastructure.\n     Create a dynamic group that includes the compute instances in your cluster’s node pools and assign the appropriate policy, so that the dynamic group is allowed to send log entries to the custom logs you created earlier. Pay close attention to the required permissions.\nIf the dynamic group and policy are configured incorrectly, then Fluentd will fail to send logs to Oracle Cloud Infrastructure Logging.\n  If you do not already have an API signing key, then see Required Keys and OCIDs in the Oracle Cloud Infrastructure documentation. You need to create an Oracle Cloud Infrastructure configuration file with the credential details and then use that configuration file to create a secret. The following requirements must be met for Fluentd Oracle Cloud Infrastructure Logging to work:\n The profile name in the Oracle Cloud Infrastructure configuration file must be DEFAULT. The key_file path in the Oracle Cloud Infrastructure configuration file must be /root/.oci/key. The actual key file does not need to be in that location, because you will be providing the actual key file location in a secret. The user associated with the API key must have the appropriate Oracle Cloud Infrastructure Identity and Access Management (IAM) policy in place to allow the Fluentd plug-in to send logs to Oracle Cloud Infrastructure. See Details for Logging in the Oracle Cloud Infrastructure documentation for the IAM policies used by the Oracle Cloud Infrastructure Logging service.  After the Verrazzano platform operator has been installed, create an opaque secret in the verrazzano-install namespace from the Oracle Cloud Infrastructure configuration and private key files. The key for the configuration file must be config and the key for the private key file data must be key.\nHere is an example kubectl command that will create the secret.\n$ kubectl create secret generic oci-fluentd -n verrazzano-install \\ --from-file=config=/home/myuser/oci_config --from-file=key=/home/myuser/keys/oci_api.pem The secret should look something like this.\napiVersion: v1 data: config: W0RFRkFVTFRdCnVzZXI9b2NpZDEudXN... key: LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS... kind: Secret metadata: name: oci-fluentd namespace: verrazzano-install type: Opaque For convenience, there is a helper script available here that you can point at an existing Oracle Cloud Infrastructure configuration file and it will create the secret for you. The script allows you to override the default configuration file location, profile name, and the name of the secret.\n  Install Verrazzano Oracle Cloud Infrastructure Logging is enabled in your cluster when installing Verrazzano. The Verrazzano installation custom resource has fields for specifying two custom logs: one for system logs and one for application logs. Here is an example Verrazzano installation YAML file for each type of credential. Note that the API references Kibana, upcoming releases will use OpenSearch Dashboards in the public API.\n     apiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: vz-oci-logging spec: profile: dev components: fluentd: enabled: true oci: systemLogId: ocid1.log.oc1.iad.system.example defaultAppLogId: ocid1.log.oc1.iad.app.example elasticsearch: enabled: false kibana: enabled: false   When using user API credentials, you need to configure the name of the secret in the Verrazzano custom resource, under the Oracle Cloud Infrastructure section of the Fluentd component settings. Your YAML file should look something like this.\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: vz-oci-logging spec: profile: dev components: fluentd: enabled: true oci: systemLogId: ocid1.log.oc1.iad.system.example defaultAppLogId: ocid1.log.oc1.iad.app.example apiSecret: oci-fluentd elasticsearch: enabled: false kibana: enabled: false The apiSecret value must match the secret you created earlier when configuring the user API credentials.\n  Override the default log objects You can override the Oracle Cloud Infrastructure Log object on an individual namespace. To specify a log identifier on a namespace, add an annotation named verrazzano.io/oci-log-id to the namespace. The value of the annotation is the Oracle Cloud Infrastructure Log object identifier.\nHere is an example namespace.\napiVersion: v1 kind: Namespace metadata: annotations: verrazzano.io/oci-log-id: ocid1.log.oc1.iad.ns.app.example creationTimestamp: \"2022-01-14T15:09:19Z\" labels: istio-injection: enabled verrazzano-managed: \"true\" name: example spec: finalizers: - kubernetes status: phase: Active Note that if you add and subsequently remove the annotation, then the logs will revert to the default Oracle Cloud Infrastructure Log object specified in the Verrazzano custom resource.\nSearch logs To search Verrazzano logs, you can use the Oracle Cloud Infrastructure Console, Oracle Cloud Infrastructure CLI, or Oracle Cloud Infrastructure SDK.\nFor example, using the Oracle Cloud Infrastructure CLI to search the system logs for records emitted by the verrazzano-application-operator container.\n$ oci logging-search search-logs --search-query=\\ \"search \\\"ocid1.compartment.oc1..example/ocid1.loggroup.oc1.iad.example/ocid1.log.oc1.iad.example\\\" | \\ where \\\"data\\\".\\\"kubernetes.container_name\\\" = 'verrazzano-application-operator' | sort by datetime desc\" \\ --time-start 2021-12-07 --time-end 2021-12-17 Search for all application log records in the springboot namespace.\n$ oci logging-search search-logs --search-query=\\ \"search \\\"ocid1.compartment.oc1..example/ocid1.loggroup.oc1.iad.example/ocid1.log.oc1.iad.example\\\" | \\ where \\\"data\\\".\\\"kubernetes.namespace_name\\\" = 'springboot' | sort by datetime desc\" \\ --time-start 2021-12-07 --time-end 2021-12-17 For more information on searching logs, see the Logging Query Language Specification.\nTroubleshooting If you are not able to view Verrazzano logs in Oracle Cloud Infrastructure Logging, then check the Fluentd container logs in the cluster to see if there are errors.\n$ kubectl logs -n verrazzano-system -l app=fluentd --tail=-1 If you see not authorized error messages, then there is likely a problem with the Oracle Cloud Infrastructure Dynamic Group or IAM policy that is preventing the Fluentd plug-in from communicating with the Oracle Cloud Infrastructure API.\nTo ensure the appropriate permissions are in place, review the Oracle Cloud Infrastructure Logging required permissions documentation.\n","categories":"","description":"Learn how to send Verrazzano logs to the Oracle Cloud Infrastructure Logging service","excerpt":"Learn how to send Verrazzano logs to the Oracle Cloud Infrastructure Logging service","ref":"/docs/monitoring/oci-logging/oci-logging/","tags":"","title":"Oracle Cloud Infrastructure Logging Service"},{"body":"Verrazzano requires the following:\n A Kubernetes cluster and a compatible kubectl. At least 2 CPUs, 100GB disk storage, and 16GB RAM available on the Kubernetes worker nodes. This is sufficient to install the development profile of Verrazzano. Depending on the resource requirements of the applications you deploy, this may or may not be sufficient for deploying your applications.  Supported Hardware Verrazzano requires x86-64; other architectures are not supported.\nSupported Software Versions Verrazzano supports the following software versions.\nKubernetes You can install Verrazzano on the following Kubernetes versions.\n   Verrazzano Kubernetes Versions     1.0 1.18, 1.19, 1.20   1.1 1.19, 1.20, 1.21   1.2 1.19, 1.20, 1.21    For more information, see Kubernetes Release Documentation. For platform specific details, see Verrazzano platform setup.\nWebLogic Server The supported versions of WebLogic Server are dependent on the WebLogic Kubernetes Operator version. See the WebLogic Server versions supported here.\nCoherence The supported versions of Coherence are dependent on the Coherence Operator version. See the Coherence versions supported here.\nHelidon Verrazzano supports all versions of Helidon. For more information, see Helidon and Helidon Commercial Offerings.\nInstalled Components Verrazzano installs a curated set of open source components. The following table lists each open source component with its version and a brief description.\n   Component Version Description     cert-manager 1.2.0 Automates the management and issuance of TLS certificates.   Coherence Operator 3.2.3 Assists with deploying and managing Coherence clusters.   OpenSearch 1.2.3 Provides a distributed, multitenant-capable full-text search engine.   ExternalDNS 0.10.2 Synchronizes exposed Kubernetes Services and ingresses with DNS providers.   Fluentd 1.12.3 Collects logs and sends them to OpenSearch.   Grafana 7.5.11 Tool to help you study, analyze, and monitor metrics.   Istio 1.10.4 Service mesh that layers transparently onto existing distributed applications.   Keycloak 15.0.2 Provides single sign-on with Identity and Access Management.   Kiali 1.34.1 Management console for the Istio service mesh.   OpenSearch Dashboards 1.2.0 Provides search and data visualization capabilities for data indexed in OpenSearch.   MySQL 8.0.28 Open source relational database management system used by Keycloak.   NGINX Ingress Controller 0.46.0 Traffic management solution for cloud‑native applications in Kubernetes.   Node Exporter 1.0.0 Prometheus exporter for hardware and OS metrics.   OAM Kubernetes Runtime 0.3.0 Plug-in for implementing Open Application Model (OAM) control plane with Kubernetes.   Prometheus 2.31.1 Provides event monitoring and alerting.   Rancher 2.5.9 Manages multiple Kubernetes clusters.   WebLogic Kubernetes Operator 3.3.7 Assists with deploying and managing WebLogic domains.    ","categories":"","description":"Review the prerequisite requirements, software versions installed and supported by Verrazzano","excerpt":"Review the prerequisite requirements, software versions installed and supported by Verrazzano","ref":"/docs/setup/prereqs/","tags":"","title":"Prerequisites"},{"body":"A Verrazzano project provides a way to group application namespaces that are owned or administered by the same user or group of users.\nThe VerrazzanoProject resource A VerrazzanoProject resource is created by a Verrazzano admin user, and specifies the following:\n A list of namespaces that the project governs. One or more users, groups, or service accounts that will be granted the verrazzano-project-admin role for the VerrazzanoProject. Project admins may deploy or delete applications and related resources in the namespaces in the project. One or more users, groups, or service accounts that will be granted the verrazzano-project-monitor role for the VerrazzanoProject. Project monitors may view the resources in the namespaces in the project, but not modify or delete them. A list of network policies to apply to the namespaces in the project.  The creation of a VerrazzanoProject results in:\n The creation of the specified namespaces in the project, if those do not already exist. The creation of a Kubernetes RoleBindings in each of the namespaces, to set up the appropriate permissions for the project admins and project monitors of the project. The creation of the specified network policies for each of the namespaces.  ","categories":"","description":"Learn about Verrazzano projects","excerpt":"Learn about Verrazzano projects","ref":"/docs/applications/projects/","tags":"","title":"Projects"},{"body":"Verrazzano provides tooling which assists in troubleshooting issues in your environment:\n k8s-dump-cluster.sh verrazzano-analysis  Tools Setup These tools are available for Linux and Mac: https://github.com/verrazzano/verrazzano/releases/.\n     Linux Instructions Use these instructions to obtain the analysis tools on Linux machines.\nDownload the tooling:  $ wget https://github.com/verrazzano/verrazzano/releases/download/v1.2.2/k8s-dump-cluster.sh $ wget https://github.com/verrazzano/verrazzano/releases/download/v1.2.2/k8s-dump-cluster.sh.sha256 $ wget https://github.com/verrazzano/verrazzano/releases/download/v1.2.2/verrazzano-analysis-linux-amd64.tar.gz $ wget https://github.com/verrazzano/verrazzano/releases/download/v1.2.2/verrazzano-analysis-linux-amd64.tar.gz.sha256 Verify the downloaded files:  $ sha256sum -c k8s-dump-cluster.sh.sha256 $ sha256sum -c verrazzano-analysis-linux-amd64.tar.gz.sha256 Unpack the verrazzano-analysis binary:  $ tar xvf verrazzano-analysis-linux-amd64.tar.gz   Mac Instructions Use these instructions to obtain the analysis tools on Mac machines.\nDownload the tooling:  $ wget https://github.com/verrazzano/verrazzano/releases/download/v1.2.2/k8s-dump-cluster.sh $ wget https://github.com/verrazzano/verrazzano/releases/download/v1.2.2/k8s-dump-cluster.sh.sha256 $ wget https://github.com/verrazzano/verrazzano/releases/download/v1.2.2/verrazzano-analysis-darwin-amd64.tar.gz $ wget https://github.com/verrazzano/verrazzano/releases/download/v1.2.2/verrazzano-analysis-darwin-amd64.tar.gz.sha256 Verify the downloaded files:  $ shasum -a 256 -c k8s-dump-cluster.sh.sha256 $ shasum -a 256 -c verrazzano-analysis-darwin-amd64.tar.gz.sha256 Unpack the verrazzano-analysis binary:  $ tar xvf verrazzano-analysis-darwin-amd64.tar.gz    Use the k8s-dump-cluster.sh tool The k8s-dump-cluster.sh tool is a shell script which runs various kubectl and helm commands against a cluster.\nNote that the data captured by this script might include sensitive information. This data is under your control; you can choose whether to share it.\nThe directory structure created by the k8s-dump-cluster.sh tool, for a specific cluster dump, appears as follows:\n$ CAPTURE_DIR cluster-dump directory per namespace (a directory at this level is assumed to represent a namespace) acme-orders.json application-configurations.json certificate-requests.json cluster-role-bindings.json cluster-roles.json cluster-roles.json coherence.json components.json {CONFIGNAME}.configmap (a file at this level for each configmap in the namespace) daemonsets.json deployments.json events.json gateways.json ingress-traits.json jobs.json multicluster-application-configurations.json multicluster-components.json multicluster-config-maps.json multicluster-logging-scopes.json multicluster-secrets.json namespace.json persistent-volume-claims.json persistent-volumes.json pods.json replicasets.json replication-controllers.json role-bindings.json services.json verrazzano-managed-clusters.json verrazzano-projects.json verrazzano_resources.json virtualservices.json weblogic-domains.json directory per pod (a directory at this level is assumed to represent a specific pod) logs.txt (includes logs for all containers and initContainers) api-resources.out application-configurations.json cluster-issuers.txt coherence.json configmap_list.out crd.json es_indexes.out gateways.json helm-ls.json helm-version.out images-on-nodes.csv ingress.json ingress-traits.json kubectl-version.json namespace_list.out network-policies.json network-policies.txt nodes.json pv.json verrazzano_resources.out virtualservices.json  The script shows the kubectl and helm commands which are run. The basic structure, shown previously, is formed by running the command, $ kubectl cluster-info dump --all-namespaces, with additional data captured into that directory structure.\nTo perform a dump of a cluster into a directory named my-cluster-dump:\n$ sh k8s-dump-cluster.sh -d my-cluster-dump\nUse the verrazzano-analysis tool The verrazzano-analysis tool analyzes data from a cluster dump captured using k8s-dump-cluster.sh, reports the issues found, and prescribes related actions to take. These tools are continually evolving with regard to what may be captured, the knowledge base of issues and actions, and the types of analysis that can be performed.\nUsers, developers, and Continuous Integration (CI) can use this tooling to quickly identify the root cause of encountered problems, determine mitigation actions, and provide a sharable report with other users or tooling.\nThe data that the analysis examines follows the structure created by the corresponding capture tooling. For example, k8s-dump-cluster.sh dumps a cluster into a specific structure, which might contain data that you do not want to share. The tooling analyzes the data and provides you with a report, which identifies issues and provides you with actions to take.\nThe verrazzano-analysis tool will find and analyze all cluster dump directories found under a specified root directory. This lets you create a directory to hold the cluster dumps of related clusters into sub-directories which the tool can analyze.\nFor example:\nmy-cluster-dumps CAPTURE_DIR-1 cluster-dump ... CAPTURE_DIR-2 cluster-dump ...  The tool analyzes each cluster dump directory found; you need to provide only the single root directory.\nTo perform an analysis of the clusters:\n$ verrazzano-analysis my-cluster-dumps\nUsage information Usage: verrazzano-analysis [options] captured-data-directory    Parameter Definition Default     -actions Include actions in the report. true   -help Display usage help.    -info Include informational messages. true   -minConfidence Minimum confidence threshold to report for issues, 0-10. 0   -minImpact Minimum impact threshold to report for issues, 0-10. 0   -reportFile Name of report output file. Output to stdout.   -support Include support data in the report. true   -version Display tool version.     ","categories":"","description":"Use the Verrazzano Analysis Tools to analyze cluster dumps","excerpt":"Use the Verrazzano Analysis Tools to analyze cluster dumps","ref":"/docs/troubleshooting/diagnostictools/verrazzanoanalysistool/","tags":"","title":"Verrazzano Analysis Tools"},{"body":"Open Application Model (OAM) is a runtime-agnostic specification for defining cloud native applications; it allows developers to focus on the application instead of the complexities of a particular runtime infrastructure. OAM provides the specification for several file formats and rules for a runtime to interpret. Verrazzano uses OAM to enable the definition of a composite application abstraction and makes OAM constructs available within a VerrazzanoApplication YAML file. Verrazzano provides the flexibility to combine what you want into a multicloud enablement. It uses the VerrazzanoApplication as a means to encapsulate a set of components, scopes, and traits, and deploy them on a selected cluster.\nOAM’s workload concept makes it easy to use many different workload types. Verrazzano includes specific workload types with special handling to deploy and manage those types, such as WebLogic, Coherence, and Helidon. OAM’s flexibility lets you create a grouping that is managed as a unit, although each component can be scaled or updated independently.\nHow does OAM work? OAM has five core concepts:\n Workloads - Declarations of the kinds of resources supported by the platform and the OpenAPI schema for that resource. Most Kubernetes CRDs can be exposed as workloads. Standard Kubernetes resource types can also be used (for example, Deployment, Service, Pod, ConfigMap). Components - Wrap a workload resource’s specification data within OAM specific metadata. Application Configurations - Describe a collection of components that comprise an application. This is also where customization (such as, environmental) of each component is done. Customization is achieved using scopes and traits. Scopes - Apply customization to several components. Traits - Apply customization to a single component.  ","categories":"","description":"","excerpt":"Open Application Model (OAM) is a runtime-agnostic specification for defining cloud native applications; it allows developers to focus on the application instead of the complexities of a particular …","ref":"/docs/concepts/verrazzanooam/","tags":"","title":"Verrazzano and the Open Application Model"},{"body":"Review the following key concepts to understand multicluster Verrazzano:\n Admin cluster - A Kubernetes cluster that serves as the central management point for deploying and monitoring applications in managed clusters. Managed clusters - A Kubernetes cluster that has the following characteristics:  It is registered with an admin cluster with a unique name. Verrazzano multicluster applications may be deployed to the managed cluster from the admin cluster. Logs and metrics for Verrazzano system components and Verrazzano multicluster applications deployed on the managed cluster are viewable from the admin cluster.   Verrazzano multicluster resources - Custom Kubernetes resources defined by Verrazzano.  Each multicluster resource serves as a wrapper for an underlying resource type. A multicluster resource allows the placement of the underlying resource to be specified as a list of names of the clusters in which the resource must be placed.    For more details, see here.\n","categories":"","description":"","excerpt":"Review the following key concepts to understand multicluster Verrazzano:\n Admin cluster - A Kubernetes cluster that serves as the central management point for deploying and monitoring applications in …","ref":"/docs/concepts/verrazzanomulticluster/","tags":"","title":"Verrazzano in a Multicluster Environment"},{"body":"A project provides a way to group application namespaces that are owned or administered by the same user or group of users. When creating a project, you can specify the subjects: users, groups and/or service accounts, that are to be granted access to the namespaces governed by the project. Two types of subjects may be specified:\n Project admins, who have both read and write access to the project’s namespaces. Project monitors, who have read-only access to the project’s namespaces.  For more information, see Projects.\n","categories":"","description":"","excerpt":"A project provides a way to group application namespaces that are owned or administered by the same user or group of users. When creating a project, you can specify the subjects: users, groups and/or …","ref":"/docs/concepts/verrazzanoproject/","tags":"","title":"Verrazzano Projects"},{"body":"","categories":"","description":"Learn about Verrazzano application workloads","excerpt":"Learn about Verrazzano application workloads","ref":"/docs/applications/workloads/","tags":"","title":"Workloads"},{"body":"","categories":"","description":"Read advice based on Analysis Tools findings and reports","excerpt":"Read advice based on Analysis Tools findings and reports","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/","tags":"","title":"Analysis Advice"},{"body":"Verrazzano issues certificates to secure access from external clients to secure system endpoints.\nA certificate from a certificate authority (CA) must be configured to issue the endpoint certificates in one of the following ways:\n Let Verrazzano generate a self-signed CA (the default). Configure a CA that you provide. Configure LetsEncrypt as the certificate issuer (requires Oracle Cloud Infrastructure DNS).  In all cases, Verrazzano uses cert-manager to manage the creation of certificates.\nNOTE Self-signed certificate authorities generate certificates that are NOT signed by a trusted authority; typically, they are not used in production environments.  Use the Verrazzano self-signed CA By default, Verrazzano creates its own self-signed CA. No configuration is required.\nUse a custom CA If you want to provide your own CA, you must:\n  (Optional) Create your own signing key pair and CA certificate.\nFor example, you can use the openssl CLI to create a key pair for the nip.io domain:\n# Generate a CA private key $ openssl genrsa -out tls.key 2048 # Create a self signed certificate, valid for 10yrs with the 'signing' option set $ openssl req -x509 -new -nodes -key tls.key -subj \"/CN=*.nip.io\" -days 3650 -reqexts v3_req -extensions v3_ca -out tls.crt The output of these commands will be two files, tls.key and tls.crt, the key and certificate for your signing key pair. These files must be named in that manner for the next step.\nIf you already have generated your own key pair, you must name the private key and certificate, tls.key and tls.crt, respectively. If your issuer represents an intermediate, ensure that tls.crt contains the issuer’s full chain in the correct order.\nYou can find more details on providing your own CA, in the cert-manager CA documentation.\n  Save your signing key pair as a Kubernetes secret.\n$ kubectl create ns mynamespace $ kubectl create secret tls myca --namespace=mynamespace --cert=tls.crt --key=tls.key   Specify the secret name and namespace location in the Verrazzano custom resource.\nThe custom CA secret must be provided to cert-manager using the following fields in spec.components.certManager.certificate.ca in the Verrazzano custom resource:\n spec.components.certManager.certificate.ca.secretName spec.components.certManager.certificate.ca.clusterResourceNamespace    For example, if you created a CA secret named myca in the namespace mynamespace, you would configure it as shown:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: custom-ca-example spec: profile: dev components: certManager: certificate: ca: secretName: myca clusterResourceNamespace: mynamespace Use LetsEncrypt certificates You can configure Verrazzano to use certificates generated by LetsEncrypt. LetsEncrypt implements the ACME protocol, which provides a standard protocol for the automated issuance of certificates signed by a trusted authority. This is managed through the spec.components.certManager.certificate.acme field in the Verrazzano custom resource.\nNOTE Using LetsEncrypt for certificates also requires using Oracle Cloud Infrastructure DNS for DNS management. For details, see the Customize DNS page.  To configure cert-manager to use LetsEncrypt as the certificates provider, you must configure a cert-manager ACME provider with the following values in the Verrazzano custom resource:\n Set the spec.components.certManager.certificate.acme.provider field to letsEncrypt. Set the spec.components.certManager.certificate.acme.emailAddress field to a valid email address for the letsEncrypt account. (Optional) Set the spec.components.certManager.certificate.acme.environment field to either staging or production (the default).  The following example configures Verrazzano to use the LetsEncrypt production environment by default, with Oracle Cloud Infrastructure DNS for DNS record management:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: letsencrypt-certs-example spec: profile: dev components: certManager: certificate: acme: provider: letsEncrypt emailAddress: jane.doe@mycompany.com dns: oci: ociConfigSecret: oci dnsZoneCompartmentOCID: ocid1.compartment.oc1..... dnsZoneOCID: ocid1.dns-zone.oc1..... dnsZoneName: example.com The following example configures Verrazzano to use the LetsEncrypt staging environment with Oracle Cloud Infrastructure DNS:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: letsencrypt-certs-example spec: profile: dev components: certManager: certificate: acme: provider: letsEncrypt emailAddress: jane.doe@mycompany.com environment: staging dns: oci: ociConfigSecret: oci dnsZoneCompartmentOCID: ocid1.compartment.oc1..... dnsZoneOCID: ocid1.dns-zone.oc1..... dnsZoneName: example.com  NOTE Certificates issued by the LetsEncrypt staging environment are signed by untrusted authorities, similar to self-signed certificates. They are typically not used in production environments.  LetsEncrypt staging versus production LetsEncrypt provides rate-limits on generated certificates to ensure fair usage across all clients. The production environment limits can be exceeded more frequently in environments where Verrazzano may be being installed or reinstalled frequently (like a test environment). This can result in failed installations due to rate limit exceptions on certificate generation.\nIn such environments, it is better to use the LetsEncrypt staging environment, which has much higher limits than the production environment. For test environments, the self-signed CA also may be more appropriate to completely avoid LetsEncrypt rate limits.\n","categories":"","description":"Customize SSL certificate generation for Verrazzano system endpoints","excerpt":"Customize SSL certificate generation for Verrazzano system endpoints","ref":"/docs/setup/customizing/certificates/","tags":"","title":"Customize Certificates"},{"body":"","categories":"","description":"Use Verrazzano analysis tooling","excerpt":"Use Verrazzano analysis tooling","ref":"/docs/troubleshooting/diagnostictools/","tags":"","title":"Diagnostic Tools"},{"body":"This example application provides a Helidon implementation of the Sock Shop Microservices Demo Application. It uses OAM resources to define the application deployment in a multicluster environment.\nBefore you begin  Set up a multicluster Verrazzano environment following the installation instructions. The example assumes that there is a managed cluster named managed1 associated with the multicluster environment. If your environment does not have a cluster of that name, then you should edit the deployment files and change the cluster name listed in the placement section.  Set up the following environment variables to point to the kubeconfig for the admin and managed clusters.\n$ export KUBECONFIG_ADMIN=/path/to/your/adminclusterkubeconfig $ export KUBECONFIG_MANAGED1=/path/to/your/managedclusterkubeconfig NOTE: The Sock Shop application deployment files are contained in the Verrazzano project located at \u003cVERRAZZANO_HOME\u003e/examples/multicluster/sockshop, where \u003cVERRAZZANO_HOME\u003e is the root of the Verrazzano project.\nDeploy the Sock Shop application   Create a namespace for the Sock Shop application by deploying the Verrazzano project.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN apply \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/sock-shop/verrazzano-project.yaml   Apply the Sock Shop OAM resources to deploy the application.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN apply \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/sock-shop/sock-shop-comp.yaml $ kubectl --kubeconfig $KUBECONFIG_ADMIN apply \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/sock-shop/sock-shop-app.yaml   Wait for the Sock Shop application to be ready. It may take a few minutes for the pod resources to start appearing on the managed cluster.\n$ kubectl --kubeconfig $KUBECONFIG_MANAGED1 wait \\ --for=condition=Ready pods \\ --all -n mc-sockshop \\ --timeout=300s   Explore the Sock Shop application The Sock Shop microservices application implements REST API endpoints including:\n /catalogue - Returns the Sock Shop catalog. This endpoint accepts the GET HTTP request method. /register - POST { \"username\":\"xxx\", \"password\":\"***\", \"email\":\"foo@example.com\", \"firstName\":\"foo\", \"lastName\":\"coo\" } to create a user. This endpoint accepts the POST HTTP request method.  NOTE: The following instructions assume that you are using a Kubernetes environment, such as OKE. Other environments or deployments may require alternative mechanisms for retrieving addresses, ports, and such.\nFollow these steps to test the endpoints:\n  Get the generated host name for the application.\n$ HOST=$(kubectl --kubeconfig $KUBECONFIG_MANAGED1 get gateway \\ -n mc-sockshop \\ -o jsonpath={.items[0].spec.servers[0].hosts[0]}) $ echo $HOST # Sample output sockshop-appconf.mc-sockshop.11.22.33.44.nip.io   Get the EXTERNAL_IP address of the istio-ingressgateway service.\n$ ADDRESS=$(kubectl --kubeconfig $KUBECONFIG_MANAGED1 get service \\ -n istio-system istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo $ADDRESS # Sample output 11.22.33.44   Access the Sock Shop example application:\n  Using the command line\n# Get catalogue $ curl -sk \\ -X GET \\ https://${HOST}/catalogue \\ --resolve ${HOST}:443:${ADDRESS} # Sample output [{\"count\":115,\"description\":\"For all those leg lovers out there....\", ...}] # Add a new user (replace values of username and password) $ curl -i \\ --header \"Content-Type: application/json\" --request POST \\ --data '{\"username\":\"foo\",\"password\":\"****\",\"email\":\"foo@example.com\",\"firstName\":\"foo\",\"lastName\":\"foo\"}' \\ -k https://${HOST}/register \\ --resolve ${HOST}:443:${ADDRESS} # Add an item to the user's cart $ curl -i \\ --header \"Content-Type: application/json\" --request POST \\ --data '{\"itemId\": \"a0a4f044-b040-410d-8ead-4de0446aec7e\",\"unitPrice\": \"7.99\"}' \\ -k https://${HOST}/carts/{username}/items \\ --resolve ${HOST}:443:${ADDRESS} # Sample output {\"itemId\":\"a0a4f044-b040-410d-8ead-4de0446aec7e\",\"quantity\":1,\"unitPrice\":7.99} # Get cart items $ curl -i \\ -k https://${HOST}/carts/{username}/items \\ --resolve ${HOST}:443:${ADDRESS} # Sample output [{\"itemId\":\"a0a4f044-b040-410d-8ead-4de0446aec7e\",\"quantity\":1,\"unitPrice\":7.99}] If you are using nip.io, then you do not need to include --resolve.\n  Local testing with a browser\nTemporarily, modify the /etc/hosts file (on Mac or Linux) or c:\\Windows\\System32\\Drivers\\etc\\hosts file (on Windows 10), to add an entry mapping the host name to the ingress gateway’s EXTERNAL-IP address. For example:\n11.22.33.44 sockshop.example.com Then, you can access the application in a browser at https://sockshop.example.com/catalogue.\nIf you are using nip.io, then you can access the application in a browser using the HOST variable (for example, https://${HOST}/catalogue). If you are going through a proxy, you may need to add *.nip.io to the NO_PROXY list.\n  Using your own DNS name\n Point your own DNS name to the ingress gateway’s EXTERNAL-IP address. In this case, you would need to edit the sock-shop-app.yaml file to use the appropriate value under the hosts section (such as yourhost.your.domain), before deploying the Sock Shop application. Then, you can use a browser to access the application at https://\u003cyourhost.your.domain\u003e/catalogue.      Troubleshooting   Verify that the application configuration, components, workloads, and ingress trait all exist.\n$ kubectl --kubeconfig $KUBECONFIG_MANAGED1 get ApplicationConfiguration -n mc-sockshop $ kubectl --kubeconfig $KUBECONFIG_MANAGED1 get Component -n mc-sockshop $ kubectl --kubeconfig $KUBECONFIG_MANAGED1 get VerrazzanoCoherenceWorkload -n mc-sockshop $ kubectl --kubeconfig $KUBECONFIG_MANAGED1 get Coherence -n mc-sockshop $ kubectl --kubeconfig $KUBECONFIG_MANAGED1 get IngressTrait -n mc-sockshop   Verify that the Sock Shop service pods are successfully created and transition to the READY state. Note that this may take a few minutes and that you may see some of the services terminate and restart.\n $ kubectl --kubeconfig $KUBECONFIG_MANAGED1 get pods -n mc-sockshop # Sample output NAME READY STATUS RESTARTS AGE carts-coh-0 2/2 Running 0 38m catalog-coh-0 2/2 Running 0 38m orders-coh-0 2/2 Running 0 38m payment-coh-0 2/2 Running 0 38m shipping-coh-0 2/2 Running 0 38m users-coh-0 2/2 Running 0 38m   A variety of endpoints are available to further explore the logs, metrics, and such, associated with the deployed Sock Shop application. You can access them according to the directions here.\n  Undeploy the Sock Shop application Regardless of its location, to undeploy the application, delete the application resources and the project from the admin cluster. Undeploy affects all clusters in which the application is located.\n  To undeploy the application, delete the Sock Shop OAM resources.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN delete \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/sock-shop/sock-shop-app.yaml $ kubectl --kubeconfig $KUBECONFIG_ADMIN delete \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/sock-shop/sock-shop-comp.yaml   Delete the project.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN delete \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/sock-shop/verrazzano-project.yaml   Delete the namespace mc-sockshop after the application pods are terminated.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN delete namespace mc-sockshop $ kubectl --kubeconfig $KUBECONFIG_MANAGED1 delete namespace mc-sockshop   ","categories":"","description":"Implementation of the [Sock Shop Microservices Demo Application](https://microservices-demo.github.io/) deployed to a multicluster environment.","excerpt":"Implementation of the [Sock Shop Microservices Demo Application](https://microservices-demo.github.io/) deployed to a multicluster environment.","ref":"/docs/samples/multicluster/sock-shop/","tags":"","title":"Multicluster Helidon Sock Shop"},{"body":"The IngressTrait custom resource contains the configuration of host and path rules for traffic routing to an application. Here is a sample ApplicationConfiguration that specifies an IngressTrait. To deploy an example application that demonstrates this IngressTrait, see Hello World Helidon.\napiVersion: core.oam.dev/v1alpha2 kind: ApplicationConfiguration metadata: name: hello-helidon-appconf namespace: hello-helidon annotations: version: v1.0.0 description: \"Hello Helidon application\" spec: components: - componentName: hello-helidon-component traits: - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: MetricsTrait spec: scraper: verrazzano-system/vmi-system-prometheus-0 - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: IngressTrait metadata: name: hello-helidon-ingress spec: rules: - paths: - path: \"/greet\" pathType: Prefix In the sample configuration, the IngressTrait hello-helidon-ingress is set on the hello-helidon-component application component and defines an ingress rule that configures a path and path type. This exposes a route for external access to the application. Note that because no hosts list is given for the IngressRule, a DNS host name is automatically generated.\nFor example, with the sample application configuration successfully deployed, the application will be accessible with the path specified in the IngressTrait and the generated host name.\n$ HOST=$(kubectl get gateways.networking.istio.io hello-helidon-hello-helidon-appconf-gw -n hello-helidon -o jsonpath={.spec.servers[0].hosts[0]}) $ echo $HOST hello-helidon-appconf.hello-helidon.11.22.33.44.nip.io $ curl -sk -X GET https://${HOST}/greet Alternatively, specific host names can be given in an IngressRule. Doing this implies that a secret and certificate have been created for the specific hosts and the secret name has been specified in the associated IngressSecurity secretName field.\nLoad balancer session affinity is configured using an HTTP cookie in a destination rule. Here is an updated sample ApplicationConfiguration that includes a destination rule with an HTTP cookie.\napiVersion: core.oam.dev/v1alpha2 kind: ApplicationConfiguration metadata: name: hello-helidon-appconf namespace: hello-helidon annotations: version: v1.0.0 description: \"Hello Helidon application\" spec: components: - componentName: hello-helidon-component traits: - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: MetricsTrait spec: scraper: verrazzano-system/vmi-system-prometheus-0 - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: IngressTrait metadata: name: hello-helidon-ingress spec: rules: - paths: - path: \"/greet\" pathType: Prefix - destination: httpCookie: name: sessioncookie path: \"/\" ttl: 600 IngressTrait    Field Type Description Required     apiVersion string oam.verrazzano.io/v1alpha1 Yes   kind string IngressTrait Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec IngressTraitSpec The desired state of an ingress trait. Yes    IngressTraitSpec IngressTraitSpec specifies the desired state of an ingress trait.\n   Field Type Description Required     rules IngressRule array A list of ingress rules to for an ingress trait. Yes   tls IngressSecurity The security parameters for an ingress trait. This is required only if specific hosts are given in an IngressRule. No    IngressRule IngressRule specifies a rule for an ingress trait.\n   Field Type Description Required     hosts string array One or more hosts exposed by the ingress trait. Wildcard hosts or hosts that are empty are filtered out. If there are no valid hosts provided, then a DNS host name is automatically generated and used. No   paths IngressPath array The paths to be exposed for an ingress trait. Yes   destination IngressDestination The destination host and port for the ingress paths. No    IngressPath IngressPath specifies a specific path to be exposed for an ingress trait.\n   Field Type Description Required     path string If no path is provided, it defaults to /. No   pathType string Path type values are case-sensitive and formatted as follows: exact: exact string matchprefix: prefix-based matchregex: regex-based matchIf the provided ingress path doesn’t contain a pathType, it defaults to prefix if the path is / and exact otherwise. No    IngressDestination IngressDestination specifies a specific destination host and port for the ingress paths.\n   Field Type Description Required     host string Destination host. No   port uint32 Destination port. No   httpCookie HttpCookie Session affinity cookie. No    NOTE If there are multiple ports defined for a service, then the destination port must be specified OR the service port name must have the prefix “http”.  HttpCookie HttpCookie specifies a session affinity cookie for an ingress trait.\n   Field Type Description Required     name string The name of the HTTP cookie. No   path string The path of the HTTP cookie. No   ttl uint32 The lifetime of the HTTP cookie (in seconds). No    IngressSecurity IngressSecurity specifies the secret containing the certificate securing the transport for an ingress trait.\n   Field Type Description Required     secretName string The name of a secret containing the certificate securing the transport. The specification of a secret here implies that a certificate was created for specific hosts, as specified in an IngressRule. Yes    ","categories":"","description":"","excerpt":"The IngressTrait custom resource contains the configuration of host and path rules for traffic routing to an application. Here is a sample ApplicationConfiguration that specifies an IngressTrait. To …","ref":"/docs/reference/api/oam/ingresstrait/","tags":"","title":"IngressTrait Custom Resource Definition"},{"body":"This document describes built-in configuration profiles that you can use to simplify a Verrazzano installation. An installation profile is a well-known configuration of Verrazzano settings that can be referenced by name, which then can be customized as needed.\nThe following table describes the Verrazzano installation profiles.\n   Profile Description Characteristics     prod Full install, production configuration. Default profile:- Full installation.- Persistent storage. - Production OpenSearch cluster topology.   dev Development or evaluation configuration. Lightweight installation:- For evaluation purposes.- No persistence.- Single-node OpenSearch cluster topology.   managed-cluster A specialized installation for managed clusters in a multicluster topology. Minimal installation for a managed cluster:- Cluster must be registered with an admin cluster to use multicluster features.    Use an installation profile To use a profile to install Verrazzano, set the profile name in the profile field of your Verrazzano custom resource.\nFor example, to use the dev profile:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: example-verrazzano spec: profile: dev To use a different profile, simply replace dev with prod or managed-cluster.\nCustomize an installation profile You can override the profile settings for any component regardless of the profile. The following example uses a customized dev profile to configure a small 8Gi persistent volume for the MySQL instance used by Keycloak to provide more stability for the Keycloak service:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: custom-dev-example spec: profile: dev components: keycloak: mysql: volumeSource: persistentVolumeClaim: claimName: mysql volumeClaimSpecTemplates: - metadata: name: mysql spec: resources: requests: storage: 8Gi For details on how to customize Verrazzano components, see Customize an Installation.\nProfile configurations The following table lists the Verrazzano components that are installed with each profile. Note that you can customize any Verrazzano installation, regardless of the profile.\n   Component dev prod managed-cluster     Istio ✔️ ✔️ ✔️   NGINX ✔️ ✔️ ✔️   cert-manager ✔️ ✔️ ✔️   External-DNS ️ ️    Prometheus ✔️ ✔️ ✔️   OpenSearch ✔️ ✔️    Console ✔️ ✔️    OpenSearch Dashboards ✔️ ✔️    Grafana ✔️ ✔️    Rancher ✔️ ✔️    Keycloak ✔️ ✔️     Prometheus and Grafana configurations The following table describes the Prometheus and Grafana configurations in each profile.\n   Profile Prometheus Grafana     prod 1 replica (128M memory, 50Gi storage) 1 replica (48M memory, 50Gi storage)   dev 1 replica (128M memory, ephemeral storage) 1 replica (48M memory, ephemeral storage)   managed-cluster 1 replica (128M memory, 50Gi storage) Not installed    OpenSearch Dashboards and OpenSearch configurations The following table describes the OpenSearch Dashboards and OpenSearch cluster topology in each profile.\n   Profile OpenSearch OpenSearch Dashboards     prod 3 master replicas (1.4Gi memory, 50Gi storage each)1 ingest replica (2.5Gi memory, no storage)2 data replicas (4.8Gi memory, 50Gi storage each) 1 replica (192M memory, ephemeral storage)   dev 1 master/data/ingest replica (1Gi memory, ephemeral storage) 1 replica (192M memory, ephemeral storage)   managed-cluster Not installed Not installed    NOTE OpenSearch containers are configured to use 75% of the configured request memory for the Java min/max heap settings.  Profile-independent defaults The following table shows the settings for components that are profile-independent (consistent across all profiles unless overridden).\n   Component Default     DNS Wildcard DNS provider nip.io.   Certificates Uses the cert-manager self-signed ClusterIssuer for certificates.   Ingress-type Defaults to LoadBalancer service type for the ingress.    For details on how to customize Verrazzano components, see Customize an Installation.\n","categories":"","description":"How to use named Verrazzano configurations to simplify an installation","excerpt":"How to use named Verrazzano configurations to simplify an installation","ref":"/docs/setup/install/profiles/","tags":"","title":"Installation Profiles"},{"body":"Verrazzano uses Kubernetes Role-Based Access Control (RBAC) to protect Verrazzano resources.\nVerrazzano includes a set of roles that can be granted to users, enabling access to Verrazzano resources managed by Kubernetes. In addition, Verrazzano creates a number of roles that grant permissions needed by various Verrazzano system components (operators and third-party components).\nVerrazzano creates default role bindings during installation and for projects, at project creation or update.\nNOTE Kubernetes RBAC must be enabled in every cluster to which Verrazzano is deployed or access control will not work. RBAC is enabled by default in most Kubernetes environments.  Verrazzano user roles The following table lists the defined Verrazzano user roles. Each is a ClusterRole intended to be granted directly to users or groups. (In some scenarios, it may be appropriate to grant a user role to a service account.)\n   Verrazzano Role Binding Scope Description     verrazzano-admin Cluster Manage Verrazzano system components, clusters, and projects. Install/update Verrazzano.   verrazzano-monitor Cluster View/monitor Verrazzano system components, clusters, and projects.   verrazzano-project-admin Namespace Deploy/manage applications.   verrazzano-project-monitor Namespace View/monitor applications.    Kubernetes user roles Verrazzano roles do not include permissions for Kubernetes itself. Instead, it relies on the default user roles provided by Kubernetes. This allows Verrazzano to easily grant the Kubernetes access appropriate to a Verrazzano role, without having to maintain a long list of fine-grained Kubernetes permissions in the Verrazzano roles.\nThe following table shows the default Kubernetes roles that are granted by default for each Verrazzano role.\n   Verrazzano Role Kubernetes Role Binding Scope     verrazzano-admin admin Cluster   verrazzano-monitor view Cluster   verrazzano-project-admin admin Namespace   verrazzano-project-monitor view Namespace    Default role bindings Verrazzano creates role bindings for the system and for projects, binding Verrazzano ClusterRoles to one or more Kubernetes Subjects. By default, each role is bound to a Keycloak group, so all Keycloak users who are members of that group will be granted the role.\nAlso, Verrazzano creates role bindings for the corresponding Kubernetes user roles. The Kubernetes role appropriate for a given Verrazzano role is bound to the same set of Subjects as the corresponding Verrazzano role.\nThe default bindings can be overridden by specifying one or more Kubernetes Subjects to which the role should be bound. Any valid Subject can be specified (user, group, or service account), but two caveats should be kept in mind:\n It’s generally better to grant a role to a group, rather than a specific user, so that roles can be granted (or withdrawn) by editing a user’s group memberships, rather than deleting a role binding and creating a new one. If you do want to grant a role directly to a specific user, the user must be specified using its unique ID, not its user name. This is because the authentication proxy impersonates the sub (subject) field from the user’s token, which contains the ID. Keycloak user IDs are guaranteed to be unique, unlike user names.  Default system role bindings Verrazzano creates role bindings for system users during installation. The default role bindings are listed below.\n   Role Default Binding Subject     verrazzano-admin group: verrazzano-admins   verrazzano-monitor group: verrazzano-monitors    Default project role bindings Verrazzano creates role bindings for project users at project creation or update. The default role bindings are listed below.\n   Role Default Binding Subject     verrazzano-project-admin group: verrazzano-project-\u003cproj_name\u003e-admins   verrazzano-project-monitor group: verrazzano-project-\u003cproj_name\u003e-monitors    NOTE The role bindings for project roles are created automatically, but the project-specific groups that they refer to are not automatically created. You must create those groups using the Keycloak console or API, or specify different binding subjects for the project.  Override default role bindings You can override the default role bindings that are created for system and project roles.\nOverride system role bindings To override the set of subjects that are bound to Verrazzano (and Kubernetes) roles during installation, add the Subjects to the Verrazzano CR you use to install Verrazzano, as shown in the following example:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: example-verrazzano spec: ... security: adminSubjects: - name: admin-group kind: Group monitorSubjects: - name: view-group kind: Group ... You can specify multiple subjects for both admin and monitor roles. You can also specify a subject or subjects for one role, but not the other. If no subjects are specified for a role, then the default binding subjects will be used.\nOverride project role bindings To override the set of subjects that are bound to Verrazzano (and Kubernetes) roles for a project, add the Subjects to the VerrazzanoProject CR for the project, as shown in the example below.\nNote that the generated role bindings will be updated if you update the VerrazzanoProject CR and change the subjects specified for either role.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: VerrazzanoProject metadata: name: my-project spec: ... security: projectAdminSubjects: - name: my-project-admin-group kind: Group projectMonitorSubjects: - name: my-project-view-group kind: Group ... As with the system role bindings, you can specify multiple subjects for both project-admin and project-monitor roles. You can also specify a subject or subjects for one role, but not the other. If no subjects are specified for a role, then the default binding subjects will be used.\n","categories":"","description":"Learn about role-based access control in Verrazzano","excerpt":"Learn about role-based access control in Verrazzano","ref":"/docs/security/rbac/rbac/","tags":"","title":"Kubernetes RBAC"},{"body":"This guide describes how to move (“Lift-and-Shift”) an on-premises WebLogic Server domain to a cloud environment running Kubernetes using Verrazzano.\nOverview The Initial steps create a very simple on-premises domain that you will move to Kubernetes. The sample domain is the starting point for the lift and shift process; it contains one application (ToDo List) and one data source. First, you’ll configure the database and the WebLogic Server domain. Then, in Lift and Shift, you will move the domain to Kubernetes with Verrazzano. This guide does not include the setup of the networking that would be needed to access an on-premises database, nor does it document how to migrate a database to the cloud.\nWhat you need   The Git command-line tool and access to GitHub\n  MySQL Database 8.x - a database server\n  WebLogic Server 12.2.1.4.0 - an application server; Note that all WebLogic Server installers are supported except the Quick Installer.\n  Maven - to build the application\n  WebLogic Deploy Tooling (WDT) - v1.9.15 or later, to convert the WebLogic Server domain to and from metadata\n  WebLogic Image Tool (WIT) - v1.9.13 or later, to build the Docker image\n  Initial steps In the initial steps, you create a sample domain that represents your on-premises WebLogic Server domain.\nCreate a database using MySQL called tododb   Download the MySQL image from Docker Hub.\n$ docker pull mysql:latest   Start the container database (and optionally mount a volume for data).\n$ export MYSQL_USER=\u003cyour-mysql-username\u003e $ export MYSQL_PASSWORD=\u003cyour-mysql-password\u003e $ export MYSQL_ROOT_PASSWORD=\u003cyour-mysql-rootpassword\u003e $ docker run --name tododb \\ -p 3306:3306 \\ -e MYSQL_USER=$MYSQL_USER \\ -e MYSQL_PASSWORD=$MYSQL_PASSWORD \\ -e MYSQL_DATABASE=tododb \\ -e MYSQL_ROOT_PASSWORD=$MYSQL_ROOT_PASSWORD \\ -d mysql:latest   Start a MySQL client to change the password algorithm to mysql_native_password.\n Assuming the database server is running, start a database CLI client. $ docker exec \\ -it tododb mysql \\ -uroot \\ -p  When prompted for the password, enter the password for the root user. After being connected, run the ALTER command at the MySQL prompt. $ ALTER USER '\u003cyour-mysql-username\u003e'@'%' identified with mysql_native_password by '\u003cyour-mysql-password\u003e';     Create a WebLogic Server domain   If you do not have WebLogic Server 12.2.1.4.0 installed, install it now.\n  Choose the GENERIC installer from WebLogic Server Downloads and follow the documented installation instructions.\n  Be aware of these domain limitations:\n There are two supported domain types, single server and single cluster. Domains must use:  The default value AdminServer for AdminServerName. WebLogic Server listen port for the Administration Server: 7001. WebLogic Server listen port for the Managed Server: 8001. Note that these are all standard WebLogic Server default values.      Save the installer after you have finished; you will need it to build the Docker image.\n  To make copying commands easier, define an environment variable for ORACLE_HOME that points to the directory where you installed WebLogic Server 12.2.1.4.0. For example:\n$ export ORACLE_HOME=$HOME/Oracle/Middleware/Oracle_Home     Use the Oracle WebLogic Server Configuration Wizard to create a domain called tododomain.\nNOTE: This example assumes that the on premises WebLogic Server domain is on Linux.\n Launch $ORACLE_HOME/oracle_common/common/bin/config.sh. Select Create a new domain. Specify a Domain Location of \u003coracle home\u003e/user_projects/domains/tododomain and click Next. Select the Basic WebLogic Server Domain [wlserver] template and click Next. Enter the password for the administrative user and click Next. Accept the defaults for Domain Mode and JDK, and click Next. Select Administration Server and click Next. Ensure that the server name is AdminServer and click Next. Click Create. After it has completed, click Next, then Finish.    To start the newly created domain, run the domain’s start.\n $ $ORACLE_HOME/user_projects/domains/tododomain/bin/startWebLogic.sh   Access the Console of the newly started domain with your browser, for example, http://localhost:7001/console, and log in using the administrator credentials you specified.\n  Add a data source configuration to access the database Using the WebLogic Server Administration Console, log in and add a data source configuration to access the MySQL database. During the data source configuration, you can accept the default values for most fields, but the following fields are required to match the application and database settings you used when you created the MySQL database.\n  In the left pane in the Console, expand Services and select Data Sources.\n  On the Summary of JDBC Data Sources page, click New and select Generic Data Source.\n  On the JDBC Data Sources page, enter or select the following information:\n Name: tododb JNDI Name: jdbc/ToDoDB Database Type: MySQL    Click Next and then click Next two more times.\n  On the Create a New JDBC Data Source page, enter the following information:\n Database Name: tododb Host name: localhost Database Port: 3306 Database User Name: \u003cyour-mysql-username\u003e Password: \u003cyour-mysql-password\u003e Confirm Password: \u003cyour-mysql-password\u003e    Click Next.\n  Select Test Configuration, and make sure you see “Connection Test Succeeded” in the Messages field of the Console.\n  Click Next.\n  On the Select Targets page, select AdminServer.\n  Click Finish to complete the configuration.\n  Build and deploy the application   Using Maven, build this project to produce todo.war.\nNOTE: You should clone this repo outside of $ORACLE_HOME or copy the WAR file to another location, as WDT may ignore it during the model creation phase.\n $ git clone https://github.com/verrazzano/examples.git $ cd examples/todo-list/ $ mvn clean package   Using the WebLogic Server Administration Console, deploy the ToDo List application.\n In the left pane in the Console, select Deployments and click Install. Use the navigation links or provide the file path to todo.war typically \u003crepo\u003e/todo-list/target. For example, if you cloned the examples repository in your $HOME directory, the location should be $HOME/examples/examples/todo-list/target/todo.war. Click Next twice, then Finish.  NOTE: The remaining steps assume that the application context is todo.\n  Initialize the database After the application is deployed and running in WebLogic Server, access the http://localhost:7001/todo/rest/items/init REST service to create the database table used by the application. In addition to creating the application table, the init service also will load four sample items into the table.\nIf you get an error here, go back to the Select Targets page in the WebLogic Server Administration Console and make sure that you selected AdminServer as the data source target.\nAccess the application  Access the application at http://localhost:7001/todo/index.html.   Add a few entries or delete some. After verifying the application and database, you may shut down the local WebLogic Server domain.  Lift and Shift steps The following steps will move the sample domain to Kubernetes with Verrazzano.\nCreate a WDT Model  If you have not already done so, download v1.9.15 or later of WebLogic Deploy Tooling (WDT) from GitHub. Unzip the installer weblogic-deploy.zip file so that you can access bin/discoverDomain.sh. To make copying commands easier, define an environment variable for WDT_HOME that points to the directory where you installed WebLogic Deploy Tooling.  $ export WDT_HOME=/install/directory   For example, to get the latest version:\n$ curl -OL https://github.com/oracle/weblogic-deploy-tooling/releases/latest/download/weblogic-deploy.zip $ unzip weblogic-deploy.zip $ cd weblogic-deploy $ export WDT_HOME=$(pwd) To create a reusable model of the application and domain, use WDT to create a metadata model of the domain.\n First, create an output directory to hold the generated scripts and models. Then, run WDT discoverDomain. $ mkdir v8o $ $WDT_HOME/bin/discoverDomain.sh \\ -oracle_home $ORACLE_HOME \\ -domain_home /path/to/domain/dir \\ -model_file ./v8o/wdt-model.yaml \\ -archive_file ./v8o/wdt-archive.zip \\ -target vz \\ -output_dir v8o   You will find the following files in ./v8o:\n create_k8s_secrets.sh - A helper script with kubectl commands to apply the Kubernetes secrets needed for this domain vz-application.yaml - Verrazzano application configuration and component file vz_variable.properties - A set of properties extracted from the WDT domain model wdt-archive.zip - The WDT archive file containing the ToDo List application WAR file wdt-model.yaml - The WDT model of the WebLogic Server domain  If you chose to skip the Access the application step and did not verify that the ToDo List application was deployed, then you should verify that you see the todo.war file inside the wdt-archive.zip file. If you do not see the WAR file, there was something wrong in your deployment of the application on WebLogic Server that will require additional troubleshooting in your domain.\nCreate a Docker image At this point, the Verrazzano model is just a template for the real model. The WebLogic Image Tool will fill in the placeholders for you, or you can edit the model manually to set the image name and domain home directory.\n If you have not already done so, download WebLogic Image Tool (WIT) from GitHub. Unzip the installer imagetool.zip file so that you can access bin/imagetool.sh. To make copying commands easier, define an environment variable for WIT_HOME that points to the directory where you installed WebLogic Image Tool.  $ export WIT_HOME=/install/directory   For example, to get the latest WIT tool:\n$ curl -OL https://github.com/oracle/weblogic-image-tool/releases/latest/download/imagetool.zip $ unzip imagetool.zip $ cd imagetool $ export WIT_HOME=$(pwd) You will need a Docker image to run your WebLogic Server domain in Kubernetes. To use WIT to create the Docker image, run imagetool create. Although WIT will download patches and PSUs for you, it does not yet download installers. Until then, you must download the WebLogic Server and Java Development Kit installer manually and provide their location to the imagetool cache addInstaller command.\n# The directory created previously to hold the generated scripts and models. $ cd v8o $ $WIT_HOME/bin/imagetool.sh cache addInstaller \\ --path /path/to/installer/jdk-8u231-linux-x64.tar.gz \\ --type jdk \\ --version 8u231 # The installer file name may be slightly different depending on # which version of the 12.2.1.4.0 installer that you downloaded, slim or generic. $ $WIT_HOME/bin/imagetool.sh cache addInstaller \\ --path /path/to/installer/fmw_12.2.1.4.0_wls_Disk1_1of1.zip \\ --type wls \\ --version 12.2.1.4.0 $ $WIT_HOME/bin/imagetool.sh cache addInstaller \\ --path /path/to/installer/weblogic-deploy.zip \\ --type wdt \\ --version latest # Paths for the files in this command assume that you are running it from the # v8o directory created during the `discoverDomain` step. $ $WIT_HOME/bin/imagetool.sh create \\ --tag your/repo/todo:1 \\ --version 12.2.1.4.0 \\ --jdkVersion 8u231 \\ --wdtModel ./wdt-model.yaml \\ --wdtArchive ./wdt-archive.zip \\ --wdtVariables ./vz_variable.properties \\ --resourceTemplates=./vz-application.yaml \\ --wdtModelOnly The imagetool create command will have created a local Docker image and updated the Verrazzano model with the domain home and image name. Check your Docker images for the tag that you used in the create command using docker images from the Docker CLI.\nIf everything worked correctly, it is time to push that image to the container registry that Verrazzano will use to access the image from Kubernetes. You can use the Oracle Cloud Infrastructure Registry (OCIR) as your repository for this example, but most Docker compliant registries should work.\nThe variables in the vz-application.yaml resource template should be resolved with information from the image tool build.\nVerify this by looking in the v8o/vz-application.yaml file to make sure that the image: {{{imageName}}} value has been set with the given --tag value.\nPush the image to your repo.\nNOTE: The image name must be the same as what is in the vz-application.yaml file under spec \u003e workload \u003e spec \u003e image for the tododomain-domain component.\n$ docker push your/repo/todo:1 Deploy to Verrazzano After the application image has been created, there are several steps required to deploy the application into a Verrazzano environment.\nThese include:\n Creating and labeling the tododomain namespace. Creating the necessary secrets required by the ToDo List application. Creating the Verrazzano components such as Service, Deployment, and ConfigMap required by the MySQL instance in the tododomain namespace. Updating the vz-application.yaml file to enable the Verrazzano MySQL components in the ToDo List ApplicationConfiguration to deploy as Kubernetes objects. Updating the vz-application.yaml file to use the Verrazzano MySQL deployment and (optionally) expose the WebLogic Server Administration Console. Applying the vz-application.yaml file.  The following steps assume that you have a Kubernetes cluster and that Verrazzano is already installed in that cluster.\nLabel the namespace Create the tododomain namespace, and add labels to allow the WebLogic Server Kubernetes Operator to manage it and enabled for Istio.\n$ kubectl create namespace tododomain $ kubectl label namespace tododomain verrazzano-managed=true istio-injection=enabled Create the required secrets If you haven’t already done so, edit and run the create_k8s_secrets.sh script to generate the Kubernetes secrets. WDT does not discover passwords from your existing domain. Before running the create secrets script, you will need to edit create_k8s_secrets.sh to set the passwords for the WebLogic Server domain and the data source. In this domain, there are a few passwords that you need to enter:\n Administrator credentials ToDo database credentials  For example:\n# Update \u003cadmin-user\u003e and \u003cadmin-password\u003e for weblogic-credentials $ create_paired_k8s_secret weblogic-credentials \u003cyour-WLS-username\u003e \u003cyour-WLS-password\u003e # Update \u003cuser\u003e and \u003cpassword\u003e for tododomain-jdbc-tododb $ create_paired_k8s_secret jdbc-tododb \u003cyour-mysql-username\u003e \u003cyour-mysql-password\u003e Then run the script:\n$ sh ./create_k8s_secrets.sh Verrazzano will need a credential to pull the image that you just created, so you need to create one more secret. The name for this credential can be changed in the vz-application.yaml file to anything you like, but it defaults to tododomain-registry-credentials.\nAssuming that you leave the name tododomain-registry-credentials, you will need to run a kubectl create secret command similar to the following:\n$ kubectl create secret docker-registry tododomain-registry-credentials \\ --docker-server=phx.ocir.io \\ --docker-email=your.name@example.com \\ --docker-username=tenancy/username \\ --docker-password='passwordForUsername' \\ --namespace=tododomain Update the application configuration Update the generated vz-application.yaml file for the todo application to:\n Update the tododomain-configmap component to use the in-cluster MySQL service URL jdbc:mysql://mysql.tododomain.svc.cluster.local:3306/tododb to access the database.  wdt_jdbc.yaml:|resources: JDBCSystemResource: 'todo-ds': JdbcResource: JDBCDriverParams: # This is the URL of the database used by the WebLogic Server application URL: \"jdbc:mysql://mysql.tododomain.svc.cluster.local:3306/tododb\" Update the tododomain-appconf ApplicationConfiguration to enable Verrazzano MySQL components to be deployed as Kubernetes objects.  apiVersion:core.oam.dev/v1alpha2kind:ApplicationConfigurationmetadata:name:tododomain-appconfnamespace:tododomainannotations:version:v1.0.0description:\"tododomain application configuration\"spec:components:- componentName:tododomain-domaintraits:- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:MetricsTraitspec:scraper:verrazzano-system/vmi-system-prometheus-0- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:IngressTraitspec:rules:- paths:# application todo- path:\"/todo\"pathType:Prefix- componentName:tododomain-configmap- componentName:todo-mysql-service- componentName:todo-mysql-deployment- componentName:todo-mysql-configmapThe file vz-application-modified.yaml is an example of a modified vz-application.yaml file. A diff of these two sample files is shown:\n$ diff vz-application.yaml vz-application-modified.yaml 30a31,33 \u003e - componentName: todo-mysql-service \u003e - componentName: todo-mysql-deployment \u003e - componentName: todo-mysql-configmap 102c105 \u003c URL: \"jdbc:mysql://localhost:3306/tododb\" --- \u003e URL: \"jdbc:mysql://mysql.tododomain.svc.cluster.local:3306/tododb\" Create Verrazzano components for MySQL As noted previously, moving a production environment to Verrazzano would require migrating the data as well. While data migration is beyond the scope of this guide, we will still need to include a MySQL instance to be deployed with the application in the Verrazzano environment.\nTo do so, first, we need to create the Verrazzano components for MySQL by applying the mysql-oam.yaml file in the tododomain namespace. The components will be deployed as Kubernetes objects when the ToDo List application is deployed by applying the vz-application.yaml file in the next step.\n  Download the mysql-oam.yaml file.\n  Then, apply the YAML file:\n  $ kubectl apply -f mysql-oam.yaml # Expected response component.core.oam.dev/todo-mysql-service created component.core.oam.dev/todo-mysql-deployment created component.core.oam.dev/todo-mysql-configmap created $ kubectl get components -ntododomain # Expected response todo-mysql-configmap ConfigMap 26s todo-mysql-deployment Deployment 26s todo-mysql-service Service 26s Deploy the ToDo List application and MySQL instance. Finally, run kubectl apply to apply the Verrazzano components and Verrazzano application configuration files to start your domain.\n$ kubectl apply -f vz-application.yaml This will:\n Create the application Component resources for the ToDo List application. Deploys the Verrazzano component resources as Kubernetes objects and creates the MySQL instance. Create the application configuration resources that create the instance of the ToDo List application in the Verrazzano cluster.  Wait for the ToDo List example application to be ready.\n$ kubectl wait pod \\ --for=condition=Ready tododomain-adminserver \\ -n tododomain # Expected response pod/tododomain-adminserver condition met Verify that the pods are in the Running state:\n$ kubectl get pod -n tododomain # Sample output NAME READY STATUS RESTARTS AGE mysql-55bb4c4565-c8zf5 1/1 Running 0 8m tododomain-adminserver 4/4 Running 0 5m Access the application from your browser   Get the generated host name for the application.\n$ kubectl get gateways.networking.istio.io tododomain-tododomain-appconf-gw \\ -n tododomain \\ -o jsonpath={.spec.servers[0].hosts[0]} # Sample output tododomain-appconf.tododomain.11.22.33.44.nip.io   Initialize the database by accessing the init URL.\nhttps://tododomain-appconf.tododomain.11.22.33.44.nip.io/todo/rest/items/init   Access the application.\nhttps://tododomain-appconf.tododomain.11.22.33.44.nip.io/todo   Access the WebLogic Server Administration Console   Set up port forwarding.\n$ kubectl port-forward pods/tododomain-adminserver 7001:7001 -n tododomain NOTE: If you are using the Oracle Cloud Infrastructure Cloud Shell to run kubectl, in order to access the WebLogic Server Administration Console using port forwarding, you will need to run kubectl on another machine.\n  Access the WebLogic Server Administration Console from your browser.\nhttp://localhost:7001/console   NOTE It is recommended that the WebLogic Server Administration Console not be exposed publicly.  ","categories":"","description":"A guide for moving WebLogic Server domains to Verrazzano","excerpt":"A guide for moving WebLogic Server domains to Verrazzano","ref":"/docs/guides/lift-and-shift/lift-and-shift/","tags":"","title":"Lift-and-Shift Guide"},{"body":"The LoggingTrait custom resource contains the configuration for an additional logging sidecar with a custom image and Fluentd configuration file. Here is a sample ApplicationConfiguration that includes a LoggingTrait. To deploy an example application with this LoggingTrait, replace the ApplicationConfiguration of the ToDo-List example application with the following sample.\napiVersion:core.oam.dev/v1alpha2kind:ApplicationConfigurationmetadata:name:todo-appconfnamespace:todo-listannotations:version:v1.0.0description:\"ToDo List example application\"spec:components:- componentName:todo-domaintraits:- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:LoggingTraitmetadata:name:logging-trait-examplenamespace:todo-listspec:loggingImage:fluent/fleuntd-example-image# Replace with custom Fluentd ImageloggingConfig:|-# Replace with Fluentd config file \u003cmatch **\u003e @type stdout \u003c/match\u003e- componentName:todo-jdbc-configmap- componentName:todo-mysql-configmap- componentName:todo-mysql-service- componentName:todo-mysql-deploymentIn this sample configuration, the LoggingTrait logging-trait-example is set on the todo-domain application component and defines a logging sidecar with the given Fluentd image and configuration file. This sidecar will be attached to the component’s pod and will gather logs according to the given Fluentd configuration file. In order for the Fluentd DaemonSet to collect the custom logs, the Fluentd configuration file needs to direct the logs to STDOUT, as demonstrated in the previous example.\nFor example, when the ToDo-List example ApplicationConfiguration is successfully deployed with a LoggingTrait, the tododomain-adminserver pod will have a container named logging-stdout.\n$ kubectl get pods tododomain-adminserver -n todo-list -o jsonpath='{.spec.containers[*].name}'  ... logging-stdout ... In this example, the logging-stdout container will run the image given in the LoggingTrait and a ConfigMap named logging-stdout-todo-domain-domain will be created with the custom Fluentd configuration file.\nLoggingTrait    Field Type Description Required     apiVersion string oam.verrazzano.io/v1alpha1 Yes   kind string LoggingTrait Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec LoggingTraitSpec The desired state of a logging trait. Yes    LoggingTraitSpec LoggingTraitSpec specifies the desired state of a logging trait.\n   Field Type Description Required     loggingConfig string A string representation of the Fluentd configuration. Yes   loggingImage string The name of the custom Fluentd image. Yes    ","categories":"","description":"","excerpt":"The LoggingTrait custom resource contains the configuration for an additional logging sidecar with a custom image and Fluentd configuration file. Here is a sample ApplicationConfiguration that …","ref":"/docs/reference/api/oam/loggingtrait/","tags":"","title":"LoggingTrait Custom Resource Definition"},{"body":"The Metrics Template CRD contains the metrics configuration for default Kubernetes workloads. Here is the default Metrics Template that Verrazzano installs.\napiVersion: app.verrazzano.io/v1alpha1 kind: MetricsTemplate metadata: name: standard-k8s-metrics-template namespace: verrazzano-system spec: workloadSelector: apiGroups: [\"apps\", \"\"] apiVersions: [\"v1\"] resources: [\"deployment\", \"statefulset\", \"replicaset\", \"pod\"] prometheusConfig: targetConfigMap: namespace: verrazzano-system name: vmi-system-prometheus-config scrapeConfigTemplate: | kubernetes_sd_configs: - namespaces: names: - {{`{{.workload.metadata.namespace}}`}} role: pod relabel_configs: - action: replace replacement: local source_labels: null target_label: verrazzano_cluster - action: keep regex: {{`{{index .workload.metadata.labels \"app.verrazzano.io/workload\"}}`}};true source_labels: - __meta_kubernetes_pod_label_app_verrazzano_io_workload - __meta_kubernetes_pod_annotation_prometheus_io_scrape - action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 source_labels: - __address__ - __meta_kubernetes_pod_annotation_prometheus_io_port target_label: __address__ - action: replace regex: (.*) source_labels: - __meta_kubernetes_pod_annotation_prometheus_io_path target_label: __metrics_path__ - action: replace regex: (.*) replacement: $1 source_labels: - __meta_kubernetes_namespace target_label: namespace - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod_name - action: labeldrop regex: (controller_revision_hash) - action: replace regex: .*/(.*)$ replacement: $1 source_labels: - name target_label: webapp {{`{{ if index .namespace.metadata.labels \"istio-injection\" }}`}} {{`{{ if eq (index .namespace.metadata.labels \"istio-injection\" ) \"enabled\" }}`}} scheme: https tls_config: ca_file: /etc/istio-certs/root-cert.pem cert_file: /etc/istio-certs/cert-chain.pem insecure_skip_verify: true key_file: /etc/istio-certs/key.pem {{`{{ end }}`}} {{`{{ end }}`}} For more information on using the Metrics Template, see Metrics Template.\nMetricsTemplate    Field Type Description Required     apiVersion string app.verrazzano.io/v1alpha1 Yes   kind string MetricsTemplate Yes   metadata ObjectMeta Refer to the Kubernetes API documentation for fields of metadata. No   spec MetricsTemplateSpec The desired state of a metrics trait. Yes    MetricsTemplateSpec    Field Type Description Required     workloadSelector WorkloadSelector Selector for target workloads. No   prometheusConfig PrometheusConfig Prometheus configuration details. No    WorkloadSelector    Field Type Description Required     namespaceSelector LabelSelector Scopes the template to a namespace. No   objectSelector LabelSelector Scopes the template to a specific workload object. No   apiGroups []string Scopes the template to given API Groups. No   apiVersions []string Scopes the template to given API Versions. No   resources []string Scopes the template to given API Resources. No    PrometheusConfig    Field Type Description Required     targetConfigMap TargetConfigMap Identity of the ConfigMap to be updated with the scrape configuration specified in scrapeConfigTemplate. Yes   scrapeConfigTemplate string Scrape configuration template to be added to the Prometheus configuration. Yes    TargetConfigMap    Field Type Description Required     namespace string Namespace of the ConfigMap to be updated with the scrape target configuration. Yes   name string Name of the ConfigMap to be updated with the scrape target configuration. Yes    ","categories":"","description":"","excerpt":"The Metrics Template CRD contains the metrics configuration for default Kubernetes workloads. Here is the default Metrics Template that Verrazzano installs.\napiVersion: app.verrazzano.io/v1alpha1 …","ref":"/docs/reference/api/verrazzano/metricstemplate/","tags":"","title":"Metrics Template Custom Resource Definition"},{"body":"The MetricsTrait custom resource contains the configuration information needed to enable metrics for an application component. Component workloads configured with a MetricsTrait are setup to emit metrics through an endpoint that are scraped by a given Prometheus deployment. Here is a sample ApplicationConfiguration that specifies a MetricsTrait. To deploy an example application that demonstrates a MetricsTrait, see Hello World Helidon.\nNote that if an ApplicationConfiguration does not specify a MetricsTrait, then a default MetricsTrait will be generated with values appropriate for the workload type.\napiVersion: core.oam.dev/v1alpha2 kind: ApplicationConfiguration metadata: name: hello-helidon-appconf namespace: hello-helidon annotations: version: v1.0.0 description: \"Hello Helidon application\" spec: components: - componentName: hello-helidon-component traits: - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: MetricsTrait spec: scraper: verrazzano-system/vmi-system-prometheus-0 - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: IngressTrait metadata: name: hello-helidon-ingress spec: rules: - paths: - path: \"/greet\" pathType: Prefix In the sample configuration, a MetricsTrait is specified for the hello-helidon-component application component.\nWith the sample application configuration successfully deployed, you can query for metrics from the application component.\n$ HOST=$(kubectl get ingress \\ -n verrazzano-system vmi-system-prometheus \\ -o jsonpath={.spec.rules[0].host}) $ echo $HOST prometheus.vmi.system.default.\u003cip\u003e.nip.io $ VZPASS=$(kubectl get secret \\ --namespace verrazzano-system verrazzano \\ -o jsonpath={.data.password} | base64 \\ --decode; echo) $ curl -sk \\ --user verrazzano:${VZPASS} \\ -X GET https://${HOST}/api/v1/query?query=vendor_requests_count_total {\"status\":\"success\",\"data\":{\"resultType\":\"vector\",\"result\":[{\"metric\":{\"__name__\":\"vendor_requests_count_total\",\"app\":\"hello-helidon\",\"app_oam_dev_component\":\"hello-helidon-component\",\"app_oam_dev_name\":\"hello-helidon-appconf\",\"app_oam_dev_resourceType\":\"WORKLOAD\",\"app_oam_dev_revision\":\"hello-helidon-component-v1\",\"containerizedworkload_oam_crossplane_io\":\"496df78f-ef8b-4753-97fd-d9218d2f38f1\",\"job\":\"hello-helidon-appconf_default_helidon-logging_hello-helidon-component\",\"namespace\":\"helidon-logging\",\"pod_name\":\"hello-helidon-workload-b7d9d95d8-ht7gb\",\"pod_template_hash\":\"b7d9d95d8\"},\"value\":[1616535232.487,\"4800\"]}]}} MetricsTrait    Field Type Description Required     apiVersion string oam.verrazzano.io/v1alpha1 Yes   kind string MetricsTrait Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec MetricsTraitSpec The desired state of a metrics trait. Yes    MetricsTraitSpec MetricsTraitSpec specifies the desired state of a metrics trait.\n   Field Type Description Required     port integer The HTTP port for the related metrics endpoint. Defaults to 8080. No   path string The HTTP path for the related metrics endpoint. Defaults to /metrics. No   secret string The name of an opaque secret (for example, user name and password) within the workload’s namespace for metrics endpoint access. No   scraper string The Prometheus deployment used to scrape the related metrics endpoints. Defaults to verrazzano-system/vmi-system-prometheus-0. No    ","categories":"","description":"","excerpt":"The MetricsTrait custom resource contains the configuration information needed to enable metrics for an application component. Component workloads configured with a MetricsTrait are setup to emit …","ref":"/docs/reference/api/oam/metricstrait/","tags":"","title":"MetricsTrait Custom Resource Definition"},{"body":"The MultiClusterApplicationConfiguration custom resource is an envelope used to distribute core.oam.dev/v1alpha2/ApplicationConfiguration resources in a multicluster environment.\nHere is a sample MultiClusterApplicationConfiguration that specifies an ApplicationConfiguration resource to create on the cluster named managed1. To deploy an example application that demonstrates a MultiClusterApplicationConfiguration, see Multicluster ToDo List.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: MultiClusterApplicationConfiguration metadata: name: todo-appconf namespace: mc-todo-list spec: template: metadata: annotations: version: v1.0.0 description: \"ToDo List example application\" spec: components: - componentName: todo-domain traits: - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: MetricsTrait spec: scraper: verrazzano-system/vmi-system-prometheus-0 - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: IngressTrait spec: rules: - paths: - path: \"/todo\" pathType: Prefix - componentName: todo-jdbc-config - componentName: mysql-initdb-config - componentName: todo-mysql-service - componentName: todo-mysql-deployment placement: clusters: - name: managed1 secrets: - tododomain-repo-credentials - tododomain-jdbc-tododb - tododomain-weblogic-credentials MultiClusterApplicationConfiguration A MultiClusterApplicationConfiguration is an envelope to create core.oam.dev/v1alpha2/ApplicationConfiguration resources on the clusters specified in the placement section.\n   Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string MultiClusterApplicationConfiguration Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec MultiClusterApplicationConfigurationSpec The desired state of a core.oam.dev/v1alpha2/ApplicationConfiguration resource. Yes   status MultiClusterResourceStatus The runtime status of a multicluster resource. No    MultiClusterApplicationConfigurationSpec MultiClusterApplicationConfigurationSpec specifies the desired state of a core.oam.dev/v1alpha2/ApplicationConfiguration resource.\n   Field Type Description Required     template ApplicationConfigurationTemplate The embedded core.oam.dev/v1alpha2/ApplicationConfiguration resource. Yes   placement Placement Clusters in which the resource is to be placed. Yes   secrets string array List of secrets used by the application. These secrets must be created in the application’s namespace before deploying a MultiClusterApplicationConfiguration resource. No    ApplicationConfigurationTemplate ApplicationConfigurationTemplate has the metadata and spec of the core.oam.dev/v1alpha2/ApplicationConfiguration resource.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec ApplicationConfigurationSpec An instance of the struct ApplicationConfigurationSpec defined in core_types.go. No    ","categories":"","description":"","excerpt":"The MultiClusterApplicationConfiguration custom resource is an envelope used to distribute core.oam.dev/v1alpha2/ApplicationConfiguration resources in a multicluster environment.\nHere is a sample …","ref":"/docs/reference/api/multicluster/multiclusterapplicationconfiguration/","tags":"","title":"MultiClusterApplicationConfiguration Custom Resource Definition"},{"body":"The MultiClusterComponent custom resource is an envelope used to distribute core.oam.dev/v1alpha2/Component resources in a multicluster environment.\nNOTE: Starting with Verrazzano v1.1.0, it is preferred that the MultiClusterComponent custom resource not be used; instead directly use core.oam.dev/v1alpha2/Component resources in your application. See the example application, Multicluster ToDo List, which directly uses core.oam.dev/v1alpha2/Component resources.\nHere is a sample MultiClusterComponent that specifies a OAM Component resource to create on the cluster named managed1.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: MultiClusterComponent metadata: name: hello-helidon-component namespace: hello-helidon spec: template: spec: workload: apiVersion: oam.verrazzano.io/v1alpha1 kind: VerrazzanoHelidonWorkload metadata: name: hello-helidon-workload namespace: hello-helidon labels: app: hello-helidon spec: deploymentTemplate: metadata: name: hello-helidon-deployment podSpec: containers: - name: hello-helidon-container image: \"ghcr.io/verrazzano/example-helidon-greet-app-v1:0.1.12-1-20210409130027-707ecc4\" ports: - containerPort: 8080 name: http placement: clusters: - name: managed1 MultiClusterComponent A MultiClusterComponent is an envelope to create core.oam.dev/v1alpha2/Component resources on the clusters specified in the placement section.\n   Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string MultiClusterComponent Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec MultiClusterComponentSpec The desired state of a core.oam.dev/v1alpha2/Component resource. Yes   status MultiClusterResourceStatus The runtime status of a multicluster resource. No    MultiClusterComponentSpec MultiClusterComponentSpec specifies the desired state of a core.oam.dev/v1alpha2/Component resource.\n   Field Type Description Required     template ComponentTemplate The embedded core.oam.dev/v1alpha2/Component resource. Yes   placement Placement Clusters in which the resource is to be placed. Yes    ComponentTemplate ComponentTemplate has the metadata and spec of the core.oam.dev/v1alpha2/Component resource.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec ComponentSpec An instance of the struct ComponentSpec defined in core_types.go. No    ","categories":"","description":"","excerpt":"The MultiClusterComponent custom resource is an envelope used to distribute core.oam.dev/v1alpha2/Component resources in a multicluster environment.\nNOTE: Starting with Verrazzano v1.1.0, it is …","ref":"/docs/reference/api/multicluster/multiclustercomponent/","tags":"","title":"MultiClusterComponent Custom Resource Definition"},{"body":"The MultiClusterConfigMap custom resource is an envelope used to distribute Kubernetes ConfigMap resources in a multicluster environment.\nNOTE: Starting with Verrazzano v1.1.0, it is preferred that the MultiClusterConfigMap custom resource not be used; instead directly use core.oam.dev/v1alpha2/Component to define ConfigMap resources in your application. See the example application, Multicluster ToDo List, which uses core.oam.dev/v1alpha2/Component resources to define ConfigMaps.\nHere is a sample MultiClusterConfigMap that specifies a Kubernetes ConfigMap to create on the cluster named managed1.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: MultiClusterConfigMap metadata: name: mymcconfigmap namespace: multiclustertest spec: template: metadata: name: myconfigmap namespace: myns data: simple.key: \"simplevalue\" placement: clusters: - name: managed1 MultiClusterConfigMap A MultiClusterConfigMap is an envelope to create Kubernetes ConfigMap resources on the clusters specified in the placement section.\n   Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string MultiClusterConfigMap Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec MultiClusterConfigMapSpec The desired state of a Kubernetes ConfigMap. Yes   status MultiClusterResourceStatus The runtime status of a multicluster resource. No    MultiClusterConfigMapSpec MultiClusterConfigMapSpec specifies the desired state of a Kubernetes ConfigMap.\n   Field Type Description Required     template ConfigMapTemplate The embedded Kubernetes ConfigMap. Yes   placement Placement Clusters in which the ConfigMap is to be placed. Yes    ConfigMapTemplate ConfigMapTemplate has the metadata and spec of the Kubernetes ConfigMap.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   immutable *bool Corresponds to the immutable field of the struct ConfigMap defined in types.go. No   data map[string]string Corresponds to the data field of the struct ConfigMap defined in types.go. No   binaryData map[string][]byte Corresponds to the binaryData field of the struct ConfigMap defined in types.go. No    ","categories":"","description":"","excerpt":"The MultiClusterConfigMap custom resource is an envelope used to distribute Kubernetes ConfigMap resources in a multicluster environment.\nNOTE: Starting with Verrazzano v1.1.0, it is preferred that …","ref":"/docs/reference/api/multicluster/multiclusterconfigmap/","tags":"","title":"MultiClusterConfigMap Custom Resource Definition"},{"body":"The MultiClusterResourceStatus subresource is shared by multicluster custom resources.\nMultiClusterResourceStatus MultiClusterResourceStatus specifies the status portion of a multicluster resource.\n   Field Type Description Required     conditions Condition array The current state of a multicluster resource. No   state string The state of the multicluster resource. State values are case-sensitive and formatted as follows: Pending: deployment to cluster is in progressSucceeded: deployment to cluster successfully completedFailed: deployment to cluster failed No   clusters ClusterLevelStatus array Array of status information for each cluster. No    Condition Condition describes current state of a multicluster resource across all clusters.\n   Field Type Description Required     type string The condition of the multicluster resource which can be checked with a kubectl wait command. Condition values are case-sensitive and formatted as follows: DeployComplete: deployment to all clusters completed successfullyDeployFailed: deployment to all clusters failed Yes   status ConditionStatus An instance of the type ConditionStatus that is defined in types.go. Yes   lastTransitionTime string The last time the condition transitioned from one status to another. No   message string A message with details about the last transition. No    ClusterLevelStatus ClusterLevelStatus describes the status of the multicluster resource on an individual cluster.\n   Field Type Description Required     name string Name of the cluster. Yes   state string The state of the multicluster resource. State values are case-sensitive and formatted as follows: Pending: deployment is in progressSucceeded: deployment successfully completedFailed: deployment failed No   message string Message with details about the status in this cluster. No   lastUpdateTime string The last time the resource state was updated. Yes    ","categories":"","description":"","excerpt":"The MultiClusterResourceStatus subresource is shared by multicluster custom resources.\nMultiClusterResourceStatus MultiClusterResourceStatus specifies the status portion of a multicluster resource. …","ref":"/docs/reference/api/multicluster/multiclusterresourcestatus/","tags":"","title":"MultiClusterResourceStatus Subresource"},{"body":"The MultiClusterSecret custom resource is an envelope used to distribute Kubernetes Secret resources in a multicluster environment.\nNOTE: Starting with Verrazzano v1.1.0, it is preferred that the MultiClusterSecret custom resource not be used; instead specify secrets in the MultiClusterApplicationConfiguration resource. See the example application, Multicluster ToDo List where secrets are specified in a MultiClusterApplicationConfiguration resource.\nHere is a sample MultiClusterSecret that specifies a Kubernetes secret to create on the cluster named managed1.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: MultiClusterSecret metadata: name: mymcsecret namespace: multiclustertest spec: template: data: username: dmVycmF6emFubw== password: dmVycmF6emFubw== spec: placement: clusters: - name: managed1 MultiClusterSecret A MultiClusterSecret is an envelope to create Kubernetes Secret resources on the clusters specified in the placement section.\n   Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string MultiClusterSecret Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec MultiClusterSecretSpec The desired state of a Kubernetes Secret. Yes   status MultiClusterResourceStatus The runtime status of a multicluster resource. No    MultiClusterSecretSpec MultiClusterSecretSpec specifies the desired state of a Kubernetes Secret.\n   Field Type Description Required     template SecretTemplate The embedded Kubernetes Secret. Yes   placement Placement Clusters in which the Secret is to be placed. Yes    SecretTemplate SecretTemplate has the metadata and spec of the Kubernetes Secret.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   data map[string][]byte Corresponds to the data field of the struct Secret defined in types.go. No   stringData map[string]string Corresponds to the stringData field of the struct Secret defined in types.go. No   type string Corresponds to the type field of the struct Secret defined in types.go. No    ","categories":"","description":"","excerpt":"The MultiClusterSecret custom resource is an envelope used to distribute Kubernetes Secret resources in a multicluster environment.\nNOTE: Starting with Verrazzano v1.1.0, it is preferred that the …","ref":"/docs/reference/api/multicluster/multiclustersecret/","tags":"","title":"MultiClusterSecret Custom Resource Definition"},{"body":"Network traffic refers to the data flowing across the network. In the context of this document, it is useful to think of network traffic from two perspectives: traffic based on direction and traffic related to component types, system or applications. Traffic direction is either north-south traffic, which enters and leaves the cluster, or east-west traffic, which stays within the cluster.\nFirst is a description of getting traffic into the cluster, then how traffic flows after it is in the cluster.\nIngress Ingress is an overloaded term, so it needs to be understood in context. Sometimes the term means external access into the cluster, as in “ingress to the cluster.” The term also refers to the Kubernetes Ingress resource. In addition, it might be used to mean network ingress to a container in a Pod. Here, it’s used to refer to both general ingress into the cluster and the Kubernetes Ingress resource.\nDuring installation, Verrazzano creates the necessary network resources to access both system components and applications. The following ingress and load balancers description is in the context of a Verrazzano installation.\nLoadBalancer Services To reach Pods from outside a cluster, an external IP address must be exposed using a LoadBalancer or NodePort service. Verrazzano creates two LoadBalancer services, one for system component traffic and another for application traffic. The specifics of how the service gets traffic into the cluster depends on the underlying Kubernetes platform. With Oracle OKE, creating a LoadBalancer type service will result in an Oracle Cloud Infrastructure load balancer being created and configured to load balance to a set of Pods.\nIngress for system components To provide ingress to system components, Verrazzano installs a NGINX Ingress Controller, which includes a NGINX load balancer. Verrazzano also creates Kubernetes Ingress resources to configure ingress for each system component that requires ingress. An Ingress resource is used is to specify HTTP/HTTPS routes to Kubernetes services, along with an endpoint hostname and a TLS certificate. An Ingress by itself doesn’t do anything; it is just a resource. An ingress controller is needed to watch Ingress resources and reconcile them, configuring the underlying Kubernetes load balancer to handle the service routing. The NGINX Ingress Controller processes Ingress resources and configures NGINX with the ingress route information, and such.\nThe NGINX Ingress Controller is a LoadBalancer service, as seen here:\n$ kubectl get service -n ingress-nginx # Sample output ingress-controller-ingress-nginx-controller LoadBalancer Using the OKE example, traffic entering the Oracle Cloud Infrastructure load balancer is routed to the NGINX load balancer, then routed from there to the Pods belonging to the services described in the Ingress.\nIngress for applications Verrazzano also provides ingress into applications, but uses an Istio ingress gateway, which is an Envoy proxy, instead of NGINX. Istio has a Gateway resource that provides load balancer information, such as hosts, ports, and certificates for traffic coming into the mesh. For more information, see Istio Gateway. Just as an Ingress needs a corresponding Ingress controller, the same is true for the Gateway resource, where there is a corresponding Istio ingress gateway controller. However, unlike the Ingress, the Gateway resource doesn’t have service routing information. That is handled by the Istio VirtualService resource. The combination of Gateway and VirtualService is basically a superset of Ingress, because the combination provides more features than Ingress. In summary, the Istio ingress gateway provides ingress to the cluster using information from both the Gateway and VirtualService resources.\nBecause Verrazzano doesn’t create any applications during installations, there is no need to create a Gateway and VirtualService at that time. However, during installation, Verrazzano does create the Istio ingress gateway, which is a LoadBalancer service, along with the Istio egress gateway, which is a ClusterIP service.\n$ kubectl get service -n istio-system # Sample output istio-ingressgateway LoadBalancer Again, referring to the OKE use case, this means that there will another Oracle Cloud Infrastructure load balancer created, routing traffic to the Istio ingress gateway Pod, for example, the Envoy proxy.\nExternal DNS When you install Verrazzano, you can optionally specify an external DNS for your domain. If you do that, Verrazzano will not only create the DNS records, using ExternalDNS, but also it will configure your host name in the Ingress resources. You can then use that host name to access the system components through the NGINX Ingress Controller.\nSystem traffic System traffic includes all traffic that enters and leaves system Pods.\nNorth-south system traffic North-south traffic includes all system traffic that enters or leaves a Kubernetes cluster.\nIngress The following lists the Verrazzano system components which are accessed through the NGINX Ingress Controller from a client external to the cluster:\n OpenSearch Keycloak OpenSearch Dashboards Grafana Prometheus Rancher Verrazzano Console Verrazzano API  Egress The following table shows Verrazzano system components that initiate requests to a destination outside the cluster.\n   Component Destination Description     cert-manager Let’s Encrypt Get signed certificate.   ExternalDNS External DNS Create and delete DNS entries in an external DNS.   Fluentd OpenSearch Fluentd on the managed cluster calls OpenSearch on the admin cluster.   Prometheus Prometheus Prometheus on the admin cluster scrapes metrics from Prometheus on the managed cluster.   Rancher Agent Rancher Rancher agent on the managed cluster sends requests to Rancher on the admin cluster.   Verrazzano Authentication Proxy Keycloak Calls Keycloak for authentication, which includes redirects.   Verrazzano Platform Operator Kubernetes API server Multicluster agent on the managed cluster calls API server on the admin cluster.    East-west system traffic The following tables show Verrazzano system components that send traffic to a destination inside the cluster, with the following exceptions:\n Usage of CoreDNS: It can be assumed that any Pod in the cluster can access CoreDNS for name resolution. Envoy to Istiod: The Envoy proxies all make requests to the Istio control plane to get dynamic configuration, and such. This includes both the gateways and the mesh sidecar proxies. That traffic is not shown. Traffic within a component is not shown, for example, traffic between OpenSearch Pods. Prometheus scraping traffic is shown in the second table.     Component Destination Description     cert-manager Kubernetes API server Perform CRUD operations on Kubernetes resources.   Fluentd OpenSearch Fluentd sends data to OpenSearch.   Grafana Prometheus UI for Prometheus data.   OpenSearch Dashboards OpenSearch UI for OpenSearch.   NGINX Ingress Controller Kubernetes API server Perform CRUD operations on Kubernetes resources.   Istio Kubernetes API server Perform CRUD operations on Kubernetes resources.   Rancher Kubernetes API server Perform CRUD operations on Kubernetes resources.   Verrazzano Authentication Proxy Keycloak Calls Keycloak for token authentication.   Verrazzano Authentication Proxy VMI components Access UIs for OpenSearch Dashboards, Grafana, and such.   Verrazzano Authentication Proxy Kubernetes API server Perform CRUD operations on Kubernetes resources.   Verrazzano Application Operator Kubernetes API server Perform CRUD operations on Kubernetes resources.   Verrazzano Monitoring Operator Kubernetes API server Perform CRUD operations on Kubernetes resources.   Verrazzano Operator Kubernetes API server Perform CRUD operations on Kubernetes resources.   Verrazzano Platform Operator Kubernetes API server Perform CRUD operations on Kubernetes resources.   Verrazzano Platform Operator Rancher Register the managed cluster with Rancher.    Prometheus scraping traffic This table shows Prometheus traffic for each system component scrape target.\n   Target Description     cadvisor Kubernetes metrics.   OpenSearch Envoy metrics.   Grafana Envoy metrics.   Istiod Istio control plane metrics.   Istiod Envoy metrics.   Istio egress gateway Envoy metrics.   Istio ingress gateway Envoy metrics.   Keycloak Envoy metrics.   OpenSearch Dashboards Envoy metrics.   MySQL Envoy metrics.   NGINX Ingress Controller Envoy metrics.   NGINX Ingress Controller NGINX metrics.   NGINX default back end Envoy metrics.   Node exporter Node metrics.   Prometheus Envoy metrics.   Prometheus Prometheus metrics.   Verrazzano Console Envoy metrics.   Verrazzano API Envoy metrics.   WebLogic operator Envoy metrics.    Webhooks Several of the system components are controllers, and some of those have webhooks. Webhooks are called by the Kubernetes API server on a component HTTPS port to validate or mutate API payloads before they reach the API server.\nThe following components use webhooks:\n cert-manager Coherence Operator Istio Rancher Verrazzano Application Operator Verrazzano Platform Operator  Application traffic Application traffic includes all traffic to and from Verrazzano applications.\nNorth-south application traffic After Verrazzano is installed, you can deploy applications into the Istio mesh. When doing so, you will likely need ingress into the application. As previously mentioned, this can be done with Istio using the Gateway and VirtualService resources. Verrazzano will create those resources for you when you use an IngressTrait in your ApplicationConfiguration. The Istio ingress gateway created during installation will be shared by all applications in the mesh, and the Gateway resource is bound to the Istio ingress gateway that was created during installation. This is done by the selector field in the Gateway:\n selector: istio: ingressgateway Verrazzano creates a Gateway/VirtualService pair for each IngressTrait. Following is an example of those two resources created by Verrazzano.\nHere is the Gateway; in this case both the host name and certificate were generated by Verrazzano.\napiVersion: v1 items: - apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: ... name: hello-helidon-hello-helidon-appconf-gw namespace: hello-helidon ... spec: selector: istio: ingressgateway servers: - hosts: - hello-helidon-appconf.hello-helidon.1.2.3.4.nip.io port: name: HTTPS number: 443 protocol: HTTPS tls: credentialName: hello-helidon-hello-helidon-appconf-cert-secret mode: SIMPLE Here is the VirtualService; notice that it refers back to the Gateway and that it contains the service routing information.\napiVersion: v1 items: - apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: ... name: hello-helidon-ingress-rule-0-vs namespace: hello-helidon spec: gateways: - hello-helidon-hello-helidon-appconf-gw hosts: - hello-helidon-appconf.hello-helidon.1.2.3.4.nip.io HTTP: - match: - uri: prefix: /greet route: - destination: host: hello-helidon port: number: 8080 East-west application traffic To manage east-west traffic, each service in the mesh should be routed using a VirtualService and an optional DestinationRule. You can still send east-west traffic without either of these resources, but you won’t get any custom routing or load balancing. Verrazzano doesn’t configure east-west traffic. Consider bobbys-front-end in the Bob’s Books example at bobs-books-comp.yaml. When deploying Bob’s Books, a VirtualService is created for bobbys-front-end, because of the IngressTrait, but there are no VirtualServices for the other services in the application. When bobbys-front-end sends requests to bobbys-helidon-stock-application, this east-west traffic still goes to bobbys-helidon-stock-application through the Envoy sidecar proxies in the source and destination Pods, but there is no VirtualService representing bobbys-helidon-stock-application, where you could specify a canary deployment or custom load balancing. This is something you could configure manually, but it is not configured by Verrazzano.\nProxies Verrazzano uses network proxies in multiple places. The two proxy products are Envoy and NGINX. The following table shows which proxies are used and in which Pod they run.\n   Usage Proxy Pod Namespace Description     System ingress NGINX ingress-controller-ingress-nginx-controller-* ingress-nginx Provides external access to Verrazzano system components.   Verrazzano authentication proxy NGINX verrazzano-authproxy-* verrazzano-system Verrazzano authentication proxy server for Kubernetes API and SSO.   Application ingress Envoy istio-ingressgateway-* istio-system Provides external access to Verrazzano applications.   Application egress Envoy istio-egressgateway-* istio-system Provides control of application egress traffic.   Istio mesh sidecar Envoy ingress-controller-ingress-nginx-controller-* ingress-nginx NGINX Ingress Controller in the Istio mesh.   Istio mesh sidecar Envoy ingress-controller-ingress-nginx-defaultbackend-* ingress-nginx NGINX default backend in the Istio mesh.   Istio mesh sidecar Envoy fluentd-* verrazzano-system Fluentd in the Istio mesh.   Istio mesh sidecar Envoy keycloak-* keycloak Keycloak in the Istio mesh.   Istio mesh sidecar Envoy mysql-* keycloak MySQL used by Keycloak in the Istio mesh.   Istio mesh sidecar Envoy verrazzano-api-* verrazzano-system Verrazzano API in the Istio mesh.   Istio mesh sidecar Envoy verrazzano-console-* verrazzano-system Verrazzano Console in the Istio mesh.   Istio mesh sidecar Envoy vmi-system-es-master-* verrazzano-system OpenSearch in the Istio mesh.   Istio mesh sidecar Envoy vmi-system-es-data-* verrazzano-system OpenSearch in the Istio mesh.   Istio mesh sidecar Envoy vmi-system-es-ingest-* verrazzano-system OpenSearch in the Istio mesh.   Istio mesh sidecar Envoy vmi-system-kibana-* verrazzano-system OpenSearch Dashboards in the Istio mesh.   Istio mesh sidecar Envoy vmi-system-prometheus-* verrazzano-system Prometheus in the Istio mesh.   Istio mesh sidecar Envoy vmi-system-grafana-* verrazzano-system Grafana in the Istio mesh.   Istio mesh sidecar Envoy weblogic-operator-* verrazzano-system WebLogic operator in the Istio mesh.    Multicluster Some Verrazzano components send traffic between Kubernetes clusters. Those components are the Verrazzano agent, Verrazzano authentication proxy, and Prometheus.\nMulticluster egress The following table shows Verrazzano system components that initiate requests between the admin and managed clusters. All of these requests go through the NGINX Ingress Controller on the respective destination cluster.\n   Source Cluster Source Component Destination Cluster Destination Component Description     Admin Prometheus Managed Prometheus Scape metrics on managed clusters.   Admin Verrazzano Console Managed Verrazzano Authentication Proxy Admin cluster proxy sends Kubernetes API requests to managed cluster proxy.   Managed Fluentd Admin OpenSearch Fluentd sends logs to OpenSearch.   Managed Rancher Agent Admin Rancher Rancher Agent sends requests Rancher.   Managed Verrazzano Authentication Proxy Admin Keycloak Proxy sends requests to Keycloak.   Managed Verrazzano Agent Admin Kubernetes API server Agent, in the platform operator, sends requests Kubernetes API server.    Verrazzano agent In the multicluster topology, the Verrazzano platform operator has an agent thread running on the managed cluster that sends requests to the Kubernetes API server on the admin cluster. The URL for the admin cluster Kubernetes API server is registered on the managed cluster by the user.\nVerrazzano authentication proxy In a multicluster topology, the Verrazzano authentication proxy runs on both the admin and managed clusters.\nOn the admin cluster, the authentication proxy connects to in-cluster Keycloak, using the Keycloak Service. On the managed cluster, the authentication proxy connects to Keycloak on the admin cluster through the NGINX Ingress Controller running on the admin cluster.\nFor SSO, the authentication proxy also needs to send requests to Keycloak, either in-cluster or through the cluster ingress. When a request comes into the authentication proxy without an authentication header, the proxy sends a request to Keycloak through the NGINX Ingress Controller, so the request exits the cluster. Otherwise, if the authentication proxy is on the admin cluster, then the request is sent directly to Keycloak within the cluster. If the authentication proxy is on the managed cluster, then it must send requests to Keycloak on the admin cluster.\nPrometheus A single Prometheus service in the cluster, scrapes metrics from Pods in system components and applications. It also scrapes Pods in the Istio mesh using HTTPS, and outside the mesh using HTTP. In the multicluster case, the Prometheus on the admin cluster, scrapes metrics from Prometheus on the managed cluster, through the NGINX Ingress Controller on the managed cluster.\n","categories":"","description":"Understand Verrazzano network traffic","excerpt":"Understand Verrazzano network traffic","ref":"/docs/networking/traffic/net-traffic/","tags":"","title":"Network Traffic"},{"body":"The Placement subresource is shared by multicluster custom resources.\nPlacement Placement contains the name of each cluster where this resource will be located.\n   Field Type Description Required     clusters Cluster array An array of cluster locations. Yes    Cluster Cluster contains the name of a single cluster.\n   Field Type Description Required     cluster string The name of a cluster. Yes    ","categories":"","description":"","excerpt":"The Placement subresource is shared by multicluster custom resources.\nPlacement Placement contains the name of each cluster where this resource will be located.\n   Field Type Description Required …","ref":"/docs/reference/api/multicluster/placement/","tags":"","title":"Placement Subresource"},{"body":"","categories":"","description":"How to prepare Kubernetes platforms before installing Verrazzano","excerpt":"How to prepare Kubernetes platforms before installing Verrazzano","ref":"/docs/setup/platforms/","tags":"","title":"Platform Setup"},{"body":"Prerequisites The Quick Start assumes that you have already installed a Kubernetes cluster. For instructions on preparing Kubernetes platforms for installing Verrazzano, see Platform Setup.\n Find the Verrazzano prerequisite requirements here. Review the list of the software versions supported and installed by Verrazzano. For detailed installation instructions, see the Installation Guide.  Install the Verrazzano platform operator Verrazzano provides a Kubernetes operator to manage the life cycle of Verrazzano installations. The operator works with a custom resource defined in the cluster. You can install, uninstall, and update Verrazzano installations by updating the Verrazzano custom resource. The Verrazzano platform operator controller will apply the configuration from the custom resource to the cluster for you.\nNOTE: If you just created the cluster, then you must wait until your nodes reach Ready status before installing Verrazzano.\nTo install the Verrazzano platform operator:\n  Deploy the Verrazzano platform operator.\n$ kubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/v1.2.2/operator.yaml   Wait for the deployment to complete.\n$ kubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator # Sample output deployment \"verrazzano-platform-operator\" successfully rolled out   Confirm that the operator pod is correctly defined and running.\n$ kubectl -n verrazzano-install get pods # Sample output NAME READY STATUS RESTARTS AGE verrazzano-platform-operator-59d5c585fd-lwhsx 1/1 Running 0 114s   Install Verrazzano You install Verrazzano by creating a Verrazzano custom resource in your Kubernetes cluster. Verrazzano currently supports a default production (prod) profile and a development (dev) profile suitable for evaluation.\nThe development profile has the following characteristics:\n Wildcard (nip.io) DNS Self-signed certificates Shared observability stack used by the system components and all applications Ephemeral storage for the observability stack (if the pods are restarted, you lose all of your logs and metrics) Single-node, reduced memory OpenSearch cluster  NOTE Because the dev profile installs self-signed certificates, when installing Verrazzano on macOS, you might see: Your connection is not private. For a workaround, see this FAQ.  To install Verrazzano:\n  Install Verrazzano with its dev profile.\n$ kubectl apply -f - \u003c\u003cEOF apiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: example-verrazzano spec: profile: dev defaultVolumeSource: persistentVolumeClaim: claimName: verrazzano-storage volumeClaimSpecTemplates: - metadata: name: verrazzano-storage spec: resources: requests: storage: 2Gi EOF   Wait for the installation to complete.\n$ kubectl wait \\ --timeout=20m \\ --for=condition=InstallComplete \\ verrazzano/example-verrazzano   (Optional) View the installation logs. You can view the logs with the following command:\n$ kubectl logs -n verrazzano-install \\ -f $(kubectl get pod \\ -n verrazzano-install \\ -l app=verrazzano-platform-operator \\ -o jsonpath=\"{.items[0].metadata.name}\") | grep '^{.*}$' \\ | jq -r '.\"@timestamp\" as $timestamp | \"\\($timestamp) \\(.level) \\(.message)\"'   Deploy an example application The Hello World Helidon example application provides a simple Hello World REST service written with Helidon. For more information and the code of this application, see the Verrazzano Examples.\nTo deploy the Hello World Helidon example application:\n  Create a namespace for the example application and add labels identifying the namespace as managed by Verrazzano and enabled for Istio.\n$ kubectl create namespace hello-helidon $ kubectl label namespace hello-helidon verrazzano-managed=true istio-injection=enabled   Apply the hello-helidon resources to deploy the application.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-comp.yaml -n hello-helidon $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-app.yaml -n hello-helidon   Wait for the application to be ready.\n$ kubectl wait \\ --for=condition=Ready pods \\ --all -n hello-helidon \\ --timeout=300s # Sample output pod/hello-helidon-deployment-78468f5f9c-czmp4 condition met This creates the Verrazzano OAM Component application resources for the example, waits for the pods in the hello-helidon namespace to be ready.\n  Save the host name of the load balancer exposing the application’s REST service endpoints.\n$ HOST=$(kubectl get gateways.networking.istio.io hello-helidon-hello-helidon-appconf-gw \\ -n hello-helidon \\ -o jsonpath='{.spec.servers[0].hosts[0]}')   Get the default message.\n$ curl -sk \\ -X GET \\ \"https://${HOST}/greet\" # Expected response {\"message\":\"Hello World!\"}   Uninstall the example application To uninstall the Hello World Helidon example application:\n  Delete the Verrazzano application resources.\n$ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-comp.yaml $ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-app.yaml   Delete the example namespace.\n$ kubectl delete namespace hello-helidon # Expected response namespace \"hello-helidon\" deleted   Verify that the hello-helidon namespace has been deleted.\n$ kubectl get ns hello-helidon # Expected response Error from server (NotFound): namespaces \"hello-helidon\" not found   Uninstall Verrazzano To uninstall Verrazzano:\n  Delete the Verrazzano custom resource.\n$ kubectl delete verrazzano example-verrazzano  NOTE This command blocks until the uninstall has completed. To follow the progress, you can view the uninstall logs.    (Optional) View the uninstall logs.\nThe Verrazzano operator launches a Kubernetes job to delete the Verrazzano installation. You can view the uninstall logs from that job with the following command:\n$ kubectl logs -n verrazzano-install -f \\ $( \\ kubectl get pod \\ -n verrazzano-install \\ -l job-name=verrazzano-uninstall-example-verrazzano \\ -o jsonpath=\"{.items[0].metadata.name}\" \\ )   Next steps See the Verrazzano Example Applications.\n","categories":"","description":"Instructions for getting started with Verrazzano","excerpt":"Instructions for getting started with Verrazzano","ref":"/docs/quickstart/","tags":"","title":"Quick Start"},{"body":"The Verrazzano custom resource contains the configuration information for an installation. Here is a sample Verrazzano custom resource file that uses Oracle Cloud Infrastructure DNS. See other examples here.\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: example-verrazzano spec: environmentName: env profile: prod components: certManager: certificate: acme: provider: letsEncrypt emailAddress: emailAddress@example.com dns: oci: ociConfigSecret: oci dnsZoneCompartmentOCID: dnsZoneCompartmentOcid dnsZoneOCID: dnsZoneOcid dnsZoneName: my.dns.zone.name ingress: type: LoadBalancer VerrazzanoSpec    Field Type Description Required     environmentName string Name of the installation. This name is part of the endpoint access URLs that are generated. The default value is default. No   profile string The installation profile to select. Valid values are prod (production) and dev (development). The default is prod. No   version string The version to install. Valid versions can be found here. Defaults to the current version supported by the Verrazzano platform operator. No   components Components The Verrazzano components. No   defaultVolumeSource VolumeSource Defines the type of volume to be used for persistence for all components unless overridden, and can be one of either EmptyDirVolumeSource or PersistentVolumeClaimVolumeSource. If PersistentVolumeClaimVolumeSource is declared, then the claimName must reference the name of an existing VolumeClaimSpecTemplate declared in the volumeClaimSpecTemplates section. No   volumeClaimSpecTemplates VolumeClaimSpecTemplate Defines a named set of PVC configurations that can be referenced from components to configure persistent volumes. No    VolumeClaimSpecTemplate    Field Type Description Required     metadata ObjectMeta Metadata about the PersistentVolumeClaimSpec template. No   spec PersistentVolumeClaimSpec A PersistentVolumeClaimSpec template that can be referenced by a Component to override its default storage settings for a profile. At present, only a subset of the resources.requests object are honored depending on the component. No    Components    Field Type Description Required     authProxy AuthProxyComponent The AuthProxy component configuration. No   certManager CertManagerComponent The cert-manager component configuration. No   dns DNSComponent The DNS component configuration. No   ingress IngressComponent The ingress component configuration. No   istio IstioComponent The Istio component configuration. No   fluentd FluentdComponent The Fluentd component configuration. No   keycloak KeycloakComponent The Keycloak component configuration. No   elasticsearch OpenSearchComponent The OpenSearch component configuration. No   prometheus PrometheusComponent The Prometheus component configuration. No   kibana OpenSearchDashboardsComponent The OpenSearch Dashboards component configuration. No   grafana GrafanaComponent The Grafana component configuration. No   kiali KialiComponent The Kiali component configuration. No    AuthProxy Component    Field Type Description Required     enabled Boolean If true, then AuthProxy will be installed. No   kubernetes AuthProxyKubernetes The Kubernetes resources than can be configured for AuthProxy. No    AuthProxy Kubernetes Configuration    Field Type Description Required     replicas uint32 The number of pods to replicate. No   affinity Affinity A Kubernetes affinity definition. No    CertManager Component    Field Type Description Required     certificate Certificate The certificate configuration. No    Certificate    Field Type Description Required     acme Acme The ACME configuration. Either acme or ca must be specified. No   ca CertificateAuthority The certificate authority configuration. Either acme or ca must be specified. No    Acme    Field Type Description Required     provider string Name of the Acme provider. Yes   emailAddress string Email address of the user. Yes    CertificateAuthority    Field Type Description Required     secretName string The secret name. Yes   clusterResourceNamespace string The secrete namespace. Yes    DNS Component    Field Type Description Required     wildcard DNS-Wilcard Wildcard DNS configuration. This is the default with a domain of nip.io. No   oci DNS-OCI Oracle Cloud Infrastructure DNS configuration. No   external DNS-External External DNS configuration. No    DNS Wildcard    Field Type Description Required     domain string The type of wildcard DNS domain. For example, nip.io, sslip.io, and such. Yes    DNS Oracle Cloud Infrastructure    Field Type Description Required     ociConfigSecret string Name of the Oracle Cloud Infrastructure configuration secret. Generate a secret based on the Oracle Cloud Infrastructure configuration profile you want to use. You can specify a profile other than DEFAULT and specify the secret name. See instructions by running ./install/create_oci_config_secret.sh. Yes   dnsZoneCompartmentOCID string The Oracle Cloud Infrastructure DNS compartment OCID. Yes   dnsZoneOCID string The Oracle Cloud Infrastructure DNS zone OCID. Yes   dnsZoneName string Name of Oracle Cloud Infrastructure DNS zone. Yes   dnsScope string Scope of the Oracle Cloud Infrastructure DNS zone (PRIVATE, GLOBAL). If not specified, then defaults to GLOBAL. No    DNS External    Field Type Description Required     suffix string The suffix for DNS names. Yes    Ingress Component    Field Type Description Required     type string The ingress type. Valid values are LoadBalancer and NodePort. The default value is LoadBalancer. Yes   nginxInstallArgs NGINXInstallArgs list A list of values to use during NGINX installation. No   ports PortConfig list The list port configurations used by the ingress. No    NGINX Install Args    Name Type ValueType Description Required     controller.service.externalIPs NameValue string list The external IP address used by the NGINX Ingress Controller. No   controller.service.externalTrafficPolicy NameValue string Preserves the client source IP address. See Bare-metal considerations. No   controller.service.annotations.* NameValue string Annotations used for NGINX Ingress Controller. For sample usage, see Customize Ingress. No   controller.autoscaling.enabled NameValue Boolean If true, then enable horizonal pod autoscaler. Default false. No   controller.autoscaling.minReplicas NameValue string Minimum replicas used for autoscaling. Default 1. No    Port Config    Field Type Description Required     name string The port name. No   port string The port value. Yes   targetPort string The target port value. The default is same as the port value. Yes   protocol string The protocol used by the port. TCP is the default. No   nodePort string The nodePort value. No    Name Value    Field Type Description Required     name string The name of a Helm override for a Verrazzano component chart, specified with a —set flag on the Helm command line, for example, helm install --set name=value. For more information about chart overrides, see Customize Ingress. Yes   value string The value of a Helm override for a Verrazzano component chart, specified with a —set flag on the Helm command line, for example, helm install --set name=value. Either value or valueList must be specified. For more information about chart overrides, see Customize Ingress. No   valueList string list The list of Helm override values for a Verrazzano component, each specified with a —set flag on the Helm command line, for example, helm install --set name[0]=\u003cfirst element of valueList\u003e —set name[1]=\u003csecond element of valueList\u003e. Either value or valueList must be specified. For more information about chart overrides, see Customize Ingress. No   setString Boolean Specifies if the argument requires the Helm --set-string command-line flag to override a chart value, for example, helm install --set-string name=value. No    Istio Component    Field Type Description Required     enabled Boolean If true, then Istio will be installed. No   istioIngress IstioIngress The Istio ingress gateway configuration. No   istioEgress IstioEgress The Istio egress gateway configuration. No   istioInstallArgs IstioInstallArgs list A list of values to use during Istio installation. Each argument is specified as either a name/value or name/valueList pair. No    Istio Ingress Configuration    Field Type Description Required     kubernetes IstioKubernetes The Kubernetes resources than can be configured for an Istio ingress gateway. No    Istio Egress Configuration    Field Type Description Required     kubernetes IstioKubernetes The Kubernetes resources than can be configured for an Istio egress gateway. No    Istio Kubernetes Configuration    Field Type Description Required     replicas uint32 The number of pods to replicate. No   affinity Affinity A Kubernetes affinity definition. No    Istio Install Args    Name Type ValueType Description Required     gateways.istio-ingressgateway.externalIPs NameValue string list The external IP address used by the Istio ingress gateway. No   gateways.istio-ingressgateway.serviceAnnotations.* NameValue string Annotations used for the Istio ingress gateway. For sample usage, see Customize Ingress. No    Fluentd Component    Field Type Description Required     enabled Boolean If true, then Fluentd will be installed. No   extraVolumeMounts ExtraVolumeMount list A list of host path volume mounts in addition to /var/log into the Fluentd DaemonSet. The Fluentd component collects log files in the /var/log/containers directory of Kubernetes worker nodes. The /var/log/containers directory may contain symbolic links to files located outside the /var/log directory. If the host path directory containing the log files is located outside of /var/log, the Fluentd DaemonSet must have the volume mount of that directory to collect the logs. No   elasticsearchURL string The target OpenSearch URLs. Specify this option in this format. The default http://vmi-system-es-ingest-oidc:8775 is the VMI OpenSearch URL. No   elasticsearchSecret string The secret containing the credentials for connecting to OpenSearch. This secret needs to be created in the verrazzano-install namespace prior to creating the Verrazzano custom resource. Specify the OpenSearch login credentials in the username and password fields in this secret. Specify the CA for verifying the OpenSearch certificate in the ca-bundle field, if applicable. The default verrazzano is the secret for connecting to the VMI OpenSearch. No   oci OCILoggingConfiguration The Oracle Cloud Infrastructure Logging configuration. No    Extra Volume Mount    Field Type Description Required     source string The source host path. Yes   destination string The destination path on the Fluentd Container, defaults to the source host path. No   readOnly Boolean Specifies if the volume mount is read-only, defaults to true. No    Oracle Cloud Infrastructure Logging Configuration    Field Type Description Required     systemLogId string The OCID of the Oracle Cloud Infrastructure Log that will collect system logs. Yes   defaultAppLogId string The OCID of the Oracle Cloud Infrastructure Log that will collect application logs. Yes   apiSecret string The name of the secret containing the Oracle Cloud Infrastructure API configuration and private key. No    Keycloak Component    Field Type Description Required     enabled Boolean If true, then Keycloak will be installed. No   mysql MySQLComponent Contains the MySQL component configuration needed for Keycloak. No    MySQL Component    Field Type Description Required     volumeSource VolumeSource Defines the type of volume to be used for persistence for Keycloak/MySQL, and can be one of either EmptyDirVolumeSource or PersistentVolumeClaimVolumeSource. If PersistentVolumeClaimVolumeSource is declared, then the claimName must reference the name of a VolumeClaimSpecTemplate declared in the volumeClaimSpecTemplates section. No    OpenSearch Component    Field Type Description Required     enabled Boolean If true, then OpenSearch will be installed. No   installArgs OpenSearchInstallArgs list A list of values to use during OpenSearch installation. Each argument is specified as either a name/value or name/valueList pair. For sample usage, see Customize OpenSearch. No    OpenSearch Install Args    Name Type ValueType Description Required     nodes.master.replicas NameValue string The number of master node replicas. No   nodes.master.requests.memory NameValue string The master node memory request amount expressed as a Quantity. No   nodes.ingest.replicas NameValue string The number of ingest node replicas. No   nodes.ingest.requests.memory NameValue string The ingest node memory request amount expressed as a Quantity. No   nodes.data.replicas NameValue string The number of data node replicas. No   nodes.data.requests.memory NameValue string The data node memory request amount expressed as a Quantity. No   nodes.data.requests.storage NameValue string The data storage request amount expressed as a Quantity. No    OpenSearch Dashboards Component    Field Type Description Required     enabled Boolean If true, then OpenSearch Dashboards will be installed. No    Prometheus Component    Field Type Description Required     enabled Boolean If true, then Prometheus will be installed. No    Grafana Component    Field Type Description Required     enabled Boolean If true, then Grafana will be installed. No    Kiali Component    Field Type Description Required     enabled Boolean If true, then Kiali will be installed. No    ","categories":"","description":"","excerpt":"The Verrazzano custom resource contains the configuration information for an installation. Here is a sample Verrazzano custom resource file that uses Oracle Cloud Infrastructure DNS. See other …","ref":"/docs/reference/api/verrazzano/verrazzano/","tags":"","title":"Verrazzano Custom Resource Definition"},{"body":"VerrazzanoCoherenceWorkload The VerrazzanoCoherenceWorkload custom resource contains the configuration information for a Coherence workload within Verrazzano. Here is a sample component that specifies a VerrazzanoCoherenceWorkload. To deploy an example application that demonstrates this workload type, see Sock Shop.\napiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: carts namespace: sockshop spec: workload: apiVersion: oam.verrazzano.io/v1alpha1 kind: VerrazzanoCoherenceWorkload spec: template: metadata: name: carts-coh spec: cluster: SockShop role: Carts replicas: 1 image: ghcr.io/helidon-sockshop/carts-coherence:2.2.0 imagePullPolicy: Always application: type: helidon jvm: args: - \"-Dcoherence.k8s.operator.health.wait.dcs=false\" - \"-Dcoherence.metrics.legacy.names=false\" memory: heapSize: 2g coherence: logLevel: 9 ports: - name: http port: 7001 service: name: carts port: 80 serviceMonitor: enabled: true - name: metrics port: 7001 serviceMonitor: enabled: true VerrazzanoCoherenceWorkload    Field Type Description Required     apiVersion string oam.verrazzano.io/v1alpha1 Yes   kind string VerrazzanoCoherenceWorkload Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec VerrazzanoCoherenceWorkloadSpec The desired state of a Verrazzano Coherence workload. Yes    VerrazzanoCoherenceWorkloadSpec VerrazzanoCoherenceWorkloadSpec specifies the desired state of a Verrazzano Coherence workload.\n   Field Type Description Required     template RawExtension The metadata and spec for the underlying Coherence resource. Yes    VerrazzanoHelidonWorkload The VerrazzanoHelidonWorkload custom resource contains the configuration information for a Helidon workload within Verrazzano. Here is a sample component that specifies a VerrazzanoHelidonWorkload. To deploy an example application that demonstrates this workload type, see Hello World Helidon.\napiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: hello-helidon-component namespace: hello-helidon spec: workload: apiVersion: oam.verrazzano.io/v1alpha1 kind: VerrazzanoHelidonWorkload metadata: name: hello-helidon-workload labels: app: hello-helidon spec: deploymentTemplate: metadata: name: hello-helidon-deployment podSpec: containers: - name: hello-helidon-container image: \"ghcr.io/verrazzano/example-helidon-greet-app-v1:0.1.10-3-20201016220428-56fb4d4\" ports: - containerPort: 8080 name: http VerrazzanoHelidonWorkload    Field Type Description Required     apiVersion string oam.verrazzano.io/v1alpha1 Yes   kind string VerrazzanoHelidonWorkload Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec VerrazzanoHelidonWorkloadSpec The desired state of a Verrazzano Helidon workload. Yes    VerrazzanoHelidonWorkloadSpec VerrazzanoHelidonWorkloadSpec specifies the desired state of a Verrazzano Helidon workload.\n   Field Type Description Required     deploymentTemplate DeploymentTemplate The embedded deployment. Yes    DeploymentTemplate DeploymentTemplate specifies the metadata and pod spec of the underlying deployment.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   strategy DeploymentStrategy The replacement strategy of the underlying deployment. No   podSpec PodSpec The pod spec of the underlying deployment. Yes    VerrazzanoWebLogicWorkload The VerrazzanoWebLogicWorkload custom resource contains the configuration information for a WebLogic Domain workload within Verrazzano. Here is a sample component that specifies a VerrazzanoWebLogicWorkload. To deploy an example application that demonstrates this workload type, see the ToDo List Lift-and-Shift application.\napiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: todo-domain namespace: todo-list spec: workload: apiVersion: oam.verrazzano.io/v1alpha1 kind: VerrazzanoWebLogicWorkload spec: template: metadata: name: todo-domain namespace: todo-list spec: domainUID: tododomain domainHome: /u01/domains/tododomain image: container-registry.oracle.com/verrazzano/example-todo:0.8.0 imagePullSecrets: - name: tododomain-repo-credentials domainHomeSourceType: \"FromModel\" includeServerOutInPodLog: true replicas: 1 webLogicCredentialsSecret: name: tododomain-weblogic-credentials configuration: introspectorJobActiveDeadlineSeconds: 900 model: configMap: tododomain-jdbc-config domainType: WLS modelHome: /u01/wdt/models runtimeEncryptionSecret: tododomain-runtime-encrypt-secret secrets: - tododomain-jdbc-tododb serverPod: env: - name: JAVA_OPTIONS value: \"-Dweblogic.StdoutDebugEnabled=false\" - name: USER_MEM_ARGS value: \"-Djava.security.egd=file:/dev/./urandom -Xms64m -Xmx256m \" - name: WL_HOME value: /u01/oracle/wlserver - name: MW_HOME value: /u01/oracle VerrazzanoWebLogicWorkload    Field Type Description Required     apiVersion string oam.verrazzano.io/v1alpha1 Yes   kind string VerrazzanoWebLogicWorkload Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec VerrazzanoWebLogicWorkloadSpec The desired state of a Verrazzano WebLogic workload. Yes    VerrazzanoWebLogicWorkloadSpec VerrazzanoWebLogicWorkloadSpec specifies the desired state of a Verrazzano WebLogic workload.\n   Field Type Description Required     template RawExtension The metadata and spec for the underlying WebLogic Domain resource. Yes    ","categories":"","description":"","excerpt":"VerrazzanoCoherenceWorkload The VerrazzanoCoherenceWorkload custom resource contains the configuration information for a Coherence workload within Verrazzano. Here is a sample component that specifies …","ref":"/docs/reference/api/oam/workloads/","tags":"","title":"Verrazzano Workload Custom Resource Definitions"},{"body":"The VerrazzanoManagedCluster custom resource is used to register a managed cluster with an admin cluster. Here is a sample VerrazzanoManagedCluster that registers the cluster named managed1. To deploy an example application that demonstrates a VerrazzanoManagedCluster, see Multicluster Hello World Helidon.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: VerrazzanoManagedCluster metadata: name: managed1 namespace: verrazzano-mc spec: description: \"Managed Cluster 1\" caSecret: ca-secret-managed1 VerrazzanoManagedCluster    Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string VerrazzanoManagedCluster Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec VerrazzanoManagedClusterSpec The managed cluster specification. Yes   status VerrazzanoManagedClusterStatus The runtime status this resource. No    VerrazzanoManagedClusterSpec VerrazzanoManagedClusterSpec specifies a managed cluster to associate with an admin cluster.\n   Field Type Description Required     description string The description of the managed cluster. No   caSecret string The name of a Secret that contains the CA certificate of the managed cluster. This is used to configure the admin cluster to scrape metrics from the Prometheus endpoint on the managed cluster. See the steps 3 and 4 in instructions for how to create this Secret. Yes   serviceAccount string The name of the ServiceAccount that was generated for the managed cluster. This field is managed by a Verrazzano Kubernetes operator. No   managedClusterManifestSecret string The name of the Secret containing generated YAML manifest file to be applied by the user to the managed cluster. This field is managed by a Verrazzano Kubernetes operator. No    VerrazzanoManagedClusterStatus    Field Type Description Required     conditions Condition array The current state of this resource. No   lastAgentConnectTime string The last time the agent from this managed cluster connected to the admin cluster. No   apiUrl string The Verrazzano API server URL for the managed cluster. No    Condition Condition describes current state of this resource.\n   Field Type Description Required     type string The condition of the multicluster resource which can be checked with a kubectl wait command. Condition values are case-sensitive and formatted as follows: Ready: the VerrazzanoManagedCluster is ready to be used and all resources needed have been generated. Yes   status ConditionStatus An instance of the type ConditionStatus that is defined in types.go. Yes   lastTransitionTime string The last time the condition transitioned from one status to another. No   message string A message with details about the last transition. No    ","categories":"","description":"","excerpt":"The VerrazzanoManagedCluster custom resource is used to register a managed cluster with an admin cluster. Here is a sample VerrazzanoManagedCluster that registers the cluster named managed1. To deploy …","ref":"/docs/reference/api/multicluster/verrazzanomanagedcluster/","tags":"","title":"VerrazzanoManagedCluster Custom Resource Definition"},{"body":"The VerrazzanoProject custom resource is used to create the application namespaces and their associated security settings on one or more clusters. The namespaces are always created on the admin cluster. Here is a sample VerrazzanoProject that specifies a namespace to create on the cluster named managed1.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: VerrazzanoProject metadata: name: hello-helidon namespace: verrazzano-mc spec: template: namespaces: - metadata: name: hello-helidon placement: clusters: - name: managed1 VerrazzanoProject    Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string VerrazzanoProject Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec VerrazzanoProjectSpec The project specification. Yes   status MultiClusterResourceStatus The runtime status of a multicluster resource. No    VerrazzanoProjectSpec VerrazzanoProjectSpec specifies the namespaces to create and on which clusters to create them.\n   Field Type Description Required     template ProjectTemplate The project template. Yes   placement Placement Clusters on which the namespaces are to be created. Yes    ProjectTemplate ProjectTemplate contains the list of namespaces to create and the optional security configuration for each namespace.\n   Field Type Description Required     namespaces NamespaceTemplate array The list of application namespaces to create for this project. Yes   security SecuritySpec The project security configuration. No   networkPolicies NetworkPolicyTemplate array The network policies applied to namespaces in the project. No    NamespaceTemplate NamespaceTemplate contains the metadata and specification of a Kubernetes namespace.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec NamespaceSpec An instance of the struct NamespaceSpec defined in types.go. No    SecuritySpec SecuritySpec defines the security configuration for a project.\n   Field Type Description Required     projectAdminSubjects Subject The subject to bind to the verrazzano-project-admin role. Encoded as an instance of the struct Subject defined in types.go. No   projectMonitorSubjects Subject The subject to bind to the verrazzano-project-monitoring role. Encoded as an instance of the struct Subject defined in types.go. No    NetworkPolicyTemplate NetworkPolicyTemplate contains the metadata and specification of the underlying NetworkPolicy. NOTE To add application NetworkPolicy, see NetworkPolicies for applications.     Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec NetworkPolicySpec An instance of the struct NetworkPolicySpec defined in types.go. No    ","categories":"","description":"","excerpt":"The VerrazzanoProject custom resource is used to create the application namespaces and their associated security settings on one or more clusters. The namespaces are always created on the admin …","ref":"/docs/reference/api/multicluster/verrazzanoproject/","tags":"","title":"VerrazzanoProject Custom Resource Definition"},{"body":"You may want to add additional sidecars to Verrazzano workloads; you may use any image or sidecar container. This guide will serve as an introduction by showing you how to create a custom Fluentd sidecar for application logs.\nVerrazzano creates and manages a Fluentd sidecar injection for each WebLogic pod. This allows application logs to interact with the cluster-wide Fluentd DaemonSet. However, these resources are not currently configurable and additional containers are required to customize the Fluentd configuration file and the container image. For more information on Fluentd sidecars and DaemonSet, see Logging.\nThe following instructions use the ToDo List example application to demonstrate how to attach and deploy a custom Fluentd sidecar to a VerrazzanoWebLogicWorkload component. Before deploying the application, you will need to edit the application and component YAML files. Run the following commands to create a local copy of them:\n$ curl https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/todo-list/todo-list-components.yaml --output todo-list-components.yaml $ curl https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/todo-list/todo-list-application.yaml --output todo-list-application.yaml The todo-list-components.yaml file contains the VerrazzanoWebLogicWorkload, which is where you will modify the deployment.\nCreate a Fluentd custom sidecar configuration file Before deploying the VerrazzanoWebLogicWorkload component, create a ConfigMap that contains the Fluentd config file.\napiVersion:v1kind:ConfigMapmetadata:name:fluentdconfnamespace:todo-listdata:fluent.conf:|... \u003cmatch **\u003e @type stdout \u003c/match\u003eIn order to interact with the Fluentd DaemonSet that Verrazzano manages, the configuration must redirect logs to stdout, as shown in the match block at the end of the Fluentd config file. This ConfigMap must be deployed before or with all other application resources.\nCreate Fluentd custom sidecar volumes Now that the Fluentd configuration ConfigMap is deployed, create volumes to grant Fluentd access to the application logs and the Fluentd configuration file.\napiVersion:core.oam.dev/v1alpha2kind:Componentmetadata:name:todo-domainnamespace:todo-listspec:workload:apiVersion:oam.verrazzano.io/v1alpha1kind:VerrazzanoWebLogicWorkloadspec:template:metadata:name:todo-domainnamespace:todo-listspec:domainUID:tododomaindomainHome:/u01/domains/tododomainimage:container-registry.oracle.com/verrazzano/example-todo:0.1.12-1-20210624160519-017d358imagePullSecrets:- name:tododomain-repo-credentialsdomainHomeSourceType:\"FromModel\"includeServerOutInPodLog:truereplicas:1webLogicCredentialsSecret:name:tododomain-weblogic-credentialsconfiguration:introspectorJobActiveDeadlineSeconds:900model:configMap:tododomain-jdbc-configdomainType:WLSmodelHome:/u01/wdt/modelsruntimeEncryptionSecret:tododomain-runtime-encrypt-secretsecrets:- tododomain-jdbc-tododbserverPod:# ---- BEGIN: Add volumes for Fluentd container ----volumes:- emptyDir:{}name:shared-log-files- name:fdconfigconfigMap:name:fluentdconf# ---- END: Add volumes for Fluentd container ----env:- name:JAVA_OPTIONSvalue:\"-Dweblogic.StdoutDebugEnabled=false\"- name:USER_MEM_ARGSvalue:\"-Djava.security.egd=file:/dev/./urandom -Xms64m -Xmx256m \"- name:WL_HOMEvalue:/u01/oracle/wlserver- name:MW_HOMEvalue:/u01/oracleThe example volume shared-log-files is used to enable the Fluentd container to view logs from application containers. This example uses an emptyDir volume type for ease of access, but you can use other volume types.\nThe fdconfig example volume mounts the previously deployed ConfigMap containing the Fluentd configuration. This allows the attached Fluentd sidecar to access the embedded Fluentd configuration file.\nCreate the Fluentd custom sidecar container The final resource addition to the VerrazzanoWebLogicWorkload is to create the custom sidecar container.\napiVersion:core.oam.dev/v1alpha2kind:Componentmetadata:name:todo-domainnamespace:todo-listspec:workload:apiVersion:oam.verrazzano.io/v1alpha1kind:VerrazzanoWebLogicWorkloadspec:template:metadata:name:todo-domainnamespace:todo-listspec:domainUID:tododomaindomainHome:/u01/domains/tododomainimage:container-registry.oracle.com/verrazzano/example-todo:0.1.12-1-20210624160519-017d358imagePullSecrets:- name:tododomain-repo-credentialsdomainHomeSourceType:\"FromModel\"includeServerOutInPodLog:truereplicas:1webLogicCredentialsSecret:name:tododomain-weblogic-credentialsconfiguration:introspectorJobActiveDeadlineSeconds:900model:configMap:tododomain-jdbc-configdomainType:WLSmodelHome:/u01/wdt/modelsruntimeEncryptionSecret:tododomain-runtime-encrypt-secretsecrets:- tododomain-jdbc-tododbserverPod:# ---- BEGIN: Add Fluentd container with volumeMounts ----containers:- image:ghcr.io/verrazzano/fluentd-kubernetes-daemonset:v1.12.3-20210517195222-f345ec2name:fluentdenv:- name:FLUENT_UIDvalue:root- name:FLUENT_CONFvalue:fluent.conf- name:FLUENTD_ARGSvalue:-c /fluentd/etc/fluent.confvolumeMounts:- mountPath:/scratchname:shared-log-filesreadOnly:true- name:fdconfigmountPath:/fluentd/etc/# ---- END: Add Fluentd container with volumeMounts ----volumes:- emptyDir:{}name:shared-log-files- name:fdconfigconfigMap:name:fluentdconfenv:- name:JAVA_OPTIONSvalue:\"-Dweblogic.StdoutDebugEnabled=false\"- name:USER_MEM_ARGSvalue:\"-Djava.security.egd=file:/dev/./urandom -Xms64m -Xmx256m \"- name:WL_HOMEvalue:/u01/oracle/wlserver- name:MW_HOMEvalue:/u01/oracleThis example container uses the Verrazzano Fluentd image, but you can use any image with additional Fluentd plug-ins in its place.\nMounted are both volumes created to enable the Fluentd sidecar to monitor and parse logs. VerrazzanoWebLogicWorkloads mount a volume in the /scratch directory containing log files. Thus, any sidecar containers are limited to log access under that directory. As shown previously, the shared-log-file volume is mounted at /scratch for this reason.\nThe example Fluentd configuration volume is mounted at /fluentd/etc/. While this path is more flexible, the FLUEND_ARGS environment variable needs to be updated accordingly.\nDeploy the Fluentd sidecar Now that the resources have been configured, you can deploy the application. Follow steps 1 through 3 in the ToDo List example application instructions. Replace the deployment commands in step 4 with your locally edited YAML files:\n$ kubectl apply -f todo-list-components.yaml $ kubectl apply -f todo-list-application.yaml Now, follow the ToDo List instructions from step 5 onward, as needed.\nTo verify that a deployment successfully created a custom Fluentd sidecar:\n Verify that the container name exists on the WebLogic application pod. $ kubectl get pods -n \u003capplication-namespace\u003e \u003capplication-pod-name\u003e -o jsonpath=\"{.spec.containers[*].name}\" | tr -s '[[:space:]]' '\\n' ... fluentd ...  Verify that the Fluentd sidecar is redirecting logs to stdout. kubectl logs -n \u003capplication-namespace\u003e \u003capplication-pod-name\u003e fluentd  Follow the instructions at Verrazzano Logging to ensure that the Fluentd DaemonSet collected the logs from stdout. These logs will appear in the Verrazzano-managed OpenSearch and OpenSearch Dashboards.  ","categories":"","description":"A guide for deploying custom sidecars to Verrazzano workload components","excerpt":"A guide for deploying custom sidecars to Verrazzano workload components","ref":"/docs/guides/sidecar/","tags":"","title":"Deploy a Kubernetes Sidecar with Verrazzano"},{"body":"See the following support information, learning channels, and Verrazzano release history:\n Verrazzano Community Support - Join Verrazzano on Slack! Verrazzano GitHub Project Oracle Verrazzano Enterprise Container Platform Support (available for purchase; contact Oracle Sales) Check us out on Medium Learn about Verrazzano on YouTube  Release History \u0026 Error Correction Dates\nThe timeline for Verrazzano releases and the date of their end of error correction.\n   Verrazzano Release Date Latest Patch Release Latest Patch Release Date End of Error Correction     1.6 2023-06-28 1.6.3 2023-08-02 2024-06-30*   1.5 2023-02-15 1.5.5 2023-08-02 2024-02-28   1.4 2022-09-30 1.4.6 2023-07-31 2023-10-31   1.3 2022-05-24 1.3.8 2022-11-17 2023-05-31   1.2 2022-03-14 1.2.2 2022-05-10 2022-11-30   1.1 2021-12-16 1.1.2 2022-03-09 2022-09-30   1.0 2021-08-02 1.0.4 2021-12-20 2022-06-30    *Projected date. Actual date will be determined when the next minor or major release is available. ","categories":"","description":"","excerpt":"See the following support information, learning channels, and Verrazzano release history:\n Verrazzano Community Support - Join Verrazzano on Slack! Verrazzano GitHub Project Oracle Verrazzano …","ref":"/docs/help/","tags":"","title":"Get Help"},{"body":"The Hello World Helidon example is a Helidon-based service that returns a “Hello World” response when invoked. The example application is specified using Open Application Model (OAM) component and application configuration YAML files, and then deployed by applying those files.\nThe example application has two endpoints, which differ in configuration source:\n /greet- uses a microprofile properties file. Deploy this application by using the instructions here. /config- uses a Kubernetes ConfigMap. Deploy this application by using the instructions here.  For more information and the code of this application, see the Verrazzano examples.\n","categories":"","description":"A simple Hello World REST service written with Helidon","excerpt":"A simple Hello World REST service written with Helidon","ref":"/docs/samples/hello-world/","tags":"","title":"Hello World Helidon"},{"body":"","categories":"","description":"How to install Verrazzano","excerpt":"How to install Verrazzano","ref":"/docs/setup/install/","tags":"","title":"Install"},{"body":"Verrazzano manages and secures network traffic between Verrazzano system components and deployed applications. Verrazzano does not manage or secure traffic for the Kubernetes cluster itself, or for non-Verrazzano services or applications running in the cluster. Traffic is secured at two levels in the network stack:\n ISO Layer 3/4: Using NetworkPolicies to control IP access to Pods. ISO Layer 6: Using TLS and mTLS to provide authentication, confidentiality, and integrity for connections within the cluster, and for external connections.  NetworkPolicies By default, all Pods in a Kubernetes cluster have network access to all other Pods in the cluster. Kubernetes has a NetworkPolicy resource that provides network level 3 and 4 security for Pods, restricting both ingress and egress IP traffic for a set of Pods in a namespace. Verrazzano configures all system components with NetworkPolicies to control ingress. Egress is not restricted.\nNOTE: A NetworkPolicy resource needs a NetworkPolicy controller to implement the policy, otherwise the policy has no effect. You must install a Kubernetes CNI plug-in that provides a NetworkPolicy controller, such as Calico, before installing Verrazzano, or else the policies are ignored.\nNetworkPolicies for system components Verrazzano installs a set of NetworkPolicies for system components to control ingress into the Pods. A policy is scoped to a namespace and uses selectors to specify the Pods that the policy applies to, along with the ingress and egress rules. For example, the following policy applies to the Verrazzano API Pod in the verrazzano-system namespace. This policy allows network traffic from NGINX Ingress Controller on port 8775, and from Prometheus on port 15090. No other Pods can reach those ports or any other ports of the Verrazzano API Pod. Notice that namespace selectors need to be used; the NetworkPolicy resource does not support specifying the namespace name.\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy ... spec: PodSelector: matchLabels: app: verrazzano-api ingress: - from: - namespaceSelector: matchLabels: verrazzano.io/namespace: ingress-nginx PodSelector: matchLabels: app.kubernetes.io/instance: ingress-controller ports: - port: 8775 protocol: TCP - from: - namespaceSelector: matchLabels: verrazzano.io/namespace: verrazzano-system PodSelector: matchLabels: app: system-prometheus ports: - port: 15090 protocol: TCP The following table shows all of the ingresses that allow network traffic into system components. The ports shown are Pod ports, which is what NetworkPolicies require.\n   Component Pod Port From Description      Verrazzano Application Operator 9443 Kubernetes API Server Webhook entrypoint.    Verrazzano Platform Operator 9443 Kubernetes API Server Webhook entrypoint.    Verrazzano Console 8000 NGINX Ingress Access from external client.    Verrazzano Console 15090 Prometheus Prometheus scraping.    Verrazzano Authentication Proxy 8775 NGINX Ingress Access from external client.    Verrazzano Authentication Proxy 15090 Prometheus Prometheus scraping.    cert-manager 9402 Prometheus Prometheus scraping.    Coherence Operator 9443 Prometheus Webhook entrypoint.    OpenSearch 8775 NGINX Ingress Access from external client.    OpenSearch 8775 Fluentd Access from Fluentd.    OpenSearch 9200 OpenSearch Dashboards, Internal OpenSearch data port.    OpenSearch 9300 Internal OpenSearch cluster port.    OpenSearch 15090 Prometheus Envoy metrics scraping.    Istio control plane 15012 Envoy Envoy access to istiod.    Istio control plane 15014 Prometheus Prometheus scraping.    Istio control plane 15017 Kubernetes API Server Webhook entrypoint.    Istio ingress gateway 8443 External Application ingress.    Istio ingress gateway 15090 Prometheus Prometheus scraping.    Istio egress gateway 8443 Mesh services Application egress.    Istio egress gateway 15090 Prometheus Prometheus scraping.    Keycloak 8080 NGINX Ingress Access from external client.    Keycloak 15090 Prometheus Prometheus scraping.    MySql 15090 Prometheus Prometheus scraping.    MySql 3306 Keycloak Keycloak datastore.    Node exporter 9100 Prometheus Prometheus scraping.    Rancher 80 NGINX Ingress Access from external client.    Rancher 9443 Kubernetes API Server Webhook entrypoint.    Prometheus 8775 NGINX Ingress Access from external client.    Prometheus 9090 Grafana Acccess for Grafana UI.     NetworkPolicies for applications By default, applications do not have NetworkPolicies that restrict ingress into the application or egress from it. You can configure them for the application namespaces using the NetworkPolicy section of a Verrazzano project.\nNOTE Verrazzano requires specific ingress to and egress from application pods. If you add a NetworkPolicy for your application namespace or pods, you must add an additional policy to ensure that Verrazzano still has the required access it needs. The ingress policy is only needed if you restrict ingress. Likewise, the egress policy is only needed if you restrict egress. The following are the ingress and egress NetworkPolicies:\n ingress NetworkPolicies  ingress: - from: - namespaceSelector: matchLabels: verrazzano.io/namespace: istio-system podSelector: matchLabels: app: istiod - from: - namespaceSelector: matchLabels: verrazzano.io/namespace: istio-system podSelector: matchLabels: app: istio-ingressgateway - from: - namespaceSelector: matchLabels: verrazzano.io/namespace: verrazzano-system podSelector: matchLabels: app: system-prometheus - from: - namespaceSelector: matchLabels: verrazzano.io/namespace: verrazzano-system podSelector: matchLabels: app: coherence-operator - from: - namespaceSelector: matchLabels: verrazzano.io/namespace: verrazzano-system podSelector: matchLabels: app: weblogic-operator   egress NetworkPolicies  egress: - ports: - port: 15012 protocol: TCP to: - namespaceSelector: matchLabels: verrazzano.io/namespace: istio-system podSelector: matchLabels: app: istiod - to: - namespaceSelector: matchLabels: verrazzano.io/namespace: istio-system podSelector: matchLabels: app: istio-egressgateway - ports: - port: 53 protocol: TCP - port: 53 protocol: UDP to: - namespaceSelector: matchLabels: verrazzano.io/namespace: kube-system - ports: - port: 8000 protocol: TCP to: - namespaceSelector: matchLabels: verrazzano.io/namespace: verrazzano-system podSelector: matchLabels: app: coherence-operator   NetworkPolicies for Envoy sidecar proxies As mentioned, Envoy sidecar proxies run in both system component pods and application pods. Each proxy sends requests to the Istio control plane pod, istiod, for a variety of reasons. During installation, Verrazzano creates a NetworkPolicy named istiod-access in the istio-system namespace to give ingress to system component and application sidecar proxies.\nmTLS Istio can be enabled to use mTLS between services in the mesh, and also between the Istio gateways and Envoy sidecar proxies. There are various options to customize mTLS usage, for example it can be disabled on a per-port level. The Istio control plane, Istiod, is a CA and provides key and certificate rotation for the Envoy proxies, both gateways and sidecars.\nVerrazzano configures Istio to have strict mTLS for the mesh. All components and applications put into the mesh will use mTLS, with the exception of Coherence clusters, which are not in the mesh. Also, all traffic between the Istio ingress gateway and mesh sidecars use mTLS, and the same is true between the proxy sidecars and the egress gateway.\nVerrazzano sets up mTLS during installation with the PeerAuthentication resource as follows:\napiVersion: v1 items: - apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication ... spec: mtls: mode: STRICT TLS TLS is used by external clients to access the cluster, both through the NGINX Ingress Controller and the Istio ingress gateway. The certificate used by these TLS connections vary; see Verrazzano security for details. All TLS connections are terminated at the ingress proxy. Traffic between the two proxies and the internal cluster Pods always uses mTLS, because those Pods are all in the Istio mesh.\nIstio mesh Istio provides extensive security protection for both authentication and authorization, as described in Istio Security. Access control and mTLS are two security features that Verrazzano configures. These security features are available in the context of a service mesh.\nA service mesh is an infrastructure layer that provides certain capabilities like security, observability, load balancing, and such, for services. Istio defines a service mesh here. In the context of Istio on Kubernetes, a service in the mesh is a Kubernetes Service. Consider the Bob’s Books example application, which has several OAM Components defined. At runtime, there is a Kubernetes Service for each component, and each Service is in the mesh, with one or more Pods associated with the service. All services in the mesh have an Envoy proxy in front of their Pods, intercepting network traffic to and from the Pod. In Kubernetes, that proxy happens to be a sidecar running in each Pod.\nThere are various ways to put a service in the mesh. Verrazzano uses the namespace label, istio-injection: enabled, to designate that all Pods in a given namespace are in the mesh. When a Pod is created in that namespace, the Istio control plane mutating webhook, changes the Pod spec to add the Envoy proxy sidecar container, causing the Pod to be in the mesh.\nDisabling sidecar injection In certain cases, Verrazzano needs to disable sidecar injection for specific Pods in a namespace. This is done in two ways: first, during installation, Verrazzano modifies the istio-sidecar-injector ConfigMap using a Helm override file for the Istio chart. This excludes several components from the mesh, such as the Verrazzano application operator. Second, certain Pods, such as Coherence Pods, are labeled at runtime with sidecar.istio.io/inject=\"false\" to exclude them from the mesh.\nComponents in the mesh The following Verrazzano components are in the mesh and use mTLS for all service to service communication.\n OpenSearch Fluentd Grafana Kiali OpenSearch Dashboards Keycloak MySQL NGINX Ingress Controller Prometheus Verrazzano Authentication Proxy Verrazzano Console WebLogic Kubernetes Operator  Some of these components, have mesh-related details that are worth noting, as described in the following sections.\nNGINX The NGINX Ingress Controller listens for HTTPS traffic, and provides ingress into the cluster. NGINX is configured to do TLS termination of client connections. All traffic from NGINX to the mesh services use mTLS, which means that traffic is fully encrypted from the client to the target back-end services.\nKeycloak and MySQL Keycloak and MySQL are also in the mesh and use mTLS for network traffic. Because all of the components that use Keycloak are in the mesh, there is end to end mTLS security for all identity management handled by Keycloak. The following components access Keycloak:\n Verrazzano Authentication Proxy Verrazzano Console OpenSearch Prometheus Grafana Kiali OpenSearch Dashboards  Prometheus Although Prometheus is in the mesh, it is configured to use the Envoy sidecar and mTLS only when communicating with Keycloak. All the traffic related to scraping metrics, bypasses the sidecar proxy, doesn’t use the service IP address, but rather connects to the scrape target using the Pod IP address. If the scrape target is in the mesh, then HTTPS is used; otherwise, HTTP is used. For Verrazzano multicluster, Prometheus also connects from the admin cluster to the Prometheus server in the managed cluster by using the managed cluster NGINX Ingress, using HTTPS. Prometheus in the managed cluster and never establishes connections to targets outside the cluster.\nBecause Prometheus is in the mesh, additional configuration is done to allow the Envoy sidecar to be bypassed when scraping Pods. This is done with the Prometheus Pod annotation traffic.sidecar.istio.io/includeOutboundIPRanges: \u003ckeycloak-service-ip\u003e. This causes traffic bound for Keycloak to go through the Envoy sidecar, and all other traffic to bypass the sidecar.\nWebLogic Kubernetes Operator When the WebLogic operator creates a domain, it needs to communicate with the Pods in the domain. Verrazzano puts the WebLogic operator in the mesh so that it can communicate with the domain Pods using mTLS. As a result, the WebLogic domain must be created in the mesh.\nApplications in the mesh Before you create a Verrazzano application, you should decide if it should be in the mesh. You control sidecar injection, for example, mesh inclusion, by labeling the application namespace with istio-injection=enabled or istio-injection=disabled. By default, applications will not be put in the mesh if that label is missing. If your application uses a Verrazzano project, then Verrazzano will label the namespaces in the project to enable injection. If the application is in the mesh, then mTLS will be used. You can change the PeerAuthentication mTLS mode as desired if you don’t want strict mTLS. Also, if you need to add mTLS port exceptions, you can do this with DestinationRules or by creating another PeerAuthentication resource in the application namespace. Consult the Istio documentation for more information.\nWebLogic When the WebLogic operator creates a domain, it needs to communicate with the Pods in the domain. Verrazzano puts the WebLogic operator in the mesh so that it can communicate with the domain Pods using mTLS. Because of that, the WebLogic domain must be created in the mesh. Also, because mTLS is used, do not configure WebLogic to use TLS. If you want to use a custom certificate for your application, you can specify that in the ApplicationConfiguration, but that TLS connection will be terminated at the Istio ingress gateway, which you configure using a Verrazzano IngressTrait.\nCoherence Coherence clusters are represented by the Coherence resource, and are not in the mesh. When Verrazzano creates a Coherence cluster in a namespace that is annotated to do sidecar injection, it disables injection of the Coherence resource using the sidecar.istio.io/inject=\"false\" label shown previously. Furthermore, Verrazzano will create a DestinationRule in the application namespace to disable mTLS for the Coherence extend port 9000. This allows a service in the mesh to call the Coherence extend proxy. For an example, see Bobs Books.\nHere is an example of a DestinationRule created for the Bob’s Books application which includes a Coherence cluster.\nAPI Version: networking.istio.io/v1beta1 Kind: DestinationRule ... Spec: Host: *.bobs-books.svc.cluster.local Traffic Policy: Port Level Settings: Port: Number: 9000 Tls: Tls: Mode: ISTIO_MUTUAL Istio access control Istio lets you control access to your workload in the mesh, using the AuthorizationPolicy resource. This lets you control which services or Pods can access your workloads. Some of these options require mTLS; for more information, see Authorization Policy.\nVerrazzano always creates AuthorizationPolicies for applications, but never for system components. During application deployment, Verrazzano creates the policy in the application namespace and configures it to allow access from the following:\n Other Pods in the application Istio ingress gateway Prometheus scraper  This prevents other Pods in the cluster from gaining network access to the application Pods.\nIstio uses a service identity to determine the identity of the request’s origin; for Kubernetes this identity is a service account. Verrazzano creates a per-application AuthorizationPolicy as follows:\nAuthorizationPolicy apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy ... spec: rules: - from: - source: principals: - cluster.local/ns/sales/sa/greeter - cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account - cluster.local/ns/verrazzano-system/sa/verrazzano-monitoring-operator WebLogic domain access For WebLogic applications, the WebLogic operator must have access to the domain Pods for two reasons. First, it must access the domain servers to get health status; second it must inject configuration into the Monitoring Exporter sidecar running in the domain server Pods. When a WebLogic domain is created, Verrazzano adds an additional source, cluster.local/ns/verrazzano-system/sa/weblogic-operator-sa to the principals section to permit that access.\n","categories":"","description":"Learn about Verrazzano network security","excerpt":"Learn about Verrazzano network security","ref":"/docs/networking/security/net-security/","tags":"","title":"Network Security"},{"body":"Verrazzano sets up the following load balancers on Kubernetes at installation:\n Load balancer for NGINX ingress Load balancer for Istio ingress  Verrazzano allows customizing the load balancers allocated by Oracle Container Engine (OKE) using annotations defined by OKE. For a detailed description of different load balancer customization annotations, see the OKE documentation here.\nThis document describes how to use these annotations to customize the following settings for Verrazzano load balancers:\n Load balancer shape Private IP address and subnet placement  Customize the load balancer shape At installation, Verrazzano lets you customize the shape and size of the load balancers created. Oracle Cloud Infrastructure offers a flexible load balancer which uses Dynamic Shape:\n 10 Mbps 100 Mbps 400 Mbps 8,000 Mbps  For more details on service limits and shape, see here.\nFor example, you can set up an NGINX load balancer with 10Mbps as follows:\napiVersion:install.verrazzano.io/v1alpha1kind:Verrazzanometadata:name:example-verrazzanospec:profile:devenvironmentName:defaultcomponents:ingress:type:LoadBalancernginxInstallArgs:- name:controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-shape\"value:\"10Mbps\"For example, you can set up an Istio load balancer with 10Mbps as follows:\napiVersion:install.verrazzano.io/v1alpha1kind:Verrazzanometadata:name:example-verrazzanospec:profile:devenvironmentName:defaultcomponents:ingress:type:LoadBalanceristioInstallArgs:- name:gateways.istio-ingressgateway.serviceAnnotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-shape\"value:\"10Mbps\"Use private IP addresses with a load balancer At installation, Verrazzano lets you customize the IP address and subnet of the load balancers created. This is achieved using OKE annotations on the NGINX and Istio load balancer services, as documented here.\nThe following example configures the NGINX load balancer service to have a private load balancer IP address on the private subnet identified by OCID ocid1.subnet.oc1.phx.aaaa..sdjxa, and uses the default (public) load balancer configuration for Istio:\napiVersion:install.verrazzano.io/v1alpha1kind:Verrazzanometadata:name:example-verrazzanospec:profile:devenvironmentName:defaultcomponents:ingress:type:LoadBalancernginxInstallArgs:- name:controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-internal\"value:\"true\"- name:controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-subnet1\"value:\"ocid1.subnet.oc1.phx.aaaa..sdjxa\"The following example configures the Istio ingress gateway service to have a private load balancer IP address on the private subnet identified by OCID ocid1.subnet.oc1.phx.aaaa..sdjxa, and uses the default (public) load balancer configuration for NGINX:\napiVersion:install.verrazzano.io/v1alpha1kind:Verrazzanometadata:name:example-verrazzanospec:profile:devenvironmentName:defaultcomponents:ingress:type:LoadBalancer istio:istioInstallArgs:- name:gateways.istio-ingressgateway.serviceAnnotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-internal\"value:\"true\"- name:gateways.istio-ingressgateway.serviceAnnotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-subnet1\"value:\"ocid1.subnet.oc1.phx.aaaa..sdjxa\"The following example configures both NGINX and Istio to have a private load balancer IP address on the private subnet identified by OCID ocid1.subnet.oc1.phx.aaaa..sdjxa:\napiVersion:install.verrazzano.io/v1alpha1kind:Verrazzanometadata:name:example-verrazzanospec:profile:devenvironmentName:defaultcomponents:ingress:type:LoadBalancernginxInstallArgs:- name:controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-internal\"value:\"true\"- name:controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-subnet1\"value:\"ocid1.subnet.oc1.phx.aaaa..sdjxa\"istio:istioInstallArgs:- name:gateways.istio-ingressgateway.serviceAnnotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-internal\"value:\"true\"- name:gateways.istio-ingressgateway.serviceAnnotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-subnet1\"value:\"ocid1.subnet.oc1.phx.aaaa..sdjxa\"","categories":"","description":"Customize load balancers on OKE for Verrazzano system and application endpoints","excerpt":"Customize load balancers on OKE for Verrazzano system and application endpoints","ref":"/docs/setup/customizing/ociloadbalancerips/","tags":"","title":"Customize Load Balancers on OKE"},{"body":"","categories":"","description":"Supported Kubernetes platforms and installation guide for Verrazzano","excerpt":"Supported Kubernetes platforms and installation guide for Verrazzano","ref":"/docs/setup/","tags":"","title":"Setup"},{"body":"ToDo List is an example application containing a WebLogic component. For more information and the source code of this application, see the Verrazzano Examples.\nBefore you begin  Set up a multicluster Verrazzano environment following the installation instructions. The example assumes that there is a managed cluster named managed1 associated with the multicluster environment. If your environment does not have a cluster of that name, then you should edit the deployment files and change the cluster name listed in the placement section. To download the example application image, you must first accept the license agreement.  In a browser, navigate to https://container-registry.oracle.com/ and sign in. Search for example-todo and weblogic. For each one:  Select the image name in the results. From the drop-down menu, select your language and click Continue. Then read and accept the license agreement.      Set up the following environment variables to point to the kubeconfig for the admin and managed clusters.\n$ export KUBECONFIG_ADMIN=/path/to/your/adminclusterkubeconfig $ export KUBECONFIG_MANAGED1=/path/to/your/managedclusterkubeconfig NOTE: The ToDo List application deployment files are contained in the Verrazzano project located at \u003cVERRAZZANO_HOME\u003e/examples/multicluster/todo-list, where \u003cVERRAZZANO_HOME\u003e is the root of the Verrazzano project.\nDeploy the ToDo List example application   Create a namespace for the multicluster ToDo List example by applying the Verrazzano project file.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN apply \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/todo-list/verrazzano-project.yaml   Create a docker-registry secret to enable pulling the ToDo List example image from the registry.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN create secret docker-registry tododomain-repo-credentials \\ --docker-server=container-registry.oracle.com \\ --docker-username=YOUR_REGISTRY_USERNAME \\ --docker-password=YOUR_REGISTRY_PASSWORD \\ --docker-email=YOUR_REGISTRY_EMAIL \\ -n mc-todo-list Replace YOUR_REGISTRY_USERNAME, YOUR_REGISTRY_PASSWORD, and YOUR_REGISTRY_EMAIL with the values you use to access the registry.\n  Create and label secrets for the WebLogic domain:\n# Replace the values of the WLS_USERNAME and WLS_PASSWORD environment variables as appropriate. $ export WLS_USERNAME=\u003cusername\u003e $ export WLS_PASSWORD=\u003cpassword\u003e $ kubectl --kubeconfig $KUBECONFIG_ADMIN create secret generic tododomain-weblogic-credentials \\ --from-literal=password=$WLS_PASSWORD \\ --from-literal=username=$WLS_USERNAME \\ -n mc-todo-list $ kubectl --kubeconfig $KUBECONFIG_ADMIN create secret generic tododomain-jdbc-tododb \\ --from-literal=username=$WLS_USERNAME \\ --from-literal=password=$WLS_PASSWORD \\ -n mc-todo-list $ kubectl --kubeconfig $KUBECONFIG_ADMIN -n mc-todo-list label secret tododomain-jdbc-tododb weblogic.domainUID=tododomain Note that the ToDo List example application is preconfigured to use specific secret names. For the source code of this application, see the Verrazzano Examples.\n  Apply the component and multicluster application resources to deploy the ToDo List application.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN apply \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/todo-list/todo-list-components.yaml $ kubectl --kubeconfig $KUBECONFIG_ADMIN apply \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/todo-list/mc-todo-list-application.yaml   Wait for the ToDo List example application to be ready. The tododomain-adminserver pod may take several minutes to be created and Ready.\n$ kubectl --kubeconfig $KUBECONFIG_MANAGED1 wait pod \\ --for=condition=Ready tododomain-adminserver \\ -n mc-todo-list \\ --timeout=300s   Get the generated host name for the application.\n$ HOST=$(kubectl --kubeconfig $KUBECONFIG_MANAGED1 get gateway \\ -n mc-todo-list \\ -o jsonpath='{.items[0].spec.servers[0].hosts[0]}') $ echo $HOST # Sample output todo-appconf.mc-todo-list.11.22.33.44.nip.io   Get the EXTERNAL_IP address of the istio-ingressgateway service.\n$ ADDRESS=$(kubectl --kubeconfig $KUBECONFIG_MANAGED1 get service \\ -n istio-system istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo $ADDRESS # Sample output 11.22.33.44   Access the ToDo List example application:\n  Using the command line\n# The expected response of this query is the HTML of a web page $ curl -sk https://${HOST}/todo/ \\ --resolve ${HOST}:443:${ADDRESS} If you are using nip.io, then you do not need to include --resolve.\n  Local testing with a browser\nTemporarily, modify the /etc/hosts file (on Mac or Linux) or c:\\Windows\\System32\\Drivers\\etc\\hosts file (on Windows 10), to add an entry mapping the host name to the ingress gateway’s EXTERNAL-IP address. For example:\n11.22.33.44 todo.example.com Then, you can access the application in a browser at https://todo.example.com/todo.\n  Using your own DNS name\n Point your own DNS name to the ingress gateway’s EXTERNAL-IP address. In this case, you would need to have edited the todo-list-application.yaml file to use the appropriate value under the hosts section (such as yourhost.your.domain), before deploying the ToDo List application. Then, you can use a browser to access the application at https://\u003cyourhost.your.domain\u003e/todo/.  Accessing the application in a browser will open a page, “Derek’s ToDo List”, with an edit field and an Add button that lets add tasks.\n    A variety of endpoints associated with the deployed ToDo List application, are available to further explore the logs, metrics, and such. You can access them according to the directions here.\n  Troubleshooting   Verify that the application configuration, domain, and ingress trait all exist.\n$ kubectl --kubeconfig $KUBECONFIG_MANAGED1 get ApplicationConfiguration -n mc-todo-list # Sample output NAME AGE todo-appconf 19h $ kubectl --kubeconfig $KUBECONFIG_MANAGED1 get Domain -n mc-todo-list # Sample output NAME AGE todo-domain 19h $ kubectl --kubeconfig $KUBECONFIG_MANAGED1 get IngressTrait -n mc-todo-list # Sample output NAME AGE todo-domain-trait-7cbd798c96 19h   Verify that the WebLogic Administration Server and MySQL pods have been created and are running. Note that this will take several minutes.\n$ kubectl --kubeconfig $KUBECONFIG_MANAGED1 get pods -n mc-todo-list # Sample output NAME READY STATUS RESTARTS AGE mysql-5c75c8b7f-vlhck 2/2 Running 0 19h tododomain-adminserver 4/4 Running 0 19h   Undeploy the ToDo List application Regardless of its location, to undeploy the application, delete the application resources and the project from the admin cluster. Undeploy affects all clusters in which the application is located.\n  To undeploy the application, delete the ToDo List OAM resources.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN delete \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/todo-list/mc-todo-list-application.yaml $ kubectl --kubeconfig $KUBECONFIG_ADMIN delete \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/todo-list/todo-list-components.yaml   Delete the project.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN delete \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/multicluster/todo-list/verrazzano-project.yaml   Delete the namespace mc-todo-list after the application pods are terminated. The secrets created for the WebLogic domain also will be deleted.\n$ kubectl --kubeconfig $KUBECONFIG_ADMIN delete namespace mc-todo-list $ kubectl --kubeconfig $KUBECONFIG_MANAGED1 delete namespace mc-todo-list   ","categories":"","description":"ToDo List is an example application containing a WebLogic component deployed to a multicluster environment.","excerpt":"ToDo List is an example application containing a WebLogic component deployed to a multicluster environment.","ref":"/docs/samples/multicluster/todo-list/","tags":"","title":"Multicluster ToDo List"},{"body":"Each cluster of a multicluster environment is upgraded separately. Start with the admin cluster, and then for each managed cluster, follow the Upgrade Verrazzano instructions.\nVerify the upgrade of each managed cluster For each managed cluster, follow the instructions in each of the following sections:\n Verify that managed cluster registration completed Verify that managed cluster metrics are being collected Verify that managed cluster logs are being collected  ","categories":"","description":"How to upgrade a multicluster Verrazzano environment","excerpt":"How to upgrade a multicluster Verrazzano environment","ref":"/docs/setup/upgrade/multicluster/","tags":"","title":"Upgrade Multicluster Verrazzano"},{"body":"A Verrazzano application can contain any number of Coherence component workloads, where each workload is a standalone Coherence cluster, independent from other Coherence clusters in the application.\nVerrazzano uses the standard Coherence operator to provision and manage clusters, as documented at Coherence Operator. The Coherence operator uses a CRD, coherence.oracle.com (Coherence resource), to represent a Coherence cluster. When a Verrazzano application with Coherence is provisioned, Verrazzano configures the default logging and metrics for the Coherence cluster. Logs are sent to OpenSearch and metrics to Prometheus.\nYou can view this telemetry data using the OpenSearch Dashboards and Grafana consoles.\nOAM Component The custom resource YAML file for the Coherence cluster is specified as a VerrazzanoCoherenceWorkload custom resource. In the following example, everything under the spec: section is standard Coherence resource YAML that you would typically use to provision a Coherence cluster. Including this Component reference in your ApplicationConfiguration will result in a new Coherence cluster being provisioned. You can have multiple clusters in the same application with no conflict.\napiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: orders namespace: sockshop spec: workload: apiVersion: oam.verrazzano.io/v1alpha1 kind: VerrazzanoCoherenceWorkload spec: template: metadata: name: orders-coh spec: cluster: SockShop ... Life cycle With Verrazzano, you manage the life cycle of applications using Component and ApplicationConfiguration resources. Typically, you would modify the Coherence cluster resource to make changes or to do lifecycle operations, like scale in and scale out. However, in the Verrazzano environment, the cluster resource is owned by the Verrazzano application operator and will be reconciled to match the Component workload resource. Therefore, you need to manage the cluster configuration by modifying the resource, either by kubectl edit or applying a new YAML file. Verrazzano will notice that the Component resource changed and will update the Coherence resource as needed.\nProvisioning When you apply the Component YAML file shown previously, Kubernetes will create a component.oam.verrazzano.io resource, but the Coherence cluster will not be created until you create the ApplicationConfiguration resource, which references the Coherence component. When the application is created, Verrazzano creates a Coherence custom resource for each cluster, which is subsequently processed by the Coherence operator, resulting in a new cluster. After a cluster is created, the Coherence operator will monitor the Coherence resource to reconcile the state of the cluster. You can add a new Coherence workload to a running application, or remove an existing workload, by modifying the ApplicationConfiguration resource, and adding or removing the Coherence component.\nScaling Scaling a Coherence cluster is done by modifying the replicas field in the Component resource. Verrazzano will modify the Coherence resource replicas field and the cluster will be scaled accordingly. The following example configuration shows the replicas field that specifies the number of pods in the cluster.\napiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: orders namespace: sockshop spec: workload: apiVersion: oam.verrazzano.io/v1alpha1 kind: VerrazzanoCoherenceWorkload spec: template: metadata: name: orders-coh spec: cluster: SockShop replicas: 3 ... NOTE: A Coherence cluster provisioned with Verrazzano does not support autoscaling with a Horizontal Pod Autoscaler.\nTermination You can terminate the Coherence cluster by removing the Component from the ApplicationConfiguration or by deleting the ApplicationConfiguration resource entirely.\nNOTE Do not delete the Coherence component if the application is still using it.  Logging When a Coherence cluster is provisioned, Verrazzano configures it to send logs to OpenSearch. This is done by injecting Fluentd sidecar configuration into the Coherence resource. The Coherence operator will create the pod with the Fluentd sidecar. This sidecar periodically copies the Coherence logs from /logs to stdout, enabling the Fluentd DaemonSet in the verrazzano-system namespace to send the logs to OpenSearch. Note that the Fluend sidecar running in the Coherence pod never communicates with OpenSearch or any other network endpoint.\nThe logs are placed in a per-namespace OpenSearch index named verrazzano-namespace-\u003cnamespace\u003e, for example: verrazzano-namespace-sockshop. All logs from Coherence pods in the same namespace will go into the same index, even for different applications. This is standard behavior and there is no way to disable or change it.\nEach log record has some Coherence and application fields, along with the log message itself. For example:\n kubernetes.labels.coherenceCluster SockShop kubernetes.labels.app_oam_dev/name sockshop-appconf kubernetes.labels.app_oam_dev/component orders ... Metrics Verrazzano uses Prometheus to scrape metrics from Coherence cluster pods. Like logging, metrics scraping is also enabled during provisioning, however, the Coherence resource YAML file must have proper metrics configuration. For details, see Coherence Metrics. In summary, there are two ways to configure the Coherence metrics endpoint. Coherence has a default metrics endpoint that you can enable. If your application serves metrics from its own endpoint, such as a Helidon application, then do not use the native Coherence metrics endpoint. To see the difference, examine the socks-shop and bobs-books examples.\nBobs Books The bobs-books example uses the default Coherence metrics endpoint, so the configuration must enable this feature, shown in the following metrics section of the roberts-coherence component in the YAML file, bobs-books-comp.yaml.\ncoherence: metrics: enabled: true Sock Shop The sock-shop example, which is a Helidon application with embedded Coherence, explicitly specifies the metrics port 7001 and doesn’t enable Coherence metrics. Coherence metrics still will be scraped, but not at the default endpoint.\n ports: ... - name: metrics port: 7001 serviceMonitor: enabled: true Because sock-shop components are not using the default Coherence metrics port, you must add a MetricsTrait section to the ApplicationConfiguration for each component, specifying the metrics port as follows:\n - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: MetricsTrait metadata: name: carts-metrics spec: port: 7001 Prometheus configuration Prometheus is configured to scrape targets using the ConfigMaps in the verrazzano-system namespace. During application deployments, Verrazzano updates the vmi-system-prometheus-config ConfigMap and adds targets for the application pods. Verrazzano also annotates those pods to match the expected annotations in the ConfigMap. When the application is deleted, Verrazzano removes the targets from the ConfigMap. You do not need to manually modify the ConfigMap or annotate the application pods.\nHere is an example of thesock-shop Prometheus ConfigMap section for catalog. Notice that pods in the sock-shop namespace with labels app_oam_dev_name and app_oam_dev_component are targeted. Prometheus will find those pods and then look at the pod annotations, verrazzano_io/metricsEnabled, verrazzano_io/metricsPath, and verrazzano_io/metricsPort for scrape configuration.\n- job_name: sockshop-appconf_default_sockshop_catalog ... kubernetes_sd_configs: - role: pod namespaces: names: - sockshop relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_verrazzano_io_metricsEnabled, __meta_kubernetes_pod_label_app_oam_dev_name, __meta_kubernetes_pod_label_app_oam_dev_component] ... - source_labels: [__meta_kubernetes_pod_annotation_verrazzano_io_metricsPath] ... - source_labels: [__address__, __meta_kubernetes_pod_annotation_verrazzano_io_metricsPort] Here is the corresponding catalog pod labels and annotations.\nkind: Pod metadata: labels: ... app.oam.dev/component: catalog app.oam.dev/name: sockshop-appconf annotations: ... verrazzano.io/metricsEnabled: \"true\" verrazzano.io/metricsPath: /metrics verrazzano.io/metricsPort: \"7001\" Istio integration Verrazzano ensures that Coherence clusters are not included in an Istio mesh, even if the namespace has the istio-injection: enabled label. This is done by adding the sidecar.istio.io/inject: \"false\" annotation to the Coherence resource, resulting in Coherence pods being created with that label. However, other application components in the mesh using mutual TLS authentication (mTLS) may need to communicate with Coherence. To handle this case, Verrazzano automatically creates an Istio DestinationRule to disable TLS for the Coherence port. This policy disables mTLS for port 9000, which happens to be used as a Coherence extend port for Bob’s Books.\n trafficPolicy: portLevelSettings: - port: number: 9000 tls: {} ... Currently, port 9000 is the only port where TLS is disabled, so you need to use this as the Coherence extend port if other components in the mesh access Coherence over the extend protocol.\nSummary Verrazzano makes it easy to deploy and observe Coherence clusters in your application, providing seamless integration with other components in your application running in an Istio mesh.\n","categories":"","description":"Use a Coherence workload in an application","excerpt":"Use a Coherence workload in an application","ref":"/docs/applications/workloads/coherence/coherence/","tags":"","title":"Coherence Workload"},{"body":"","categories":"","description":"How to customize Verrazzano installation configurations","excerpt":"How to customize Verrazzano installation configurations","ref":"/docs/setup/customizing/","tags":"","title":"Customize Installations"},{"body":"Helidon is a collection of Java libraries for writing microservices. Helidon provides an open source, lightweight, fast, reactive, cloud native framework for developing Java microservices. It is available as two frameworks:\n Helidon SE is a compact toolkit that embraces the latest Java SE features: reactive streams, asynchronous and functional programming, and fluent-style APIs. Helidon MP implements and supports Eclipse MicroProfile, a baseline platform definition that leverages Java EE and Jakarta EE technologies for microservices and delivers application portability across multiple runtimes.  Helidon is designed and built with container-first philosophy.\n Small footprint, low memory usage and faster startup times. All 3rd party dependencies are stored separately to enable Docker layering. Provides readiness, liveness and customizable health information for container schedulers like Kubernetes.  Containerized Helidon applications are generally deployed as Deployment in Kubernetes.\nVerrazzano integration Verrazzano supports application definition using Open Application Model (OAM). Verrrazzano applications are composed of components and application configurations.\nHelidon applications are first class citizen in Verrazzano with specialized Helidon workload support, for example, VerrazzanoHelidonWorkload. VerrazzanoHelidonWorkload is supported as part of verrazzano-application-operator in the Verrazzano installation and no additional operator setup or installation is required. VerrazzanoHelidonWorkload also supports all the traits and scopes defined by Verrazzano along with core ones defined by the OAM specification.\nVerrazzanoHelidonWorkload is modeled after ContainerizedWorkload, for example, it is used for long-running workloads in containers. However, VerrazzanoHelidonWorkload closely resembles and directly refers to Kubernetes Deployment schema. This enables an easy lift and shift of existing containerized Helidon applications.\nThe complete VerrazzanoHelidonWorkload API definition and description is available at VerrazzanoHelidonWorkload.\nVerrazzano Helidon application development With Verrazzano, you manage the life cycle of applications using Component and ApplicationConfiguration resources. A Verrazzano application can contain any number of VerrazzanoHelidonWorkload components, where each workload is a standalone containerized Helidon application, independent of any other in the application.\nIn the following example, everything under the spec: section is the custom resource YAML file for the containerized Helidon application, as defined by VerrazzanoHelidonWorkload custom resource. Including this Component reference in your ApplicationConfiguration will result in a new containerized Helidon application being provisioned.\napiVersion:core.oam.dev/v1alpha2kind:Componentmetadata:name:hello-helidon-componentnamespace:hello-helidonspec:workload:apiVersion:oam.verrazzano.io/v1alpha1kind:VerrazzanoHelidonWorkloadmetadata:name:hello-helidon-workloadlabels:app:hello-helidonspec:deploymentTemplate:metadata:name:hello-helidon-deploymentpodSpec:containers:- name:hello-helidon-container......The Application Development Guide provides end-to-end instructions for developing and deploying the Verrazzano Helidon application.\nFor more Verrazzano Helidon application examples, see Examples.\nProvisioning When you apply the previous Component YAML file, Kubernetes will create a component.oam.verrazzano.io resource, but the containerized Helidon application will not be created until you create the ApplicationConfiguration resource, which references the VerrazzanoHelidonWorkload component. When the application is created, Verrazzano creates a Deployment and Service resource for each containerized Helidon application.\nTypically, you would modify the Deployment and Service resource to make changes or to do lifecycle operations, like scale in and scale out. However, in the Verrazzano environment, the containerized Helidon application resource is owned by the verrazzano-application-operator and will be reconciled to match the component workload resource. Therefore, you need to manage the application configuration by modifying the VerrazzanoHelidonWorkload or ApplicationConfiguration resource, either by kubectl edit or applying new YAML file. Verrazzano will notice that the Component resource change and will update the Deployment and Service resource as needed.\nYou can add a new VerrazzanoHelidonWorkload to a running application, or remove an existing workload, by modifying the ApplicationConfiguration resource and adding or removing the VerrazzanoHelidonWorkload component.\nScaling The recommended way to scale containerized Helidon application replicas is to specify ManualScalerTrait with VerrazzanoHelidonWorkload in ApplicationConfiguration. The following example configuration shows the replicaCount field that specifies the number of replicas for the application.\n...spec:components:- componentName:hello-helidon-componenttraits:- trait:apiVersion:core.oam.dev/v1alpha2kind:ManualScalerTraitspec:replicaCount:2...Verrazzano will modify the Deployment resource replicas field and the containerized Helidon application replicas will be scaled accordingly.\nNOTE Make sure the replicas defined on the VerrazzanoHelidonWorkload component and that replicaCount defined on ManualScalerTrait for that component matches, or else the DeploymentController in Kubernetes and OAM runtime in verrazzano-application-operator will compete to create a different number of Pods for same containerized Helidon application. To avoid confusion, we recommend that you specify replicaCount defined on ManualScalerTrait and leave replicas undefined on VerrazzanoHelidonWorkload (as it is optional).  Logging When a containerized Helidon application is provisioned on Verrazzano, Verrazzano will configure the default logging and send logs to OpenSearch. Logs can be viewed using the OpenSearch Dashboards console.\nThe logs are placed in a per-namespace OpenSearch index named verrazzano-namespace-\u003cnamespace\u003e, for example: verrazzano-namespace-hello-helidon. All logs from containerized Helidon application pods in the same namespace will go into the same index, even for different applications. This is standard behavior and there is no way to disable or change it.\nMetrics Verrazzano uses Prometheus to scrape metrics from containerized Helidon application pods. Like logging, metrics scraping is also enabled during provisioning. Metrics can be viewed using the Grafana console.\nVerrazzano lets you to customize configuration information needed to enable metrics using MetricsTrait for an application component.\nIngress Verrazzano lets you to configure traffic routing to a containerized Helidon application, using IngressTrait for an application component.\nTroubleshooting Whenever you have a problem with your Verrazzano Helidon application, there are some basic techniques you can use to troubleshoot. Troubleshooting shows you some simple things to try when troubleshooting, as well as how to solve common problems you may encounter.\n","categories":"","description":"Develop Helidon applications on Verrazzano","excerpt":"Develop Helidon applications on Verrazzano","ref":"/docs/applications/workloads/helidon/helidon/","tags":"","title":"Helidon Workload"},{"body":"Before you begin Install Verrazzano by following the installation instructions.\nNOTE: The Sock Shop example application deployment files are contained in the Verrazzano project located at \u003cVERRAZZANO_HOME\u003e/examples/sockshop, where \u003cVERRAZZANO_HOME\u003e is the root of the Verrazzano project.\nDeploy the Sock Shop application This example application provides various implementations of the Sock Shop Microservices Demo Application. It uses OAM resources to define the application deployment.\n Coherence and Helidon in the helidon subdirectory. Coherence and Micronaut in the micronaut subdirectory. Coherence and Spring in the spring subdirectory.  NOTE To run this application in the default namespace:\n$kubectl label namespace default verrazzano-managed=true If you chose the default namespace, you can skip Step 1. and ignore the -n option in the rest of the commands.\n   Create a namespace for the Sock Shop application and add a label identifying the namespace as managed by Verrazzano.\n$ kubectl create namespace sockshop $ kubectl label namespace sockshop verrazzano-managed=true   To deploy the application, apply the Sock Shop OAM resources. Choose to deploy either the helidon, micronaut, or spring variant.\n      $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/sock-shop/helidon/sock-shop-comp.yaml -n sockshop $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/sock-shop/helidon/sock-shop-app.yaml -n sockshop   $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/sock-shop/micronaut/sock-shop-comp.yaml -n sockshop $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/sock-shop/micronaut/sock-shop-app.yaml -n sockshop   $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/sock-shop/spring/sock-shop-comp.yaml -n sockshop $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/sock-shop/spring/sock-shop-app.yaml -n sockshop      Wait for the Sock Shop application to be ready.\n$ kubectl wait \\ --for=condition=Ready pods \\ --all -n sockshop \\ --timeout=300s   Explore the application The Sock Shop microservices application implements REST API endpoints including:\n /catalogue - Returns the Sock Shop catalog. This endpoint accepts the GET HTTP request method. /register - POST { \"username\":\"xxx\", \"password\":\"***\", \"email\":\"foo@example.com\", \"firstName\":\"foo\", \"lastName\":\"bar\" } to create a user. This endpoint accepts the POST HTTP request method.  NOTE: The following instructions assume that you are using a Kubernetes environment, such as OKE. Other environments or deployments may require alternative mechanisms for retrieving addresses, ports, and such.\nFollow these steps to test the endpoints:\n  Get the generated host name for the application.\n$ HOST=$(kubectl get gateways.networking.istio.io \\ -n sockshop \\ -o jsonpath={.items[0].spec.servers[0].hosts[0]}) $ echo $HOST # Sample output sockshop-appconf.sockshop.11.22.33.44.nip.io   Get the EXTERNAL_IP address of the istio-ingressgateway service.\n$ ADDRESS=$(kubectl get service \\ -n istio-system istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo $ADDRESS # Sample output 11.22.33.44   Access the Sock Shop application:\n  Using the command line\n# Get catalogue $ curl -sk \\ -X GET \\ https://${HOST}/catalogue \\ --resolve ${HOST}:443:${ADDRESS} # Sample output [{\"count\":115,\"description\":\"For all those leg lovers out there....\", ...}] # Add a new user (replace values of username and password) $ curl -i \\ --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"foo\",\"password\":\"****\",\"email\":\"foo@example.com\",\"firstName\":\"foo\",\"lastName\":\"foo\"}' \\ -k https://${HOST}/register \\ --resolve ${HOST}:443:${ADDRESS} # Add an item to the user's cart $ curl -i \\ --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"itemId\": \"a0a4f044-b040-410d-8ead-4de0446aec7e\",\"unitPrice\": \"7.99\"}' \\ -k https://${HOST}/carts/{username}/items \\ --resolve ${HOST}:443:${ADDRESS} # Get cart items $ curl -i \\ -k https://${HOST}/carts/{username}/items \\ --resolve ${HOST}:443:${ADDRESS} # Sample output [{\"itemId\":\"a0a4f044-b040-410d-8ead-4de0446aec7e\",\"quantity\":1,\"unitPrice\":7.99}] If you are using nip.io, then you do not need to include --resolve.\n  Local testing with a browser\nTemporarily, modify the /etc/hosts file (on Mac or Linux) or c:\\Windows\\System32\\Drivers\\etc\\hosts file (on Windows 10), to add an entry mapping the host name to the ingress gateway’s EXTERNAL-IP address. For example:\n11.22.33.44 sockshop.example.com Then, you can access the application in a browser at https://sockshop.example.com/catalogue.\nIf you are using nip.io, then you can access the application in a browser using the HOST variable (for example, https://${HOST}/catalogue). If you are going through a proxy, you may need to add *.nip.io to the NO_PROXY list.\n  Using your own DNS name\n Point your own DNS name to the ingress gateway’s EXTERNAL-IP address. In this case, you would need to edit the sock-shop-app.yaml file to use the appropriate value under the hosts section (such as yourhost.your.domain), before deploying the Sock Shop application. Then, you can use a browser to access the application at https://\u003cyourhost.your.domain\u003e/catalogue.      A variety of endpoints associated with the deployed application, are available to further explore the logs, metrics, and such. You can access them according to the directions here.\n  Troubleshooting   Verify that the application configuration, component, workload, and ingress trait all exist.\n$ kubectl get ApplicationConfiguration -n sockshop $ kubectl get Component -n sockshop $ kubectl get VerrazzanoCoherenceWorkload -n sockshop $ kubectl get IngressTrait -n sockshop   Verify that the Sock Shop service pods are successfully created and transition to the READY state. Note that this may take a few minutes and that you may see some of the services terminate and restart.\n $ kubectl get pods -n sockshop # Sample output NAME READY STATUS RESTARTS AGE carts-coh-0 1/1 Running 0 41s catalog-coh-0 1/1 Running 0 40s orders-coh-0 1/1 Running 0 39s payment-coh-0 1/1 Running 0 37s shipping-coh-0 1/1 Running 0 36s users-coh-0 1/1 Running 0 35s   Undeploy the application   To undeploy the application, delete the Sock Shop OAM resources. Choose to undeploy either the helidon, micronaut, or spring variant.\n      $ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/sock-shop/helidon/sock-shop-comp.yaml -n sockshop $ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/sock-shop/helidon/sock-shop-app.yaml -n sockshop   $ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/sock-shop/micronaut/sock-shop-comp.yaml -n sockshop $ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/sock-shop/micronaut/sock-shop-app.yaml -n sockshop   $ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/sock-shop/spring/sock-shop-comp.yaml -n sockshop $ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/sock-shop/spring/sock-shop-app.yaml -n sockshop      Delete the namespace sockshop after the application pods are terminated.\n$ kubectl delete namespace sockshop   ","categories":"","description":"Implementations of the Sock Shop Microservices Demo Application","excerpt":"Implementations of the Sock Shop Microservices Demo Application","ref":"/docs/samples/sock-shop/","tags":"","title":"Sock Shop"},{"body":"Get the consoles URLs Verrazzano installs several consoles. The endpoints for an installation are stored in the Status field of the installed Verrazzano Custom Resource. You can get the endpoints for these consoles by issuing the following command and examining the Status.Instance field:\n$ kubectl get vz -o yaml\nThe resulting output is similar to the following (abbreviated to show only the relevant portions):\n ... status: conditions: - lastTransitionTime: \"2021-06-30T03:10:00Z\" message: Verrazzano install in progress status: \"True\" type: InstallStarted - lastTransitionTime: \"2021-06-30T03:18:33Z\" message: Verrazzano install completed successfully status: \"True\" type: InstallComplete instance: consoleUrl: https://verrazzano.default.11.22.33.44.nip.io elasticUrl: https://elasticsearch.vmi.system.default.11.22.33.44.nip.io grafanaUrl: https://grafana.vmi.system.default.11.22.33.44.nip.io keyCloakUrl: https://keycloak.default.11.22.33.44.nip.io kialiUrl: https://kiali.vmi.system.default.11.22.33.44.nip.io kibanaUrl: https://kibana.vmi.system.default.11.22.33.44.nip.io prometheusUrl: https://prometheus.vmi.system.default.11.22.33.44.nip.io rancherUrl: https://rancher.default.11.22.33.44.nip.io If you have jq installed, then you can use the following command to get the instance URLs more directly:\n$ kubectl get vz -o jsonpath=\"{.items[].status.instance}\" | jq .\nThe following is an example of the output:\n{ \"consoleUrl\": \"https://verrazzano.default.11.22.33.44.nip.io\", \"elasticUrl\": \"https://elasticsearch.vmi.system.default.11.22.33.44.nip.io\", \"grafanaUrl\": \"https://grafana.vmi.system.default.11.22.33.44.nip.io\", \"keyCloakUrl\": \"https://keycloak.default.11.22.33.44.nip.io\", \"kialiUrl\": \"https://kiali.vmi.system.default.11.22.33.44.nip.io\", \"kibanaUrl\": \"https://kibana.vmi.system.default.11.22.33.44.nip.io\", \"prometheusUrl\": \"https://prometheus.vmi.system.default.11.22.33.44.nip.io\", \"rancherUrl\": \"https://rancher.default.11.22.33.44.nip.io\" } Get consoles credentials You will need the credentials to access the consoles installed by Verrazzano.\nConsoles accessed by the same user name/password  Grafana Prometheus OpenSearch Dashboards OpenSearch Kiali  User: verrazzano\nTo get the password:\n$ kubectl get secret \\ --namespace verrazzano-system verrazzano \\ -o jsonpath={.data.password} | base64 \\ --decode; echo The Keycloak admin console User: keycloakadmin\nTo get the password:\n$ kubectl get secret \\ --namespace keycloak keycloak-http \\ -o jsonpath={.data.password} | base64 \\ --decode; echo The Rancher console User: admin\nTo get the password:\n$ kubectl get secret \\ --namespace cattle-system rancher-admin-secret \\ -o jsonpath={.data.password} | base64 \\ --decode; echo Change the Verrazzano password To change the Verrazzano password, first change the user password in Keycloak and then update the Verrazzano secret.\nChange the user in Keycloak\n  Navigate to the Keycloak admin console.\na. Obtaining the Keycloak admin console URL is described here.\nb. Obtaining the Keycloak admin console credentials is described here.\n  In the left pane, under Manage, select Users.\n  In the Users pane, search for verrazzano or click View all users.\n  For the verrazzano user, click the Edit action.\n  At the top, select the Credentials tab.\n  Specify the new password and confirm.\n  Specify whether the new password is a temporary password. A temporary password must be reset on next login.\n  Click Reset Password.\n  Confirm the password reset by clicking Reset password in the confirmation dialog.\n  Update the Verrazzano secret\nGet the base64 encoding for your new password:\n$ echo -n 'MyNewPwd' | base64\nUpdate the password in the secret:\n$ kubectl edit secret verrazzano -n verrazzano-system\nReplace the existing password value with the new base64 encoded value.\n","categories":"","description":"Information and tools to support operating Verrazzano","excerpt":"Information and tools to support operating Verrazzano","ref":"/docs/access/","tags":"","title":"Access Verrazzano"},{"body":"The Verrazzano AuthProxy component enables authentication and authorization for Keycloak users accessing Verrazzano resources. You can customize the AuthProxy component using settings in the Verrazzano custom resource.\nThe following table describes the fields in the Verrazzano custom resource pertaining to the AuthProxy component.\n   Path to Field Description     spec.components.authProxy.kubernetes.replicas The number of pods to replicate. The default is 2 for the prod profile and 1 for all other profiles.   spec.components.authProxy.kubernetes.affinity The pod affinity definition expressed as a standard Kubernetes affinity definition. The default configuration spreads the AuthProxy pods across the available nodes. spec:\ncomponents:\nauthProxy:\nkubernetes:\naffinity:\npodAntiAffinity:\npreferredDuringSchedulingIgnoredDuringExecution:\n- weight: 100\npodAffinityTerm:\nlabelSelector:\nmatchExpressions:\n- key: app\noperator: In\nvalues:\n- verrazzano-authproxy\ntopologyKey: kubernetes.io/hostname    The following example customizes a Verrazzano prod profile as follows:\n Increases the replicas count to 3 Changes the podAffinity configuration to use requiredDuringSchedulingIgnoredDuringExecution  apiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: example-verrazzano spec: profile: prod components: authproxy: kubernetes: replicas: 3 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - verrazzano-authproxy topologyKey: kubernetes.io/hostname ","categories":"","description":"Customize Verrazzano AuthProxy installation settings","excerpt":"Customize Verrazzano AuthProxy installation settings","ref":"/docs/setup/customizing/authproxy/","tags":"","title":"Customize AuthProxy"},{"body":"Summary Analysis detected that there were pods that had issues due to failures to pull an image or images.\nThe analysis was not able to identify a specific root cause, however, it might have supplied data that is related to the failures.\nSteps  Review the analysis data. At a minimum, it will indicate which pods are being impacted and might give other clues on the root cause. If the service is experiencing an outage, then consult the specific service status page. For common service status pages, see Related information.  Related information  GitHub Status Oracle Cloud Infrastructure Status Kubernetes Troubleshooting  ","categories":"","description":"Analysis detected an image pull back off issue","excerpt":"Analysis detected an image pull back off issue","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/imagepullbackoff/","tags":"","title":"Image Pull Back Off"},{"body":"Summary Analysis detected that there were pods which had issues due to failures to pull an image or images where the root cause was that the image was not found.\nSteps  Review the analysis data; it enumerates the pods and related messages regarding which images had this issue. Confirm that the image name, digest, and tag are correctly specified.  Related information  Kubernetes Troubleshooting  ","categories":"","description":"Analysis detected an image pull not found issue","excerpt":"Analysis detected an image pull not found issue","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/imagepullnotfound/","tags":"","title":"Image Pull Not Found"},{"body":"Summary Analysis detected that there were pods which had issues due to failures to pull an image or images.\nThe root cause was rate limit exceeded errors while pulling images.\nSteps  Review the analysis data; it enumerates the pods and related messages regarding which images had this issue. The detailed messages might provide specific instructions for the registry that is involved. For example, it might provide a link to instructions on how to increase the limit.  Related information  Increase Rate Limits  ","categories":"","description":"Analysis detected an image pull rate limit issue","excerpt":"Analysis detected an image pull rate limit issue","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/imagepullratelimit/","tags":"","title":"Image Pull Rate Limit"},{"body":"Summary Analysis detected that there were pods which had issues due to failures to pull an image or images where the root cause was that the service was not available.\nThe service might be unreachable or might be incorrect.\nSteps  Review the analysis data; it enumerates the pods and related messages about which images had this issue. Confirm that the registry for the image is correct. The messages might identify a connectivity issue. If the service is experiencing an outage, then consult the specific service status page. For common service status pages, see Related information.  Related information  GitHub Status Oracle Cloud Infrastructure Status Kubernetes Troubleshooting  ","categories":"","description":"Analysis detected an image pull service issue","excerpt":"Analysis detected an image pull service issue","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/imagepullservice/","tags":"","title":"Image Pull Service Issue"},{"body":"Summary Analysis detected that the Verrazzano installation failed while installing the NGINX Ingress Controller.\nThe root cause appears to be that the load balancer service limit has been reached.\nSteps  Review the messages from the supporting details for the exact limits, and delete unused load balancers. If available, use a different load balancer shape. See Customizing Ingress. Refer to the Oracle Cloud Infrastructure documentation on Service Limits.  Related information  Platform Setup Kubernetes Troubleshooting More information on load balancers  ","categories":"","description":"Analysis detected that the load balancer service limit was exceeded","excerpt":"Analysis detected that the load balancer service limit was exceeded","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/ingresslblimitexceeded/","tags":"","title":"Ingress Controller Load Balancer Service Limit Reached"},{"body":"Summary Analysis detected that the Verrazzano installation failed while installing the NGINX Ingress Controller.\nThe root cause appears to be that the load balancer is either missing or unable to set the ingress IP address on the NGINX Ingress service.\nSteps Refer to the platform-specific environment setup for your platform here.\nRelated information  Platform Setup Kubernetes Troubleshooting  ","categories":"","description":"Analysis detected load balancer missing ingress IP address","excerpt":"Analysis detected load balancer missing ingress IP address","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/ingressnoloadbalancerip/","tags":"","title":"Ingress Controller No Load Balancer IP"},{"body":"Summary Analysis detected that the Verrazzano installation failed while installing the NGINX Ingress Controller.\nThe root cause appears to be that an Oracle Cloud Infrastructure IP non-ephemeral address limit has been reached.\nSteps  Review the messages from the supporting details for the exact limit. Refer to the Oracle Cloud Infrastructure documentation related to managing IP Addresses.  Related information  Public IP Addresses  ","categories":"","description":"Analysis detected ingress controller Oracle Cloud Infrastructure IP limit exceeded","excerpt":"Analysis detected ingress controller Oracle Cloud Infrastructure IP limit exceeded","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/ingressociiplimitexceeded/","tags":"","title":"Ingress Controller Oracle Cloud Infrastructure IP Limit Exceeded"},{"body":"Summary Analysis detected that the Verrazzano installation has failed, however, it did not isolate the exact reason for the failure.\nSteps Review the analysis data, which can help identify the issue.\nRelated information  Installation Guide Kubernetes Troubleshooting  ","categories":"","description":"Analysis detected an installation failure","excerpt":"Analysis detected an installation failure","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/installfailure/","tags":"","title":"Install Failure"},{"body":"Summary Analysis detected that the Verrazzano installation has failed related to the NGINX Ingress Controller, however, it was unable to isolate the specific root cause.\nSteps Review the analysis data, which might help identify the issue.\nRelated information  Installation Guide Platform Setup Kubernetes Troubleshooting  ","categories":"","description":"Analysis detected an install ingress controller failure","excerpt":"Analysis detected an install ingress controller failure","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/ingressinstallfailure/","tags":"","title":"Install Ingress Controller Failure"},{"body":"Prerequisites Before you begin, read this document, Verrazzano in a multicluster environment.\nOverview To set up a multicluster Verrazzano environment, you will need two or more Kubernetes clusters. One of these clusters will the admin cluster; the others will be managed clusters.\nThe instructions here assume an admin cluster and a single managed cluster. For each additional managed cluster, simply repeat the managed cluster instructions.\nInstall Verrazzano Install Verrazzano on each Kubernetes cluster.\n On one cluster, install Verrazzano using the dev or prod profile; this will be the admin cluster. On the other cluster, install Verrazzano using the managed-cluster profile; this will be a managed cluster. The managed-cluster profile contains only the components that are required for a managed cluster. Create the environment variables, KUBECONFIG_ADMIN, KUBECONTEXT_ADMIN, KUBECONFIG_MANAGED1, and KUBECONTEXT_MANAGED1, and point them to the kubeconfig files and contexts for the admin and managed cluster, respectively. You will use these environment variables in subsequent steps when registering the managed cluster. The following shows an example of how to set these environment variables. $ export KUBECONFIG_ADMIN=/path/to/your/adminclusterkubeconfig $ export KUBECONFIG_MANAGED1=/path/to/your/managedclusterkubeconfig # lists the contexts in each kubeconfig file $ kubectl --kubeconfig $KUBECONFIG_ADMIN config get-contexts -o=name my-admin-cluster-context some-other-cluster-context $ kubectl --kubeconfig $KUBECONFIG_MANAGED1 config get-contexts -o=name my-managed-cluster-context some-other-cluster2-context # Choose the right context name for your admin and managed clusters from the output shown and set the KUBECONTEXT # environment variables $ export KUBECONTEXT_ADMIN=\u003cadmin-cluster-context-name\u003e $ export KUBECONTEXT_MANAGED1=\u003cmanaged-cluster-context-name\u003e   For detailed instructions on how to install and customize Verrazzano on a Kubernetes cluster using a specific profile, see the Installation Guide and Installation Profiles.\nRegister the managed cluster with the admin cluster The following sections show you how to register the managed cluster with the admin cluster. As indicated, some of these steps are performed on the admin cluster and some on the managed cluster. The commands provided use the environment variables set previously to connect to the appropriate cluster.\nPreregistration setup Before registering the managed cluster, first you’ll need to set up the following items:\n A Secret containing the managed cluster’s CA certificate. Note that the cacrt field in this secret can be empty only if the managed cluster uses a well-known CA. This CA certificate is used by the admin cluster to scrape metrics from the managed cluster, for both applications and Verrazzano components. A ConfigMap containing the externally reachable address of the admin cluster. This will be provided to the managed cluster during registration so that it can connect to the admin cluster.  Follow these preregistration setup steps:\n  If needed for the admin cluster, obtain the managed cluster’s CA certificate. The admin cluster scrapes metrics from the managed cluster’s Prometheus endpoint. If the managed cluster Verrazzano installation uses self-signed certificates or LetsEncrypt staging certificates, then the admin cluster will need the managed cluster’s CA certificate to make an https connection.\n Depending on whether the Verrazzano installation on the managed cluster uses self-signed certificates, LetsEncrypt staging certificates, or certificates signed by a well-known certificate authority, choose the appropriate instructions. If you are unsure what type of certificates are used, use the following instructions.   To check if the verrazzano resource is configured to use LetsEncrypt staging certificates:\n# On the managed cluster $ kubectl --kubeconfig $KUBECONFIG_MANAGED1 --context $KUBECONTEXT_MANAGED1 \\ describe verrazzano If the output contains the following information, then LetsEncrypt staging certificates are being used.\nCert Manager: Certificate: Acme: Environment: staging Provider: letsEncrypt   To check the ca.crt field of the system-tls secret in the verrazzano-system namespace on the managed cluster:\n# On the managed cluster $ kubectl --kubeconfig $KUBECONFIG_MANAGED1 --context $KUBECONTEXT_MANAGED1 \\ -n verrazzano-system get secret system-tls -o jsonpath='{.data.ca\\.crt}' If this value is empty, then your managed cluster is using certificates signed by a well-known certificate authority. Otherwise, your managed cluster is using self-signed certificates.\n      In this case, no additional configuration is necessary.  If the managed cluster certificates are self-signed, create a file called managed1.yaml containing the CA certificate of the managed cluster as the value of the cacrt field. In the following commands, the managed cluster’s CA certificate is saved in an environment variable called MGD_CA_CERT. Then use the --dry-run option of the kubectl command to generate the managed1.yaml file.\n# On the managed cluster $ MGD_CA_CERT=$(kubectl --kubeconfig $KUBECONFIG_MANAGED1 --context $KUBECONTEXT_MANAGED1 \\ get secret system-tls \\ -n verrazzano-system \\ -o jsonpath=\"{.data.ca\\.crt}\" | base64 --decode) $ kubectl --kubeconfig $KUBECONFIG_MANAGED1 --context $KUBECONTEXT_MANAGED1 \\ create secret generic \"ca-secret-managed1\" \\ -n verrazzano-mc \\ --from-literal=cacrt=\"$MGD_CA_CERT\" \\ --dry-run=client \\ -o yaml \u003e managed1.yaml Create a Secret on the admin cluster that contains the CA certificate for the managed cluster. This secret will be used for scraping metrics from the managed cluster. The managed1.yaml file that was created in the previous step provides input to this step.\n# On the admin cluster $ kubectl --kubeconfig $KUBECONFIG_ADMIN --context $KUBECONTEXT_ADMIN \\ apply -f managed1.yaml # Once the command succeeds, you may delete the managed1.yaml file $ rm managed1.yaml   If the managed cluster certificates are LetsEncrypt staging, then create a file called managed1.yaml containing the CA certificate of the managed cluster as the value of the cacrt field. In the following commands, the managed cluster’s CA certificate is saved in an environment variable called MGD_CA_CERT. Then use the --dry-run option of the kubectl command to generate the managed1.yaml file.\n# On the admin cluster $ MGD_CA_CERT=$(kubectl --kubeconfig $KUBECONFIG_ADMIN --context $KUBECONTEXT_ADMIN \\ get secret tls-ca-additional \\ -n cattle-system \\ -o jsonpath=\"{.data.ca-additional\\.pem}\" | base64 --decode) $ kubectl --kubeconfig $KUBECONFIG_ADMIN --context $KUBECONTEXT_ADMIN \\ create secret generic \"ca-secret-managed1\" \\ -n verrazzano-mc \\ --from-literal=cacrt=\"$MGD_CA_CERT\" \\ --dry-run=client \\ -o yaml \u003e managed1.yaml Create a Secret on the admin cluster that contains the CA certificate for the managed cluster. This secret will be used for scraping metrics from the managed cluster. The managed1.yaml file that was created in the previous step provides input to this step.\n# On the admin cluster $ kubectl --kubeconfig $KUBECONFIG_ADMIN --context $KUBECONTEXT_ADMIN \\ apply -f managed1.yaml # After the command succeeds, you may delete the managed1.yaml file $ rm managed1.yaml          Use the following instructions to obtain the Kubernetes API server address for the admin cluster. This address must be accessible from the managed cluster.\n     For most types of Kubernetes clusters, except for Kind clusters, you can find the externally accessible API server address of the admin cluster from its kubeconfig file.\n# View the information for the admin cluster in your kubeconfig file $ kubectl --kubeconfig $KUBECONFIG_ADMIN --context $KUBECONTEXT_ADMIN config view --minify # Sample output apiVersion: v1 kind: Config clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://11.22.33.44:6443 name: my-admin-cluster contexts: .... .... In the output of this command, you can find the URL of the admin cluster API server from the server entry. Set the value of the ADMIN_K8S_SERVER_ADDRESS variable to this URL.\nexport ADMIN_K8S_SERVER_ADDRESS=\u003cthe server address from the config output\u003e   Kind clusters run within a Docker container. If your admin and managed clusters are Kind clusters, the API server address of the admin cluster in its kubeconfig file is usually a local address on the host machine, which will not be accessible from the managed cluster. Use the kind command to obtain the “internal” kubeconfig of the admin cluster, which will contain a server address accessible from other Kind clusters on the same machine, and therefore in the same Docker network.\n$ kind get kubeconfig --internal --name \u003cyour-admin-cluster-name\u003e | grep server In the output of this command, you can find the URL of the admin cluster API server from the server entry. Set the value of the ADMIN_K8S_SERVER_ADDRESS variable to this URL.\nexport ADMIN_K8S_SERVER_ADDRESS=\u003cthe server address from the config output\u003e      On the admin cluster, create a ConfigMap that contains the externally accessible admin cluster Kubernetes server address found in the previous step.\n# On the admin cluster $ kubectl --kubeconfig $KUBECONFIG_ADMIN --context $KUBECONTEXT_ADMIN \\ apply -f \u003c\u003cEOF - apiVersion: v1 kind: ConfigMap metadata: name: verrazzano-admin-cluster namespace: verrazzano-mc data: server: \"${ADMIN_K8S_SERVER_ADDRESS}\" EOF   Registration steps Perform the first three registration steps on the admin cluster, and the last step, on the managed cluster. The cluster against which to run the command is indicated in each code block.\nOn the admin cluster   To begin the registration process for a managed cluster named managed1, apply the VerrazzanoManagedCluster object on the admin cluster.\n# On the admin cluster $ kubectl --kubeconfig $KUBECONFIG_ADMIN --context $KUBECONTEXT_ADMIN \\ apply -f \u003c\u003cEOF - apiVersion: clusters.verrazzano.io/v1alpha1 kind: VerrazzanoManagedCluster metadata: name: managed1 namespace: verrazzano-mc spec: description: \"Test VerrazzanoManagedCluster object\" caSecret: ca-secret-managed1 EOF   Wait for the VerrazzanoManagedCluster resource to reach the Ready status. At that point, it will have generated a YAML file that must be applied on the managed cluster to complete the registration process.\n# On the admin cluster $ kubectl --kubeconfig $KUBECONFIG_ADMIN --context $KUBECONTEXT_ADMIN \\ wait --for=condition=Ready \\ vmc managed1 -n verrazzano-mc   Export the YAML file created to register the managed cluster.\n# On the admin cluster $ kubectl --kubeconfig $KUBECONFIG_ADMIN --context $KUBECONTEXT_ADMIN \\ get secret verrazzano-cluster-managed1-manifest \\ -n verrazzano-mc \\ -o jsonpath={.data.yaml} | base64 --decode \u003e register.yaml   On the managed cluster Apply the registration file exported in the previous step, on the managed cluster.\n# On the managed cluster $ kubectl --kubeconfig $KUBECONFIG_MANAGED1 --context $KUBECONTEXT_MANAGED1 \\ apply -f register.yaml # Once the command succeeds, you may delete the register.yaml file $ rm register.yaml After this step, the managed cluster will begin connecting to the admin cluster periodically. When the managed cluster connects to the admin cluster, it will update the Status field of the VerrazzanoManagedCluster resource for this managed cluster, with the following information:\n The timestamp of the most recent connection made from the managed cluster, in the lastAgentConnectTime status field. The host address of the Prometheus instance running on the managed cluster, in the prometheusHost status field. This is then used by the admin cluster to scrape metrics from the managed cluster. The API address of the managed cluster, in the apiUrl status field. This is used by the admin cluster’s authentication proxy to route incoming requests for managed cluster information, to the managed cluster’s authentication proxy.  Verify that managed cluster registration completed You can perform all the verification steps on the admin cluster.\n  Verify that the managed cluster can connect to the admin cluster. View the status of the VerrazzanoManagedCluster resource on the admin cluster, and check whether the lastAgentConnectTime, prometheusUrl, and apiUrl fields are populated. This may take up to two minutes after completing the registration steps.\n# On the admin cluster $ kubectl --kubeconfig $KUBECONFIG_ADMIN --context $KUBECONTEXT_ADMIN \\ get vmc managed1 -n verrazzano-mc -o yaml # Sample output showing the status field spec: .... .... status: apiUrl: https://verrazzano.default.172.18.0.211.nip.io conditions: - lastTransitionTime: \"2021-07-07T15:49:43Z\" message: Ready status: \"True\" type: Ready lastAgentConnectTime: \"2021-07-16T14:47:25Z\" prometheusHost: prometheus.vmi.system.default.172.18.0.211.nip.io   Verify that the managed cluster is successfully registered with Rancher. When you perform the registration steps, Verrazzano also registers the managed cluster with Rancher. View the Rancher UI on the admin cluster. If the registration with Rancher was successful, then your cluster will be listed in Rancher’s list of clusters, and will be in Active state. You can find the Rancher UI URL for your cluster by following the instructions for Accessing Verrazzano.\n  Verify that managed cluster metrics are being collected Verify that the admin cluster is collecting metrics from the managed cluster. The Prometheus output will include records that contain the name of the Verrazzano cluster (labeled as verrazzano_cluster).\nYou can find the Prometheus UI URL for your cluster by following the instructions for Accessing Verrazzano. Execute a query for a metric (for example, node_disk_io_time_seconds_total).\nSample output of a Prometheus query\nAn alternative approach to using the Prometheus UI is to query metrics from the command line. Here is an example of how to obtain Prometheus metrics from the command line. Search the output of the query for responses that have the verrazzano_cluster field set to the name of the managed cluster.\n# On the admin cluster $ prometheusUrl=$(kubectl --kubeconfig $KUBECONFIG_ADMIN --context $KUBECONTEXT_ADMIN \\ get verrazzano -o jsonpath='{.items[0].status.instance.prometheusUrl}') $ VZPASS=$(kubectl --kubeconfig $KUBECONFIG_ADMIN --context $KUBECONTEXT_ADMIN \\ get secret verrazzano --namespace verrazzano-system \\ -o jsonpath={.data.password} | base64 --decode; echo) $ curl -k --user verrazzano:${VZPASS} \"${prometheusUrl}/api/v1/query?query=node_disk_io_time_seconds_total\" Verify that managed cluster logs are being collected Verify that the admin cluster is collecting logs from the managed cluster. The output will include records which have the name of the managed cluster in the cluster_name field.\nYou can find the OpenSearch Dashboards UI URL for your cluster by following the instructions for Accessing Verrazzano. Create an index for verrazzano-namespace-verrazzano-system. Some log records will have the cluster_name field populated with the name of the managed cluster.\nSample output of a OpenSearch Dashboards screen\nAn alternative approach to using the OpenSearch Dashboards UI is to query OpenSearch from the command line. Here is an example of how to obtain log records from the command line. Search the output of the query for responses that have the cluster_name field set to the name of the managed cluster.\n# On the admin cluster $ OSD_URL=$(kubectl --kubeconfig $KUBECONFIG_ADMIN --context $KUBECONTEXT_ADMIN \\ get verrazzano -o jsonpath='{.items[0].status.instance.kibanaUrl}') $ VZPASS=$(kubectl --kubeconfig $KUBECONFIG_ADMIN --context $KUBECONTEXT_ADMIN \\ get secret verrazzano --namespace verrazzano-system \\ -o jsonpath={.data.password} | base64 --decode; echo) $ curl -k --user verrazzano:${VZPASS} -X POST -H 'kbn-xsrf: true' \"${OSD_URL}/elasticsearch/verrazzano-namespace-verrazzano-system/_search?size=25\" Run applications in multicluster Verrazzano The Verrazzano multicluster setup is now complete and you can deploy applications by following the Multicluster Hello World Helidon example application.\nUse the admin cluster UI The admin cluster serves as a central point from which to register and deploy applications to managed clusters.\nIn the Verrazzano UI on the admin cluster, you can view the following:\n The managed clusters registered with this admin cluster. VerrazzanoProjects located on this admin cluster, or any of its registered managed clusters, or both. Applications located on this admin cluster, or any of its registered managed clusters, or both.  ","categories":"","description":"How to set up a multicluster Verrazzano environment","excerpt":"How to set up a multicluster Verrazzano environment","ref":"/docs/setup/install/multicluster/","tags":"","title":"Install Multicluster Verrazzano"},{"body":"Summary Analysis detected that there were nodes reporting insufficient memory.\nSteps   Review the analysis data to identify the specific nodes involved.\n  Review the nodes to determine why they do not have sufficient memory.\na. Are the nodes sized correctly for the workload?\n For the minimum resources required for installing Verrazzano, see the Installation Guide. Refer to documentation for other applications that you are deploying for resource guidelines and take those into account.  b. Is something unexpected running on the nodes or consuming more memory than expected?\n  Related information  Installation Guide Kubernetes Troubleshooting  ","categories":"","description":"Analysis detected nodes reporting insufficient memory","excerpt":"Analysis detected nodes reporting insufficient memory","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/insufficientmemory/","tags":"","title":"Insufficient Memory"},{"body":"Verrazzano can be deployed to a number of different hosted and on-premises Kubernetes environments. Particularly in hosted environments, it may not be possible to choose the authentication providers configured for the Kubernetes API server, and Verrazzano may have no ability to view, manage, or authenticate users.\nVerrazzano installs Keycloak to provide a common user store across all Kubernetes environments. The Verrazzano admin user can create and manage user accounts in Keycloak, and Verrazzano can authenticate and authorize Keycloak users.\nAlso, you can configure Keycloak to delegate authentication to an external user store, such as Active Directory or an LDAP server.\nBecause Keycloak is not configured as an authentication provider for the Kubernetes API, authenticating Keycloak users to Kubernetes requires the use of a proxy that impersonates Keycloak users when making Kubernetes API requests. For more information about the Verrazzano authentication proxy, see Verrazzano Proxies.\nKeycloak is also used when authenticating to the Verrazzano Console and the various Verrazzano Monitoring Instance (VMI) logging and metrics consoles. The Verrazzano Console uses the OpenID Connect (OIDC) PKCE flow to authenticate users against Keycloak and obtain ID and access tokens. Authentication for VMI consoles is provided by the Verrazzano authentication proxy, which also uses PKCE to authenticate users, validates the resulting tokens, and authorizes incoming requests. For more information about the Verrazzano authentication proxy, see Verrazzano Proxies.\n","categories":"","description":"Learn about Keycloak user management and Single Sign-On (SSO)","excerpt":"Learn about Keycloak user management and Single Sign-On (SSO)","ref":"/docs/security/keycloak/keycloak/","tags":"","title":"Keycloak and SSO"},{"body":"Prepare for the Oracle Cloud Infrastructure install   Create the OKE cluster using the Oracle Cloud Infrastructure Console or by some other means.\n  For SHAPE, an OKE cluster with 3 nodes of VM.Standard2.4 Oracle Cloud Infrastructure compute instance shape has proven sufficient to install Verrazzano and deploy the Bob’s Books example application.\n  Follow the instructions provided by OKE to download the Kubernetes configuration file for your cluster, and set the following ENV variable:\n   $ export KUBECONFIG=\u003cpath to valid Kubernetes config\u003e  Optional, if your organization requires the use of a private registry to the Docker images installed by Verrazzano, see Use a Private Registry.  NOTE: Verrazzano can create network policies that can be used to limit the ports and protocols that pods use for network communication. Network policies provide additional security but they are enforced only if you install a Kubernetes Container Network Interface (CNI) plug-in that enforces them, such as Calico. For an example on OKE, see Installing Calico and Setting Up Network Policies.\nNext steps To continue, see the Installation Guide.\n","categories":"","description":"Instructions for setting up an Oracle Cloud Infrastructure Container Engine for Kubernetes (OKE) cluster for Verrazzano","excerpt":"Instructions for setting up an Oracle Cloud Infrastructure Container Engine for Kubernetes (OKE) cluster for Verrazzano","ref":"/docs/setup/platforms/oci/oci/","tags":"","title":"Oracle Cloud Infrastructure Container Engine for Kubernetes (OKE)"},{"body":"Summary Analysis detected that there were pods which were in a pending state without detecting other specific issues related to them.\nSteps Review the analysis data. At a minimum, this should indicate which pods are being impacted and it might give other clues on the root cause.\nRelated information  Kubernetes Troubleshooting  ","categories":"","description":"Analysis detected pods in a pending state","excerpt":"Analysis detected pods in a pending state","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/pendingpods/","tags":"","title":"Pending Pods"},{"body":"Summary Analysis detected that there were pods which were not in a running, succeeded, or pending state.\nThe analysis was not able to determine a specific root cause, however, it might have supplied data that is related to the pods in question. The root cause might be obvious from the supporting data, but the analysis tool isn’t isolating the specific scenario yet.\nSteps Review the analysis data. At a minimum, it should indicate which pods are being impacted and it might give other clues on the root cause.\nRelated information  Kubernetes Troubleshooting  ","categories":"","description":"Analysis detected pods with potential issues","excerpt":"Analysis detected pods with potential issues","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/podproblemsnotreported/","tags":"","title":"Problem Pods"},{"body":"Before you begin Install Verrazzano by following the installation instructions.\nNOTE: The Spring Boot example application deployment files are contained in the Verrazzano project located at \u003cVERRAZZANO_HOME\u003e/examples/springboot-app, where VERRAZZANO_HOME is the root of the Verrazzano project.\nDeploy the Spring Boot application This example provides a simple web application developed using Spring Boot. For more information and the source code of this application, see the Verrazzano Examples.\nNOTE To run this application in the default namespace:\n$kubectl label namespace default verrazzano-managed=true istio-injection=enabled If you chose the default namespace, you can skip Step 1. and ignore the -n option in the rest of the commands.\n   Create a namespace for the Spring Boot application and add a label identifying the namespace as managed by Verrazzano.\n$ kubectl create namespace springboot $ kubectl label namespace springboot verrazzano-managed=true istio-injection=enabled   To deploy the application, apply the Spring Boot OAM resources.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/springboot-app/springboot-comp.yaml -n springboot $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/springboot-app/springboot-app.yaml -n springboot   Wait for the Spring Boot application to be ready.\n$ kubectl wait \\ --for=condition=Ready pods \\ --all \\ -n springboot \\ --timeout=300s   Explore the application   Get the generated host name for the application.\n$ HOST=$(kubectl get gateways.networking.istio.io \\ -n springboot \\ -o jsonpath={.items[0].spec.servers[0].hosts[0]}) $ echo $HOST # Sample output springboot-appconf.springboot.11.22.33.44.nip.io   Get the EXTERNAL_IP address of the istio-ingressgateway service.\n$ ADDRESS=$(kubectl get service \\ -n istio-system istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo $ADDRESS # Sample output 11.22.33.44   Access the application:\n  Using the command line\n# The expected response of this query is the HTML of a web page $ curl -sk \\ https://${HOST} \\ --resolve ${HOST}:443:${ADDRESS} $ curl -sk \\ https://${HOST}/facts \\ --resolve ${HOST}:443:${ADDRESS} # Sample output In 1524, Verrazzano became the first European to enter the New York Harbor and the Hudson River. If you are using nip.io, then you do not need to include --resolve.\n  Local testing with a browser\nTemporarily, modify the /etc/hosts file (on Mac or Linux) or c:\\Windows\\System32\\Drivers\\etc\\hosts file (on Windows 10), to add an entry mapping the host name to the ingress gateway’s EXTERNAL-IP address. For example:\n11.22.33.44 springboot.example.com Then, you can access the application in a browser at https://springboot.example.com/ and https://springboot.example.com/facts.\nIf you are using nip.io, then you can access the application in a browser using the HOST variable (for example, https://${HOST}/facts). If you are going through a proxy, you may need to add *.nip.io to the NO_PROXY list.\n  Using your own DNS name\n  Point your own DNS name to the ingress gateway’s EXTERNAL-IP address.\n  In this case, you would need to have edited the springboot-app.yaml file to use the appropriate value under the hosts section (such as yourhost.your.domain), before deploying the Spring Boot application.\n  Then, you can use a browser to access the application at https://\u003cyourhost.your.domain\u003e/ and https://\u003cyourhost.your.domain\u003e/facts.\nThe actuator endpoint is accessible under the path /actuator and the Prometheus endpoint exposing metrics data in a format that can be scraped by a Prometheus server is accessible under the path /actuator/prometheus.\n      A variety of endpoints associated with the deployed application, are available to further explore the logs, metrics, and such. You can access them according to the directions here.\n  Undeploy the application   To undeploy the application, delete the Spring Boot OAM resources.\n$ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/springboot-app/springboot-app.yaml -n springboot $ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/springboot-app/springboot-comp.yaml -n springboot   Delete the namespace springboot after the application pod is terminated.\n$ kubectl delete namespace springboot   ","categories":"","description":"A Spring Boot-based simple web application","excerpt":"A Spring Boot-based simple web application","ref":"/docs/samples/spring-boot/","tags":"","title":"Spring Boot"},{"body":"To delete a Verrazzano installation, delete the Verrazzano custom resource you used to install it into your cluster.\nThe following example starts a deletion of a Verrazzano installation in the background, and then uses the kubectl logs -f command to tail the Console output of the pod performing the uninstall:\n# Get the name of the Verrazzano custom resource $ MYVZ=$(kubectl get vz -o jsonpath=\"{.items[0].metadata.name}\") # Delete the Verrazzano custom resource $ kubectl delete verrazzano $MYVZ --wait=false $ kubectl logs -n verrazzano-install \\ -f $(kubectl get pod \\ -n verrazzano-install \\ -l job-name=verrazzano-uninstall-${MYVZ} \\ -o jsonpath=\"{.items[0].metadata.name}\") ","categories":"","description":"How to uninstall Verrazzano","excerpt":"How to uninstall Verrazzano","ref":"/docs/setup/uninstall/uninstall/","tags":"","title":"Uninstall"},{"body":"Verrazzano uses the OAM specification to provide a layered approach to describing and deploying applications. The Open Application Model (OAM) is a specification developed within the Cloud Native Computing Foundation (CNCF). Verrazzano is compliant with the OAM specification version 0.2.1.\nAn ApplicationConfiguration is a composition of Components. Components encapsulate application implementation details. Application deployers apply Traits and Scopes to customize the Components for the environment.\nThe OAM specification supports extensibility. The behavior of the platform can be extended by adding OAM compliant definitions and controllers. Specifically, new workload, Trait, and Scope definitions can be added. These definitions can be referenced by Components and application configurations and are processed by custom controllers.\nApplication configurations An ApplicationConfiguration is a collection of references to Components. A set of Traits and Scopes can be applied to each Component reference. The platform uses these Components, Traits, and Scopes to generate the final application resources during deployment.\nThe following sample shows the high level structure of an ApplicationConfiguration.\napiVersion:core.oam.dev/v1alpha2kind:ApplicationConfiguration...spec:components:- componentName:example-component-1traits:...scopes:...- componentName:example-component-2...Components A Component wraps the content of a workload. The platform extracts the workload during deployment and creates new resources that result from the application of Traits and Scopes. Verrazzano and the OAM specification provide several workloads, for example VerrazzanoHelidonWorkload and ContainerizedWorkload. The workloads also can be any Kubernetes resource. For some Kubernetes resources, the oam-kubernetes-runtime operator may need to be granted additional permission.\nA Component can also be parameterized; this allows the workload content to be customized when referenced within an ApplicationConfiguration. See the OAM specification for details.\nThe following sample shows the high level structure of a Component.\napiVersion:core.oam.dev/v1alpha2kind:Component...spec:workload:...parameters:...Workloads Components contain an embedded workload. Verrazzano and the OAM specification provide several workloads, for example VerrazzanoWebLogicWorkload and ContainerizedWorkload. Workloads can also be any Kubernetes resource.\nThe following sample shows a VerrazzanoHelidonWorkload workload embedded within a Component.\napiVersion:core.oam.dev/v1alpha2kind:Component...spec:workload:apiVersion:oam.verrazzano.io/v1alpha1kind:VerrazzanoHelidonWorkloadspec:deploymentTemplate:podSpec:containers:- name:example-containerimage:......A workload can optionally have an associated WorkloadDefinition. This provides the platform with information about the schema of the workload. A WorkloadDefintion is typically provided by the platform, not an end user.\nTraits Traits customize Component workloads and generate related resources during deployment. Verrazzano provides several Traits, for example IngressTrait and MetricsTrait. The platform extracts Traits contained within an ApplicationConfiguration during deployment. This processing is similar to the extraction of workload content from Component resources. Note that for some Kubernetes resources the oam-kubernetes-runtime operator may need to be granted create permission.\nA Kubernetes operator, for example verrazzano-application-operator, processes these extracted Traits and may create additional related resources or may alter related workloads. Each Trait implementation will behave differently.\nThe following sample shows an IngressTrait applied to a referenced Component.\napiVersion:core.oam.dev/v1alpha2kind:ApplicationConfiguration...spec:components:- componentName:example-componenttraits:- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:IngressTraitspec:rules:- paths:- path:\"/greet\"Each Trait type optionally can have an associated TraitDefinition. This provides the platform with additional information about the Trait’s schema and workloads to which the Trait can be applied. A TraitDefintion is typically provided by the platform, not an end user.\nScopes Scopes customize Component workloads and generate related resources during deployment. An ApplicationConfiguration contains Scope references instead of the Scope’s content being embedded. The platform will update the Scopes with a reference to each applied Component. This update triggers the related operator to process the Scope.\nThe following sample shows a reference to a HealthScope named example-health-scope.\napiVersion:core.oam.dev/v1alpha2kind:ApplicationConfiguration...spec:components:- componentName:example-componentscopes:- scopeRef:apiVersion:core.oam.dev/v1alpha2kind:HealthScopename:example-health-scope...The following sample shows the configuration details of the referenced HealthScope.\napiVersion:core.oam.dev/v1alpha2kind:HealthScopemetadata:name:example-health-scopespec:probe-method:GETprobe-endpoint:/healthEach Scope type can optionally have an associated ScopeDefinition. This provides the platform with additional information about processing the Scope.\n The Scope’s schema. The workload types to which the Scope can be applied. The field within the Scope used to record related Component references.  A ScopeDefintion is typically provided by the platform, not an end user.\nVerrazzano workloads The Verrazzano platform provides several workload definitions and implementations:\n The VerrazzanoWebLogicWorkload is used for WebLogic workloads. See WebLogic Workload. The VerrazzanoCoherenceWorkload is used for Coherence workloads. See Coherence Workload. The VerrazzanoHelidonWorkload is used for Helidon workloads. See Helidon Workload.  OAM ContainerizedWorkload The ContainerizedWorkload should be used for long-running container workloads which are not covered by the workload types described previously. This workload type is similar to the Deployment workload. It is provided to ensure that OAM can be used for non-Kubernetes deployment environments. See the OAM specification.\nVerrazzano Traits The Verrazzano platform provides several Trait definitions and implementations:\n IngressTrait MetricsTrait  IngressTrait The IngressTrait provides a simplified integration with the Istio ingress gateway included in the Verrazzano platform. The verrazzano-application-operator processes each IngressTrait and generates related Gateway, VirtualService, and Certificate resources when processed. The Certificate is created in the istio-system namespace. The values used to create are either explicitly provided in the Trait or are derived from the environment or associated Component.\nThe following sample shows an IngressTrait that results in the application being accessible using the path /greet.\napiVersion:core.oam.dev/v1alpha2kind:ApplicationConfiguration...spec:components:- componentName:example-componenttraits:- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:IngressTraitspec:rules:- paths:- path:\"/greet\"See the API documentation for details.\nMetricsTrait The MetricsTrait provides a simplified integration with the Prometheus service included in the Verrazzano platform. The verrazzano-application-operator processes each MetricsTrait and does two things:\n Updates the workload’s annotations to provide metrics source information. Updates the Prometheus’ metrics scrape configuration with metrics scrape targets. The Verrazzano platform will automatically apply a MetricsTrait to every Component with a supported workload.  The following sample shows the a MetricsTrait that was automatically applied.\napiVersion:core.oam.dev/v1alpha2kind:ApplicationConfiguration...spec:components:- componentName:example-componenttraits:- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:MetricsTraitSee the API documentation for details.\nKubernetes resources Verrazzano and OAM provide workloads and Traits to define and customize applications. However, some situations may require resources beyond those provided. In this case, other existing Kubernetes resources can also be used. The todo-list example takes advantage of this capability in several Components to support unique Service and ConfigMap requirements.\nMost Kubernetes resources can be embedded as a workload within a Component. The following sample shows how a Deployment can be embedded as a workload within a Component. The oam-kubernetes-runtime operator will process the Component and extract the Deployment to a separate resource during deployment.\napiVersion:core.oam.dev/v1alpha2kind:Component...spec:workload:kind:DeploymentapiVersion:apps/v1name:...spec:selector:...template:...Most Kubernetes resources can also be embedded as a Trait within an ApplicationConfiguration. The following sample shows how an Ingress can be embedded as a Trait within an ApplicationConfiguration. The oam-kubernetes-runtime operator will process the ApplicationConfiguration and extract the Ingress to a separate resource during deployment. In the following sample, note that the Ingress is the Kubernetes Ingress, not the IngressTrait provided by Verrazzano.\napiVersion:core.oam.dev/v1alpha2kind:ApplicationConfiguration...spec:components:- componentName:...traits:- trait:apiVersion:networking.k8s.io/v1beta1kind:Ingress...spec:rules:...The oam-kubernetes-runtime operator has a limited set of privileges by default. Your cluster administrator may need to grant the oam-kubernetes-runtime operator additional privileges to enable the use of some Kubernetes resources as workloads or Traits. Create additional roles and role bindings for the specific resources to be embedded as workloads or Traits. The following examples of ClusterRole and ClusterRoleBinding show how oam-kubernetes-runtime can be granted privileges to manage Ingress resources.\napiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:oam-kubernetes-runtime-ingressesrules:- apiGroups:- networking.k8s.io- extensionsresources:- ingressesverbs:- create- delete- get- list- patch- updateapiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:oam-kubernetes-runtime-ingressesroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:oam-kubernetes-runtime-ingressessubjects:- kind:ServiceAccountname:oam-kubernetes-runtimenamespace:verrazzano-systemDeployment An application deployment occurs in Verrazzano through a number of Kubernetes controllers, reading and writing various resources. Each controller processes application resources, and generates or updates other related resources. Different types of controllers process different levels of application resources.\nThe ApplicationConfiguration controller processes ApplicationConfiguration and Component resources. This controller extracts and stores workload for each Component referenced within ApplicationConfiguration resources. Verrazzano implements the ApplicationConfiguration controller within the oam-kubernetes-runtime operator. Similarly, the ApplicationConfiguration controller extracts and stores Trait resources associated with Component resources in the ApplicationConfiguration.\nThe workload controllers process workload resources created by the ApplicationConfiguration controller, for example ContainerizedWorkload or VerrazzanoWebLogicWorkload. This controller processes these workload resources and generates more specific runtime resources. For example, the ContainerizedWorkload controller processes a ContainerizedWorkload resource and generates a Deployment resource. The VerrazzanoWebLogicWorkload controller processes a VerrazzanoWebLogicWorkload resource and generates a Domain resource. These controllers may take into account Traits and Scopes that are applied to the workload’s Component references within the ApplicationConfiguration. Verrazzano implements these workload controllers in two operators. Verrazzano specific workloads, for example VerrazzanoHelidonWorkload, are processed by a controller within the verrazzano-application-operator. Workloads defined by OAM, for example ContainerizedWorkload, are processed by a controller with the oam-kubernetes-runtime operator.\nThe Trait controllers process Trait resources created by the ApplicationConfiguration controller, for example MetricsTrait. The ApplicationConfiguration controller records the Component to which it was applied within each extracted Trait. The Trait controllers process extracted Trait resources, and generate or update other related resources. For example, the IngressTrait controller within the verrazzano-application-operator processes IngressTrait resources and generates related Gateway and VirtualService resources. The same operator contains a MetricsTrait controller which processes MetricsTrait resources and adds annotations to related resources such as Deployments.\nScope controllers read Scope resources updated by the ApplicationConfiguration controller during deployment. The ApplicationConfiguration controller updates the Scope resources with references to each Component to which the Scope is applied.\nThe following diagram shows the relationships between the resources and controllers described previously. The following diagram, based on the hello-helidon example, shows the processing of resources from a Kubernetes operator perspective. Controllers within the oam-kubernetes-runtime process the ApplicationConfiguration and Component resources and generate VerrazzanoHelidonWorkload and IngressTrait. Then controllers within the verrazzano-application-operator process the VerrazzanoHelidonWorkload and IngressTrait resources to generate Deployment, VirtualService, and other resources.\n","categories":"","description":"Develop applications with Verrazzano","excerpt":"Develop applications with Verrazzano","ref":"/docs/applications/","tags":"","title":"Applications"},{"body":"You can customize Verrazzano Istio component using settings in the Verrazzano custom resource.\nThe following table describes the fields in the Verrazzano custom resource pertaining to the Istio component.\n   Path to Field Description     spec.components.istio.egress.kubernetes.replicas The number of pods to replicate. The default is 2 for the prod profile and 1 for all other profiles.   spec.components.istio.egress.kubernetes.affinity The pod affinity definition expressed as a standard Kubernetes affinity definition. The default configuration spreads the Istio gateway pods across the available nodes. spec:\ncomponents:\nistio:\negress:\nkubernetes:\naffinity:\npodAntiAffinity:\npreferredDuringSchedulingIgnoredDuringExecution:\n- weight: 100\npodAffinityTerm:\nlabelSelector:\nmatchExpressions:\n- key: app\noperator: In\nvalues:\n- istio-egressgateway\ntopologyKey: kubernetes.io/hostname   spec.components.istio.ingress.kubernetes.replicas The number of pods to replicate. The default is 2 for the prod profile and 1 for all other profiles.   spec.components.istio.ingress.kubernetes.affinity The pod affinity definition expressed as a standard Kubernetes affinity definition. The default configuration spreads the Istio gateway pods across the available nodes. spec:\ncomponents:\nistio:\ningress:\nkubernetes:\naffinity:\npodAntiAffinity:\npreferredDuringSchedulingIgnoredDuringExecution:\n- weight: 100\npodAffinityTerm:\nlabelSelector:\nmatchExpressions:\n- key: app\noperator: In\nvalues:\n- istio-ingressgateway\ntopologyKey: kubernetes.io/hostname    The following example customizes a Verrazzano prod profile as follows:\n Increases the replicas count to 3 for istio-ingressgateway and istio-egressgateway Changes the podAffinity configuration to use requiredDuringSchedulingIgnoredDuringExecution for istio-ingressgateway and istio-egressgateway  apiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: example-verrazzano spec: profile: prod components: istio: ingress: kubernetes: replicas: 3 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - weight: 25 labelSelector: matchExpressions: - key: app operator: In values: - istio-ingressgateway topologyKey: kubernetes.io/hostname egress: kubernetes: replicas: 3 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - istio-egressgateway topologyKey: kubernetes.io/hostname ","categories":"","description":"Customize Istio Gateways","excerpt":"Customize Istio Gateways","ref":"/docs/setup/customizing/istio/","tags":"","title":"Customize Istio"},{"body":"This example demonstrates using standard Kubernetes resources, in conjunction with OAM resources, to define and deploy an application. Several standard Kubernetes resources are used in this example, both as workloads and traits.\n Deployment is used as a workload within a Component. Service is used as a workload within a Component. Ingress is used as a trait within an ApplicationConfiguration.  Before you begin Install Verrazzano by following the installation instructions.\nGrant permissions The oam-kubernetes-runtime is not installed with privileges that allow it to create the Kubernetes Ingress resource used in this example. The following steps create a role that allows Ingress resource creation and binds that role to the oam-kubernetes-runtime service account. For this example to work, your cluster admin will need to run the following steps to create the ClusterRole and ClusterRoleBinding.\n$ kubectl apply -f - \u003c\u003cEOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: oam-kubernetes-runtime-ingresses rules: - apiGroups: - networking.k8s.io - extensions resources: - ingresses verbs: - create - delete - get - list - patch - update --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: oam-kubernetes-runtime-ingresses roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: oam-kubernetes-runtime-ingresses subjects: - kind: ServiceAccount name: oam-kubernetes-runtime namespace: verrazzano-system EOF Deploy the application This example provides an web application using a common example application image. When accessed, the application returns the configured text.\n  Create the application namespace and add a label identifying the namespace as managed by Verrazzano.\n$ kubectl create namespace oam-kube $ kubectl label namespace oam-kube verrazzano-managed=true istio-injection=enabled   Create a Component containing a Deployment workload.\n$ kubectl apply -f - \u003c\u003cEOF apiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: oam-kube-dep-comp namespace: oam-kube spec: workload: kind: Deployment apiVersion: apps/v1 name: oam-kube-dep spec: replicas: 1 selector: matchLabels: app: oam-kube-app template: metadata: labels: app: oam-kube-app spec: containers: - name: oam-kube-cnt image: hashicorp/http-echo args: - \"-text=hello\" EOF   Create a Component containing a Service workload.\n$ kubectl apply -f - \u003c\u003cEOF apiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: oam-kube-svc-comp namespace: oam-kube spec: workload: kind: Service apiVersion: v1 metadata: name: oam-kube-svc spec: selector: app: oam-kube-app ports: - port: 5678 # Default port for image EOF   Create an ApplicationConfiguration referencing both Components and configuring an ingress trait.\n$ kubectl apply -f - \u003c\u003cEOF apiVersion: core.oam.dev/v1alpha2 kind: ApplicationConfiguration metadata: name: oam-kube-appconf namespace: oam-kube spec: components: - componentName: oam-kube-dep-comp - componentName: oam-kube-svc-comp traits: - trait: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: oam-kube-ing annotations: kubernetes.io/ingress.class: istio spec: rules: - host: oam-kube-app.example.com http: paths: - path: /example backend: serviceName: oam-kube-svc servicePort: 5678 EOF   Explore the application  Get the host name for the application. $ export HOST=$(kubectl get ingress \\ -n oam-kube oam-kube-ing \\ -o jsonpath='{.spec.rules[0].host}') $ echo \"HOST=${HOST}\"  Get the load balancer address of the ingress gateway. $ export LOADBALANCER=$(kubectl get ingress \\ -n oam-kube oam-kube-ing \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo \"LOADBALANCER=${LOADBALANCER}\"  Access the application. $ curl http://${HOST}/example --resolve ${HOST}:80:${LOADBALANCER} # Expected response hello   Undeploy the application To undeploy the application, delete the namespace created. This will result in the deletion of all explicitly and implicitly created resources in the namespace.\n$ kubectl delete namespace oam-kube If desired, the cluster admin also can remove the created ClusterRole and ClusterRoleBinding.\n$ kubectl delete ClusterRoleBinding oam-kubernetes-runtime-ingresses $ kubectl delete ClusterRole oam-kubernetes-runtime-ingresses ","categories":"","description":"Example of using standard Kubernetes resources","excerpt":"Example of using standard Kubernetes resources","ref":"/docs/samples/standard-kubernetes/","tags":"","title":"Standard Kubernetes Resources"},{"body":"Before you begin  Install Verrazzano by following the installation instructions. To download the example image, you must first accept the license agreement.  In a browser, navigate to https://container-registry.oracle.com/ and sign in. Search for example-todo and weblogic. For each one:  Select the image name in the results. From the drop-down menu, select your language and click Continue. Then read and accept the license agreement.      NOTE: The ToDo List example application deployment files are contained in the Verrazzano project located at \u003cVERRAZZANO_HOME\u003e/examples/todo-list, where \u003cVERRAZZANO_HOME\u003e is the root of the Verrazzano project.\nAll files and paths in this document are relative to \u003cVERRAZZANO_HOME\u003e/examples/todo-list.\nDeploy the ToDo List application ToDo List is an example application containing a WebLogic component. For more information and the source code of this application, see the Verrazzano Examples.\nNOTE To run this application in the default namespace:\n$kubectl label namespace default verrazzano-managed=true istio-injection=enabled If you chose the default namespace, you can skip Step 1. and ignore the -n option in the rest of the commands.\n   Create a namespace for the ToDo List example and add a label identifying the namespace as managed by Verrazzano.\n$ kubectl create namespace todo-list $ kubectl label namespace todo-list verrazzano-managed=true istio-injection=enabled   Create a docker-registry secret to enable pulling the ToDo List example image from the registry.\n$ kubectl create secret docker-registry tododomain-repo-credentials \\ --docker-server=container-registry.oracle.com \\ --docker-username=YOUR_REGISTRY_USERNAME \\ --docker-password=YOUR_REGISTRY_PASSWORD \\ --docker-email=YOUR_REGISTRY_EMAIL \\ -n todo-list Replace YOUR_REGISTRY_USERNAME, YOUR_REGISTRY_PASSWORD, and YOUR_REGISTRY_EMAIL with the values you use to access the registry.\n  Create and label secrets for the WebLogic domain:\n# Replace the values of the WLS_USERNAME and WLS_PASSWORD environment variables as appropriate. $ export WLS_USERNAME=\u003cusername\u003e $ export WLS_PASSWORD=\u003cpassword\u003e $ kubectl create secret generic tododomain-weblogic-credentials \\ --from-literal=password=$WLS_PASSWORD \\ --from-literal=username=$WLS_USERNAME \\ -n todo-list $ kubectl create secret generic tododomain-jdbc-tododb \\ --from-literal=username=$WLS_USERNAME \\ --from-literal=password=$WLS_PASSWORD \\ -n todo-list $ kubectl -n todo-list label secret tododomain-jdbc-tododb weblogic.domainUID=tododomain Note that the ToDo List example application is preconfigured to use specific secret names. For the source code of this application, see the Verrazzano Examples.\n  To deploy the application, apply the example resources.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/todo-list/todo-list-components.yaml -n todo-list $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/todo-list/todo-list-application.yaml -n todo-list   Wait for the ToDo List application to be ready. You can monitor its progress by listing pods and inspecting the output, or you can use the kubectl wait command. You may need to repeat the kubectl wait command several times before it is successful. The tododomain-adminserver pod may take a while to be created and Ready.\n$ kubectl get pods -n todo-list # -or- # $ kubectl wait pod \\ --for=condition=Ready tododomain-adminserver \\ -n todo-list   Get the generated host name for the application.\n$ HOST=$(kubectl get gateways.networking.istio.io \\ -n todo-list \\ -o jsonpath='{.items[0].spec.servers[0].hosts[0]}') $ echo $HOST # Sample output todo-appconf.todo-list.10.11.12.13.nip.io   Get the EXTERNAL_IP address of the istio-ingressgateway service.\n$ ADDRESS=$(kubectl get service \\ -n istio-system istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo $ADDRESS # Sample output 10.11.12.13   Access the ToDo List application:\n  Using the command line\n# The expected response of this query is the HTML of a web page $ curl -sk \\ https://${HOST}/todo/ \\ --resolve ${HOST}:443:${ADDRESS} If you are using nip.io, then you do not need to include --resolve.\n  Local testing with a browser\nTemporarily, modify the /etc/hosts file (on Mac or Linux) or c:\\Windows\\System32\\Drivers\\etc\\hosts file (on Windows 10), to add an entry mapping the host name to the ingress gateway’s EXTERNAL-IP address. For example:\n10.11.12.13 todo.example.com Then, you can access the application in a browser at https://todo.example.com/todo.\nIf you are using nip.io, then you can access the application in a browser using the HOST variable (for example, https://${HOST}/todo). If you are going through a proxy, then you may need to add *.nip.io to the NO_PROXY list.\n  Using your own DNS name\n  Point your own DNS name to the ingress gateway’s EXTERNAL-IP address.\n  In this case, you would need to have edited the todo-list-application.yaml file to use the appropriate value under the hosts section (such as yourhost.your.domain), before deploying the ToDo List application.\n  Then, you can use a browser to access the application at https://\u003cyourhost.your.domain\u003e/todo/.\nAccessing the application in a browser opens the page, “Derek’s ToDo List”, with an edit field and an Add button that lets you add tasks.\n      A variety of endpoints associated with the deployed ToDo List application, are available to further explore the logs, metrics, and such. You can access them according to the directions here.\n  Access the WebLogic Server Administration Console To access the Console from the machine where you are running kubectl:\n  Set up port forwarding.\n$ kubectl port-forward pods/tododomain-adminserver 7001:7001 -n todo-list NOTE: If you are using the Oracle Cloud Infrastructure Cloud Shell to run kubectl, in order to access the Console using port forwarding, you will need to run kubectl on another machine.\n  Access the WebLogic Server Administration Console from your browser.\nhttp://localhost:7001/console   NOTE It is recommended that the WebLogic Server Administration Console not be exposed publicly.  Troubleshooting   Verify that the application configuration, domain, and ingress trait all exist.\n$ kubectl get ApplicationConfiguration -n todo-list # Sample output NAME AGE todo-appconf 19h $ kubectl get Domain -n todo-list # Sample output NAME AGE todo-domain 19h $ kubectl get IngressTrait -n todo-list # Sample output NAME AGE todo-domain-trait-7cbd798c96 19h   Verify that the WebLogic Administration Server and MySQL pods have been created and are running. Note that this will take several minutes.\n$ kubectl get pods -n todo-list # Sample output NAME READY STATUS RESTARTS AGE mysql-5c75c8b7f-vlhck 2/2 Running 0 19h tododomain-adminserver 4/4 Running 0 19h   Undeploy the application   To undeploy the application, delete the ToDo List OAM resources.\n$ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/todo-list/todo-list-application.yaml -n todo-list $ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/todo-list/todo-list-components.yaml -n todo-list   Delete the namespace todo-list after the application pods are terminated. The secrets created for the WebLogic domain also will be deleted.\n$ kubectl delete namespace todo-list   ","categories":"","description":"An example application containing a WebLogic component","excerpt":"An example application containing a WebLogic component","ref":"/docs/samples/todo-list/","tags":"","title":"ToDo List"},{"body":"A Verrazzano installation consists of a stack of components, such as cert-manager, where each component has a specific release version that may be different from the overall Verrazzano version. The Verrazzano platform operator knows the versions of each component associated with the Verrazzano version. When you perform the initial Verrazzano installation, the appropriate version of each component is installed by the platform operator. Post installation, it may be necessary to update one or more of the component images or Helm charts. This update is also handled by the platform operator and is called an upgrade. Currently, Verrazzano does only patch-level upgrades, where a helm upgrade command can be issued for the component. Typically, patch-level upgrades simply replace component images with newer versions.\nApplication and system pod restarts Upgrading Verrazzano 1.0.x to 1.1.0 will result in an upgrade of Istio from 1.7.3 to 1.10.4. Because of this, all the pods in the Istio mesh need to be restarted so that the new Envoy proxy sidecar can be injected into the pods. This includes both Verrazzano applications, along with Verrazzano system pods, such as the NGINX Ingress Controller. For WebLogic workloads, Verrazzano will shut down every domain, do the upgrade, then start every domain. For all other workloads, Verrazzano will perform a rolling restart when the upgrade is complete. There is no user involvement related to restarting applications; it is done automatically during upgrade.\nUpgrade steps It is important to distinguish between updating the Verrazzano platform operator versus upgrading the Verrazzano installation. The platform operator contains the newer component charts and image versions, so it must be updated prior to upgrading the installation. Updating the platform operator has no effect on an existing installation until you initiate the Verrazzano installation upgrade. Currently, there is no way to roll back either the platform operator update or the Verrazzano installation upgrade.\nUpgrading an existing Verrazzano installation is a two-step process:\n Update the Verrazzano platform operator to the Verrazzano release version to which you want to upgrade. Upgrade the Verrazzano installation.  Update the Verrazzano platform operator In order to upgrade an existing Verrazzano installation, you must first update the Verrazzano platform operator.\n  Update the Verrazzano platform operator.\nNOTE: If you are using a private container registry, then to update the platform operator, follow the instructions at Use a Private Registry.\nTo update to the latest version:\n$ kubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/v1.2.2/operator.yaml To update to a specific version, where \u003cversion\u003e is the desired version:\n$ kubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/\u003cversion\u003e/operator.yaml   Wait for the deployment to complete.\n$ kubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator # Expected response deployment \"verrazzano-platform-operator\" successfully rolled out   Confirm that the operator pod is correctly defined and running.\n$ kubectl -n verrazzano-install get pods # Sample output NAME READY STATUS RESTARTS AGE verrazzano-platform-operator-59d5c585fd-lwhsx 1/1 Running 0 114s   Upgrade Verrazzano To upgrade the Verrazzano installation, you need to change the version of your installed Verrazzano resource to the version supported by the Verrazzano Platform Operator.\nNOTE: You may only change the version field during an upgrade; changes to other fields or component configurations are not supported at this time.\nIn one simple step, you can upgrade to a specified version of Verrazzano using this command:\n$ kubectl patch vz example-verrazzano -p '{\"spec\":{\"version\":\"v1.2.2\"}}' --type=merge Alternatively, you can upgrade the Verrazzano installation using the following steps:\n  Update the Verrazzano resource to the desired version.\nTo upgrade the Verrazzano components, you must update the version field in your Verrazzano resource spec to match the version supported by the platform operator to which you upgraded and apply it to the cluster.\nThe value of the version field in the resource spec must be a Semantic Versioning value corresponding to a valid Verrazzano release version.\nYou can update the resource by doing one of the following:\na. Editing the YAML file you used to install Verrazzano and setting the version field to the latest version.\nFor example, to upgrade to v1.2.2, your YAML file should be edited to add or update the version field:\napiVersion:install.verrazzano.io/v1alpha1kind:Verrazzanometadata:name:example-verrazzanospec:profile:devversion:v1.2.2Then apply the resource to the cluster (if you have not edited the resource in-place using kubectl edit):\n$ kubectl apply -f example-verrazzano.yaml b. Editing the Verrazzano resource directly using kubectl and setting the version field directly, for example:\n$ kubectl edit verrazzano example-verrazzano # In the resource editor, add or update the version field to \"version: v1.2.2\", then save.   Wait for the upgrade to complete:\n$ kubectl wait \\ --timeout=10m \\ --for=condition=UpgradeComplete verrazzano/example-verrazzano   If an error occurs, check the log output. You can view the logs with the following command:\n$ kubectl logs -n verrazzano-install \\ -f $(kubectl get pod \\ -n verrazzano-install \\ -l app=verrazzano-platform-operator \\ -o jsonpath=\"{.items[0].metadata.name}\") | grep '^{.*}$' \\ | jq -r '.\"@timestamp\" as $timestamp | \"\\($timestamp) \\(.level) \\(.message)\"' If an upgrade fails, you’ll see this:\n$ kubectl get vz # Sample output NAME STATUS VERSION example-verrazzano UpgradeFailed v1.1.1 You can restart the upgrade by setting the annotation verrazzano.io/upgrade-retry-version to any unique value. For example:\n$ kubectl patch vz example-verrazzano -p '{\"metadata\":{\"annotations\": {\"verrazzano.io/upgrade-retry-version\":\"v1.1.2-1\"} }}' --type=merge Verify the upgrade Check that all the pods in the verrazzano-system namespace are in the Running state. While the upgrade is in progress, you may see some pods terminating and restarting as newer versions of components are applied, for example:\n$ kubectl get pods -n verrazzano-system # Sample output coherence-operator-866798c99d-r69xt 1/1 Running 1 43m fluentd-f9fbv 2/2 Running 0 38m fluentd-n79c4 2/2 Running 0 38m fluentd-xslzw 2/2 Running 0 38m oam-kubernetes-runtime-56cdb56c98-wn2mb 1/1 Running 0 43m verrazzano-application-operator-7c95ddd5b5-7xzmn 1/1 Running 0 42m verrazzano-authproxy-594d8c8dcd-llmlr 2/2 Running 0 38m verrazzano-console-74dbf97fdf-zxvvn 2/2 Running 0 38m verrazzano-monitoring-operator-6fcf8484fd-gfkhs 1/1 Running 0 38m verrazzano-operator-66c8566f95-8lbs6 1/1 Running 0 38m vmi-system-es-master-0 2/2 Running 0 38m vmi-system-grafana-799d79648d-wsdp4 2/2 Running 0 38m vmi-system-kiali-574c6dd94d-f49jv 2/2 Running 0 41m vmi-system-kibana-77f8d998f4-zzvqr 2/2 Running 0 38m vmi-system-prometheus-0-7f89d54fbf-brg6x 3/3 Running 0 36m weblogic-operator-7b447fdb47-wlw64 2/2 Running 0 42m Check that the pods in your application namespaces are ready, for example:\n$ kubectl get pods -n todo-list # Sample output NAME READY STATUS RESTARTS AGE mysql-67575d8954-d4vkm 2/2 Running 0 39h tododomain-adminserver 4/4 Running 0 39h ","categories":"","description":"How to upgrade Verrazzano","excerpt":"How to upgrade Verrazzano","ref":"/docs/setup/upgrade/","tags":"","title":"Upgrade"},{"body":"WebLogic Server is a widely-used enterprise application server for managing Java Enterprise Edition-based applications and is certified to run on Kubernetes using the WebLogic Kubernetes Operator. The WebLogic Kubernetes Operator manages the WebLogic domain life cycle in Verrazzano. The WebLogic Domain custom resource (CR) specifies the configuration of the WebLogic domain. The operator monitors the WebLogic Domain CR and reconciles the domain by creating, updating, and deleting Kubernetes resources (Pods, Services, and such), as needed. Each pod is a WebLogic Server, an Administration Server or Managed Server.\nThe WebLogic Kubernetes Operator is installed in the verrazzano-system namespace and is also part of the istio-mesh deployed by Verrazzano.\nNOTE Verrazzano installs an instance of the WebLogic Kubernetes Operator. If you have a pre-existing instance of the operator, namespaces managed by each instance must be mutually exclusive. Do not label a namespace which is managed by the pre-existing WebLogic Kubernetes Operator, to also be managed by Verrazzano.  WebLogic OAM Component In Verrazzano, WebLogic workloads are specified as a VerrazzanoWebLogicWorkload OAM Component and one component specifies exactly one WebLogic domain. An ApplicationConfiguration can contain multiple VerrazzanoWebLogicWorkload components and therefore, multiple WebLogic domains. You can specify Traits for one or more VerrazzanoWebLogicWorkload components. All WebLogic Domain CR fields can be specified in the VerrazzanoWebLogicWorkload.\nThe following is an example WebLogic OAM Component.\napiVersion:core.oam.dev/v1alpha2\u000bkind:Component\u000bmetadata:\u000b…\u000bspec:\u000bworkload:\u000bapiVersion:oam.verrazzano.io/v1alpha1kind:VerrazzanoWebLogicWorkloadspec:\u000btemplate:metadata:name:todo-domainspec:\u000bdomainUID:tododomainomainHome:/u01/domains/tododomain\u000b…Verrazzano application operator The Verrazzano application operator monitors the VerrazzanoWebLogicWorkload custom resource (CR) and creates, updates, and deletes the Domain CR based on the specification provided in the VerrazzanoWebLogicWorkload CR. Also, it modifies the WebLogic Domain CR to add Fluentd sidecar injection for logging and a Monitoring Exporter configuration for metrics, if they do not already exist. The WebLogic Kubernetes Operator creates the WebLogic domain based on the WebLogic Domain CR.\nWebLogic domain life cycle The following are the life cycle stages of a WebLogic domain:\n Create a WebLogic domain.  Application containing WebLogic component is created. WebLogic component added to an existing application.   Delete a WebLogic domain.  Application containing WebLogic component is deleted. WebLogic component removed from an existing application.   Scale a WebLogic domain.  Modify the replicas field in the WebLogic Domain CR within the OAM Component spec.   Update a WebLogic domain.  Modify the other fields field in the WebLogic Domain CR within the OAM Component spec.    Scale-in and scale-out a WebLogic domain by modifying the OAM Component replicas count. Delete the WebLogic domain by deleting the OAM application or removing the component from the application.\nIstio integration Verrazzano creates all WebLogic domain pods in an Istio mesh; all WebLogic network traffic uses mTLS.\nThe Envoy proxy sidecar exists in front of workloads for each service providing security, load balancing, metrics, and such. Traffic in and out of the pod goes through the proxy.\nIf the namespace is labeled istio-injection=enabled, then Istio puts the WebLogic domain in the Istio mesh. You should label all the namespaces istio-injection=enabled where the WebLogic domain is to be created, or WebLogic domain creation will fail. Also, you can label the namespaces when using VerrazzanoProject, which by default, assigns the label to all the namespaces associated with the project. In the WebLogic Domain CR, the Verrazzano application operator sets the Istio enabled field.\napiVersion:v1items:- apiVersion:weblogic.oracle/v8kind:Domain…spec:…istio:enabled:trueIstio mesh ingress and egress The Verrazzano installer creates the ingress gateway service. The Ingress gateway is a LoadBalancer service; TLS is terminated at the Istio ingress gateway. Ingress to the WebLogic domain is optional; you can use an IngressTrait to enable it.\nIstio ingress and routing for a single WebLogic domain The Istio Gateway resource describes a proxy providing ingress to the Kubernetes cluster and the Istio mesh. The Gateway specifies the host, port, protocol, and so on, and is bound to a gateway service (LoadBalancer/NodePort). VirtualService specifies routes to services and load balancing.\nExample of an Istio Gateway resource:\napiVersion:networking.istio.io/v1beta1kind:Gateway…spec:selector:istio:ingressgatewayservers:- hosts:- todo-appconf.todo-list.172.18.0.230.nip.io#Host for this gateway serverport:name:httpsnumber:443protocol:HTTPStls:credentialName:todo-list-todo-appconf-cert-secret#Secret containing TLS certificatemode:SIMPLE#Terminate TLSExample of an Istio VirtualService resource:\napiVersion:networking.istio.io/v1beta1kind:VirtualService…spec:gateways:- todo-list-todo-appconf-gw#Gateway resource referencehosts:- todo-appconf.todo-list.172.18.0.230.nip.io#Host that this VS applies to. Gateway resource can have multiple hostshttp:- match:- uri:prefix:/todoroute:- destination:host:tododomain-adminserver#Back-end Kubernetes Serviceport:number:7001Istio ingress and routing for multiple WebLogic domains Multiple Gateway resources use the same Istio ingress gateway service. Verrazzano creates a single Gateway and VirtualService per IngressTrait specified on the OAM Component.\nIstio authorization policy The Istio AuthorizationPolicy resource specifies access controls for WebLogic pods, other pods in the application, the Ingress gateway, and Prometheus.\nExample Istio authorization policy\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicy...spec:rules:- from:- source:principals:- cluster.local/ns/todo-list/sa/todo-appconf- cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account- cluster.local/ns/verrazzano-system/sa/verrazzano-monitoring-operator- cluster.local/ns/verrazzano-system/sa/weblogic-operator-saselector:matchLabels:verrazzano.io/istio:todo-appconfWebLogic metrics Prometheus scrapes each WebLogic pod on the metrics port periodically.\nIf the trait doesn’t exist, Verrazzano will inject the MetricsTrait into AppConfig. The WebLogic Monitoring Exporter sidecar provides the metrics endpoint. If it doesn’t already exist, Verrazzano will inject the default Monitoring Exporter configuration into the WebLogic Domain CR. The Verrazzano application operator updates the Prometheus Configmap with WebLogic targets and Verrazzano installs Grafana dashboards to view WebLogic metrics. The WebLogic Kubernetes Operator configures the Monitoring Exporter using a REST API and labels the pods with metrics-related labels. Metrics are scraped at /metrics on port 8080.\nAppConfig default injection Review the following example MetricsTrait from the TodoList ApplicationConfiguration. If missing from ApplicationConfiguration, Verrazzano will inject the default MetricsTrait.\nkind:ApplicationConfigurationmetadata:name:todo-appconf...spec:components:- componentName:todo-domaintraits:- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:MetricsTraitspec:…Monitoring Exporter Component Review the following example monitoringExporter configuration in OAM Component.\nworkload:apiVersion:oam.verrazzano.io/v1alpha1kind:VerrazzanoWebLogicWorkload…monitoringExporter:imagePullPolicy:IfNotPresentconfiguration:\u000bmetricsNameSnakeCase:truedomainQualifier:truequeries:\u000b- key:name\u000bkeyName:locationprefix:wls_server_…Pod annotations The following annotations can be used for enabling metrics on pods.\n prometheus.io/metricsEnabled: \"true” - Enables metrics scraping. prometheus.io/metricsPath: /metrics - Specifies metrics scraping path. prometheus.io/metricsPort: ”8080\" - Specifies metrics scraping port.  Example:\napiVersion:v1kind:Podmetadata:annotations:prometheus.io/path:/metricsprometheus.io/port:\"8080\"prometheus.io/scrape:\"true\"Logging WebLogic logs are sent to OpenSearch, which is installed in the Verrazzano cluster. The Fluentd sidecar is injected into each WebLogic pod to send server logs to stdout. The Fluentd DaemonSet in the verrazzano-system namespace sends logs to OpenSearch. In OpenSearch, logs are indexed by namespace.\nLift-and-Shift WebLogic applications Verrazzano makes it easy for you to migrate WebLogic applications from on-premises installations to the cloud. You can use WebLogic Deploy Tooling (WDT) to build the domain model and the WebLogic Image Tool (WIT) to build the WebLogic domain image.\nFor detailed instructions, see the Lift-and-Shift Guide.\nDeploy WebLogic applications in Verrazzano Step 1. Create a WebLogic domain image.\n To deploy a WebLogic domain in Kubernetes, first you need to create a Docker image for the WebLogic domain. To create a WebLogic domain image using WebLogic Deploy Tooling (WDT), follow the instructions in Example Image with a WLS Domain.  Step 2. Create a VerrazzanoWebLogicWorkload component.\n To deploy and run the WebLogic domain image in Verrazzano, create the VerrazzanoWebLogicWorkload component that specifies the definition and parameters for the WebLogic domain contained in the image. For an example VerrazzanoWebLogicWorkload Component resource created for a sample WebLogic domain, see the todo-domain example. For all the option supported by the WebLogic domain configuration, see Domain.md.  Step 3. Create ApplicationConfiguration for WebLogic application.\n Next, create an ApplicationConfiguration that uses the VerrazzanoWebLogicWorkload component you created for the WebLogic domain. For an example ApplicationConfiguration using a VerrazzanoWebLogicWorkload component, see the ToDo List example application.  Step 4. Verify the domain.\n Verrazzano creates the underlying domain Kubernetes resource from the VerrazzanoWebLogicWorkload component which is then processed by the WebLogic Kubernetes Operator to create the Administration and Managed Server pods, and deploy the applications and resources associated with the WebLogic domain. To verify that the WebLogic domain is up and running, follow the steps found here.  Database connections Typically, WebLogic applications make database connections using the connection information present in the JDBCSystemResources created in a WebLogic domain. To implement this in Verrazzano, databases are deployed as separate components and the connection information is made available to the WebLogic domain using a WDT Model.\nStep 1. Deploy the database in Verrazzano.\n To deploy a database, you need to create the corresponding Component and ApplicationConfiguration that will run the database in a pod and expose its connection information as a Service. For an example, look at the tododomain-mysql descriptor.  Step 2. Create a WebLogic resource ConfigMap.\n Next, create a ConfigMap that will contain the JDBCSystemResource definition with connection information for the database. For an example, see the tododomain-configmap definition in the ToDo List example application configuration.  Step 3. Configure the WebLogic domain to use the WebLogic resource ConfigMap.\n You can configure the ConfigMap, containing the resource information for the JDBCSystemResource, in the configuration section of the VerrazzanoWebLogicWorkload component of the WebLogic domain.  ...configuration:introspectorJobActiveDeadlineSeconds:900model:configMap:tododomain-configmapdomainType:WLS...For more details, see the ToDo List example application configuration.\nIngresses To access the endpoints for a Java EE application deployed as part of a VerrazzanoWebLogicWorkload component, Verrazzano plets you specify an IngressTrait for the component which is then translated to an Istio ingress gateway and VirtualService. For an example, see the ToDo List example application, where the IngressTrait is configured for the application endpoint.\n...- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:IngressTraitspec:rules:- paths:# application todo- path:\"/todo\"pathType:Prefix...Then, you can access the endpoint using the Istio gateway, as described in Step 8. Access the ToDo List application.\n$ HOST=$(kubectl get gateways.networking.istio.io -n todo-list -o jsonpath={.items[0].spec.servers[0].hosts[0]}) $ ADDRESS=$(kubectl get service -n istio-system istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ curl -sk https://${HOST}/todo/ --resolve ${HOST}:443:${ADDRESS} References  WebLogic Kubernetes Operator documentation WebLogic Kubernetes Operator GitHub repository WebLogic Domain CR Verrazzano Application Workloads Lift and Shift Guide  ","categories":"","description":"Develop WebLogic applications with Verrazzano","excerpt":"Develop WebLogic applications with Verrazzano","ref":"/docs/applications/workloads/weblogic/","tags":"","title":"WebLogic Workload"},{"body":"Before you begin   Install Verrazzano by following the installation instructions.\n  To download the example image, you must first accept the license agreement.\n In a browser, navigate to https://container-registry.oracle.com/ and sign in. Search for example-bobbys-coherence, example-bobbys-front-end, example-bobs-books-order-manager, example-roberts-coherence, and weblogic. For each one:  Select the image name in the results. From the drop-down menu, select your language and click Continue. Then read and accept the license agreement.    NOTE: The Bob’s Books example application deployment files are contained in the Verrazzano project located at \u003cVERRAZZANO_HOME\u003e/examples/bobs-books, where \u003cVERRAZZANO_HOME\u003e is the root of the Verrazzano project.\n  Overview Bob’s Books consists of three main parts:\n A back-end “order processing” application, which is a Java EE application with REST services and a very simple JSP UI, which stores data in a MySQL database. This application runs on WebLogic Server. A front-end web store “Robert’s Books”, which is a general book seller. This is implemented as a Helidon microservice, which gets book data from Coherence, uses a Coherence cache store to persist data for the order manager, and has a React web UI. A front-end web store “Bobby’s Books”, which is a specialty children’s book store. This is implemented as a Helidon microservice, which gets book data from a (different) Coherence cache store, interfaces directly with the order manager, and has a JSF web UI running on WebLogic Server.  For more information and the source code of this application, see the Verrazzano Examples.\nDeploy the example application NOTE To run this application in the default namespace:\n$kubectl label namespace default verrazzano-managed=true istio-injection=enabled If you chose the default namespace, you can skip Step 1. and ignore the -n option in the rest of the commands.\n   Create a namespace for the example and add a label identifying the namespace as managed by Verrazzano.\n$ kubectl create namespace bobs-books $ kubectl label namespace bobs-books verrazzano-managed=true istio-injection=enabled   Create a docker-registry secret to enable pulling the example image from the registry.\n$ kubectl create secret docker-registry bobs-books-repo-credentials \\ --docker-server=container-registry.oracle.com \\ --docker-username=YOUR_REGISTRY_USERNAME \\ --docker-password=YOUR_REGISTRY_PASSWORD \\ --docker-email=YOUR_REGISTRY_EMAIL \\ -n bobs-books Replace YOUR_REGISTRY_USERNAME, YOUR_REGISTRY_PASSWORD, and YOUR_REGISTRY_EMAIL with the values you use to access the registry.\n  Create secrets for the WebLogic domains:\n# Replace the values of the WLS_USERNAME and WLS_PASSWORD environment variables as appropriate. $ export WLS_USERNAME=\u003cusername\u003e $ export WLS_PASSWORD=\u003cpassword\u003e $ kubectl create secret generic bobbys-front-end-weblogic-credentials \\ --from-literal=password=$WLS_PASSWORD \\ --from-literal=username=$WLS_USERNAME \\ -n bobs-books $ kubectl create secret generic bobs-bookstore-weblogic-credentials \\ --from-literal=password=$WLS_PASSWORD \\ --from-literal=username=$WLS_USERNAME \\ -n bobs-books $ kubectl create secret generic mysql-credentials \\ --from-literal=username=$WLS_USERNAME \\ --from-literal=password=$WLS_PASSWORD \\ --from-literal=url=jdbc:mysql://mysql.bobs-books.svc.cluster.local:3306/books \\ -n bobs-books Note that the example application is preconfigured to use specific secret names. For the source code of this application, see the Verrazzano Examples. If you want to use secret names that are different from what is specified in the source code, you will need to update the corresponding YAML file and rebuild the Docker images for the example application.\n  To deploy the application, apply the example resources.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/bobs-books/bobs-books-comp.yaml -n bobs-books $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/bobs-books/bobs-books-app.yaml -n bobs-books   Wait for all of the pods in the Bob’s Books example application to be ready. You can monitor their progress by listing the pods and inspecting the output, or you can use the kubectl wait command.\nYou may need to repeat the kubectl wait command several times before it is successful. The WebLogic Server and Coherence pods may take a while to be created and Ready.\n$ kubectl get pods -n bobs-books # -or- # $ kubectl wait \\ --for=condition=Ready pods \\ --all -n bobs-books \\ --timeout=600s   Get the EXTERNAL_IP address of the istio-ingressgateway service.\n$ ADDRESS=$(kubectl get service \\ -n istio-system istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo $ADDRESS # Sample output 11.22.33.44   Get the generated host name for the application.\n$ HOST=$(kubectl get gateways.networking.istio.io bobs-books-bobs-books-gw \\ -n bobs-books \\ -o jsonpath='{.spec.servers[0].hosts[0]}') $ echo $HOST # Sample output bobs-books.bobs-books.11.22.33.44.nip.io   Access the application. To access the application in a browser, you will need to do one of the following:\n  Option 1: If you are using nip.io, then you can access the application using the generated host name. For example:\n  Robert’s Books UI at https://bobs-books.bobs-books.11.22.33.44.nip.io/.\n  Bobby’s Books UI at https://bobs-books.bobs-books.11.22.33.44.nip.io/bobbys-front-end/.\n  Bob’s order manager UI at https://bobs-books.bobs-books.11.22.33.44.nip.io/bobs-bookstore-order-manager/orders.\n    Option 2: Temporarily, modify the /etc/hosts file (on Mac or Linux) or c:\\Windows\\System32\\Drivers\\etc\\hosts file (on Windows 10), to add an entry mapping the host used by the application to the external IP address assigned to your gateway. For example:\n11.22.33.44 bobs-books.example.com Then, you can use a browser to access the application, as shown:\n  Robert’s Books UI at https://bobs-books.example.com/.\n  Bobby’s Books UI at https://bobs-books.example.com/bobbys-front-end/.\n  Bob’s order manager UI at https://bobs-books.example.com/bobs-bookstore-order-manager/orders.\n    Option 3: Alternatively, point your own DNS name to the load balancer’s external IP address. In this case, you would need to have edited the bobs-books-app.yaml file to use the appropriate values under the hosts section for the application (such as your-roberts-books-host.your.domain), before deploying the application. Then, you can use a browser to access the application, as shown:\n  Robert’s Books UI at https://\u003cyour-roberts-books-host.your.domain\u003e/.\n  Bobby’s Books UI at https://\u003cyour-bobbys-books-host.your.domain\u003e/bobbys-front-end/.\n  Bob’s order manager UI at https://\u003cyour-bobs-orders-host.your.domain\u003e/.\n      Access the applications using the WLS Administration Console Use the WebLogic Server Administration Console to access the applications as follows.\nNOTE It is recommended that the WebLogic Server Administration Console not be exposed publicly.  Access bobs-bookstore   Set up port forwarding.\n$ kubectl port-forward pods/bobs-bookstore-adminserver 7001:7001 -n bobs-books NOTE: If you are using the Oracle Cloud Infrastructure Cloud Shell to run kubectl, in order to access the WebLogic Server Administration Console using port forwarding, you will need to run kubectl on another machine.\n  Access the WebLogic Server Administration Console from your browser.\nhttp://localhost:7001/console   Access bobbys-front-end   Set up port forwarding.\n$ kubectl port-forward pods/bobbys-front-end-adminserver 7001:7001 -n bobs-books NOTE: If you are using the Oracle Cloud Infrastructure Cloud Shell to run kubectl, in order to access the WebLogic Server Administration Console using port forwarding, you will need to run kubectl on another machine.\n  Access the WebLogic Server Administration Console from your browser.\nhttp://localhost:7001/console   Troubleshooting   Verify that the application configuration, domains, Coherence resources, and ingress trait all exist.\n$ kubectl get ApplicationConfiguration -n bobs-books $ kubectl get Domain -n bobs-books $ kubectl get Coherence -n bobs-books $ kubectl get IngressTrait -n bobs-books   Verify that the service pods are successfully created and transition to the READY state. Note that this may take a few minutes and that you may see some of the services terminate and restart.\n$ kubectl get pods -n bobs-books # Sample output NAME READY STATUS RESTARTS AGE bobbys-helidon-stock-application-868b5965c8-dk2xb 3/3 Running 0 19h bobbys-coherence-0 2/2 Running 0 19h bobbys-front-end-adminserver 3/3 Running 0 19h bobbys-front-end-managed-server1 3/3 Running 0 19h bobs-bookstore-adminserver 3/3 Running 0 19h bobs-bookstore-managed-server1 3/3 Running 0 19h mysql-669665fb54-9m8wq 2/2 Running 0 19h robert-helidon-96997fcd5-kzjkf 3/3 Running 0 19h robert-helidon-96997fcd5-nlswm 3/3 Running 0 19h roberts-coherence-0 2/2 Running 0 17h roberts-coherence-1 2/2 Running 0 17h   Undeploy the application   To undeploy the application, delete the Bob’s Books OAM resources.\n$ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/bobs-books/bobs-books-app.yaml -n bobs-books $ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/bobs-books/bobs-books-comp.yaml -n bobs-books   Delete the namespace bobs-books after the application pods are terminated. The secrets created for the WebLogic domain also will be deleted.\n$ kubectl delete namespace bobs-books   ","categories":"","description":"An example application based on WebLogic, Helidon, and Coherence","excerpt":"An example application based on WebLogic, Helidon, and Coherence","ref":"/docs/samples/bobs-books/","tags":"","title":"Bob's Books"},{"body":"A Verrazzano instance is comprised of both Verrazzano components and several third party products. Collectively, these components are called the Verrazzano system components. In addition, after Verrazzano is installed, a Verrazzano instance can include applications deployed by the user. Applications can also be referred to as components, not to be confused with OAM Components.\nAll of the system components and applications use the network to some degree. Verrazzano configures networking to provide network security and traffic management. Network settings are configured both at installation and during runtime as applications as are deployed into the Kubernetes cluster.\nHigh-level overview The following diagram shows the high-level overview of Verrazzano networking using ExternalDNS and Let’s Encrypt for certificates. ExternalDNS and cert-manager both run outside the mesh and connect to external services using TLS. This diagram does not show Prometheus scraping.\nVerrazzano system traffic enters a platform load balancer over TLS and is routed to the NGINX Ingress Controller, where TLS is terminated. From there, the traffic is routed to one of the system components in the mesh over mTLS, or using HTTP to a system component, outside the mesh.\nApplication traffic enters a second Oracle Cloud Infrastructure load balancer over TLS and is routed to the Istio ingress gateway, where TLS is terminated. From there, the traffic is routed to one of several applications using mTLS.\nNOTE: Applications can be deployed outside the mesh, but the Istio ingress gateway will send traffic to them using plaintext. You need to do some additional configuration to enable TLS passthrough, as described at Istio Gateway Passthrough.\nHigh-level network diagram Platform network connectivity A Kubernetes cluster is installed on a platform, such as Oracle OKE, an on-premises installation, a hybrid cloud topology, or such. Verrazzano interfaces only with Kubernetes; it has no knowledge of platform topology or network security. You must ensure that there is network connectivity. For example, the ingresses might use a platform load balancer that provides the entry point into the cluster for Verrazzano consoles and applications. These load balancer IP addresses must be accessible for your users. In the multicluster case, clusters might be on different platform technologies with firewalls between them. Again, you must ensure that the clusters have network connectivity.\nNetwork configuration during installation A summary of the network-related configuration follows.\nVerrazzano does the following as it relates to networking:\n Installs and configures NGINX Ingress Controller. Creates Ingress resources for system components. Installs and configures Istio. Enables strict mTLS for the mesh by creating an Istio PeerAuthentication resource. Creates an Istio egress gateway service. Creates an Istio ingress gateway service. Configures several Verrazzano system components to be in the mesh. Optionally, installs ExternalDNS and creates DNS records. Creates certificates required by TLS, used by system components. Creates certificates required by Kubernetes API server to call a webhook. Creates NetworkPolicies for all of the system components.  Network configuration during application life cycle Verrazzano does the following as it relates to applications being deployed and terminated:\n Optionally, creates an Istio Gateway and VirtualService resources. Creates Istio AuthorizationPolicies, as needed. Creates Istio DestinationRules, as needed. Optionally, creates a self-signed certificate for the application. Optionally, creates DNS records using ExternalDNS.  ","categories":"","description":"","excerpt":"A Verrazzano instance is comprised of both Verrazzano components and several third party products. Collectively, these components are called the Verrazzano system components. In addition, after …","ref":"/docs/networking/","tags":"","title":"Networking"},{"body":"Prepare for the Oracle Cloud Native Environment installation Oracle Cloud Native Environment can be installed in several different types of environments. These range from physical, on-premises hardware to virtualized cloud infrastructure. The Oracle Cloud Native Environment installation instructions assume that networking and compute resources already exist. The basic infrastructure requirements are a network with a public and private subnet and a set of hosts connected to those networks.\nOracle Cloud Infrastructure example The following is an example of Oracle Cloud Infrastructure that can be used to evaluate Verrazzano installed on Oracle Cloud Native Environment. If other environments are used, the capacity and configuration should be similar.\nYou can use the VCN Wizard of the Oracle Cloud Infrastructure Console to automatically create most of the described network infrastructure. Additional security lists/rules, as detailed below, need to be added manually. All CIDR values provided are examples and can be customized as required.\nVirtual Cloud Network (for example, CIDR 10.0.0.0/16) Public Subnet (for example, CIDR 10.0.0.0/24)\nSecurity List / Ingress Rules\n   Stateless Destination Protocol Source Ports Destination Ports Type \u0026 Code Description     No 0.0.0.0/0 ICMP   3, 4 ICMP errors   No 10.0.0.0/16 ICMP   3 ICMP errors   No 0.0.0.0/0 TCP All 22  SSH   No 0.0.0.0/0 TCP All 80  HTTP load balancer   No 0.0.0.0/0 TCP All 443  HTTPS load balancer    Security List / Egress Rules\n   Stateless Destination Protocol Source Ports Destination Ports Type \u0026 Code Description     No 10.0.1.0/24 TCP All 22  SSH   No 10.0.1.0/24 TCP All 30080  HTTP load balancer   No 10.0.1.0/24 TCP All 30443  HTTPS load balancer   No 10.0.1.0/24 TCP All 31380  HTTP load balancer   No 10.0.1.0/24 TCP All 31390  HTTPS load balancer    Private Subnet (for example, CIDR 10.0.1.0/24)\nSecurity List / Ingress Rules\n   Stateless Destination Protocol Source Ports Destination Ports Type \u0026 Code Description     No 0.0.0.0/0 ICMP   3, 4 ICMP errors   No 10.0.0.0/16 ICMP   3 ICMP errors   No 10.0.0.0/16 TCP All 22  SSH   No 10.0.0.0/24 TCP All 30080  HTTP load balancer   No 10.0.0.0/24 TCP All 30443  HTTPS load balancer   No 10.0.0.0/24 TCP All 31380  HTTP load balancer   No 10.0.0.0/24 TCP All 31390  HTTPS load balancer   No 10.0.1.0/24 TCP All 2379-2380  Kubernetes etcd   No 10.0.1.0/24 TCP All 6443  Kubernetes API Server   No 10.0.1.0/24 TCP All 6446  MySQL   No 10.0.1.0/24 TCP All 8090-8091  Oracle Cloud Native Environment Platform Agent   No 10.0.1.0/24 UDP All 8472  Flannel   No 10.0.1.0/24 TCP All 10250-10255  Kubernetes Kublet    Security List / Egress Rules\n   Stateless Destination Protocol Source Ports Destination Ports Type and Code Description     No 10.0.0.0/0 TCP    All egress traffic    DHCP Options\n   DNS Type     Internet and VCN Resolver    Route Tables\nPublic Subnet Route Table Rules\n   Destination Target     0.0.0.0/0 Internet Gateway    Private Subnet Route Table Rules\n   Destination Target     0.0.0.0/0 NAT Gateway   All Oracle Cloud Infrastructure Services Service Gateway    Internet Gateway\nNAT Gateway\nService Gateway\nThe following compute resources adhere to the guidelines provided in Oracle Cloud Native Environment: Getting Started. The attributes indicated (for example, Subnet, RAM, Shape, and Image) are recommendations that have been tested. Other values can be used if required.\nCompute Instances\n   Role Subnet Suggested RAM Compatible VM Shape Compatible VM Image     SSH Jump Host Public 8GB VM.Standard.E2.1 Oracle Linux 7.8   Oracle Cloud Native Environment Operator Host Private 16GB VM.Standard.E2.2 Oracle Linux 7.8   Kubernetes Control Plane Node Private 32GB VM.Standard.E2.4 Oracle Linux 7.8   Kubernetes Worker Node 1 Private 32GB VM.Standard.E2.4 Oracle Linux 7.8   Kubernetes Worker Node 2 Private 32GB VM.Standard.E2.4 Oracle Linux 7.8   Kubernetes Worker Node 3 Private 32GB VM.Standard.E2.4 Oracle Linux 7.8    Install Oracle Cloud Native Environment Deploy Oracle Cloud Native Environment 1.4 with the Kubernetes module, following instructions from Oracle Cloud Native Environment: Getting Started.\n Use a single Kubernetes control plane node. Skip the Kubernetes API load balancer (3.4.3). Use private CA certificates (3.5.3). Install a Container Storage Interface Driver, such as OCI-CSI or Gluster.  Notes The oci-csi module does not elect a default StorageClass or configure policies for the CSIDrivers that it installs. A reasonable choice is the oci-bv StorageClass with its CSIDriver configured with the File group policy.\nkubectl patch sc oci-bv -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' kubectl apply -f - \u003c\u003cEOF apiVersion: storage.k8s.io/v1 kind: CSIDriver metadata: name: blockvolume.csi.oraclecloud.com spec: fsGroupPolicy: File EOF Prepare for the Verrazzano installation A Verrazzano Oracle Cloud Native Environment deployment requires:\n Load balancers in front of the worker nodes in the cluster. DNS records that reference the load balancers.  NOTE: The target ports for the load balancer backends cannot be determined until you install Verrazzano.\nYou can create the load balancers before you install, but post-installation configuration is required.\nExamples for meeting these requirements follow.\nLoad Balancers Verrazzano on Oracle Cloud Native Environment uses external load balancer services. These will not automatically be provided by Verrazzano or Kubernetes. Two load balancers must be deployed outside of the subnet used for the Kubernetes cluster. One load balancer is for management traffic and the other for application traffic.\nSpecific steps will differ for each load balancer provider, but a generic configuration and an Oracle Cloud Infrastructure example follow.\nGeneric configuration:  Target Host: Host names of Kubernetes worker nodes Target Ports: See table External Ports: See table Distribution: Round-robin Health Check: TCP  Backend for management load balancer You must install Verrazzano to get the target ports for each load balancer backend. In the following table, those ports are marked TBD. Run the following command to get the target ports for the NGINX Ingress Controller:\n$ kubectl get service ingress-controller-ingress-nginx-controller -n ingress-nginx In the PORT(S) column you will see the target port associated with port 80 and 443, for example: 80:30080/TCP,443:30443.\nUse these target port values for the NGINX Ingress Controller load balancer backend.\n   Service Name Type External Port Target Port     ingress-controller-nginx-ingress-controller TCP 80 TBD   ingress-controller-nginx-ingress-controller TCP 443 TBD    Backend for application load balancer Get the target ports for the Istio ingress gateway service using the following command:\n$ kubectl get service istio-ingressgateway -n istio-system Use these port values for the Istio ingress gateway load balancer backend.\n   Service Name Type External Port Target Port     istio-ingressgateway TCP 80 TBD   istio-ingressgateway TCP 443 TBD    Oracle Cloud Infrastructure example The following details can be used to create Oracle Cloud Infrastructure load balancers for accessing application and management user interfaces, respectively. These load balancers will route HTTP/HTTPS traffic from the Internet to the private subnet. If load balancers are desired, then they should be created now even though the application and management endpoints will be installed later.\nNOTE: In the following list, the using port 0 for the health check indicates that the backend ports should be used.\n Application Load Balancer: Public Subnet  Listeners  HTTP Listener: Protocol TCP, Port 80 HTTPS Listener: Protocol TCP, Port 443   Backend Sets  HTTP Backend Sets:  Health Check: Protocol TCP, Port 0 Backends: Kubernetes Worker Nodes, Port TBD, Distribution Policy Weighted Round Robin   HTTPS Backend Sets  Health Check: Protocol TCP, Port 0 Backends: Kubernetes Worker Nodes, Port TBD, Distribution Policy Weighted Round Robin       Management Load Balancer: Public Subnet  Listeners  HTTP Listener: Protocol TCP, Port 80 HTTPS Listener: Protocol TCP, Port 443   Backend Sets  HTTP Backend Sets:  Health Check: Protocol TCP, Port 0 Backends: Kubernetes Worker Nodes, Port TBD, Distribution Policy Weighted Round Robin   HTTPS Backend Sets  Health Check: Protocol TCP, Port 0 Backends: Kubernetes Worker Nodes, Port TBD, Distribution Policy Weighted Round Robin        DNS When using the Verrazzanospec.components.dns.external DNS type, the installer searches the DNS zone you provide for two specific A records. These are used to configure the cluster and should refer to external addresses of the load balancers in the previous step. The A records will need to be created manually.\nNOTE: At this time, the only supported deployment for Oracle Cloud Native Environment is the external DNS type.\n   Record Use     ingress-mgmt Set as the .spec.externalIPs value of the ingress-controller-nginx-ingress-controller service.   ingress-verrazzano Set as the .spec.externalIPs value of the istio-ingressgateway service.    For example:\n198.51.100.10 A ingress-mgmt.myenv.example.com. 203.0.113.10 A ingress-verrazzano.myenv.example.com. Verrazzano installation will result in a number of management services that need to point to the ingress-mgmt address.\nverrazzano.myenv.example.com CNAME ingress-mgmt.myenv.example.com. keycloak.myenv.example.com CNAME ingress-mgmt.myenv.example.com. rancher.myenv.example.com CNAME ingress-mgmt.myenv.example.com. grafana.vmi.system.myenv.example.com CNAME ingress-mgmt.myenv.example.com. prometheus.vmi.system.myenv.example.com CNAME ingress-mgmt.myenv.example.com. kiali.vmi.system.myenv.example.com CNAME ingress-mgmt.myenv.example.com. kibana.vmi.system.myenv.example.com CNAME ingress-mgmt.myenv.example.com. elasticsearch.vmi.system.myenv.example.com CNAME ingress-mgmt.myenv.example.com. For simplicity, an administrator may want to create wildcard DNS records for the management addresses:\n*.system.myenv.example.com CNAME ingress-mgmt.myenv.example.com. OR\n*.myenv.example.com CNAME ingress-mgmt.myenv.example.com. Oracle Cloud Infrastructure example DNS is configured in Oracle Cloud Infrastructure by creating DNS zones in the Oracle Cloud Infrastructure Console. When creating a DNS zone, use these values:\n Method: Manual Zone Name: \u003cdns-suffix\u003e Zone Type: Primary  The value for \u003cdns-suffix\u003e excludes the environment (for example, use the example.com portion of myenv.example.com).\nDNS A records must be manually added to the zone and published using values described above. DNS CNAME records, in the same way.\nDuring the Verrazzano install, these steps should be performed on the Oracle Cloud Native Environment operator node.\nEdit the sample Verrazzano custom resource install-olcne.yaml file and provide these configuration settings for your Oracle Cloud Native Environment:\n The value for spec.environmentName is a unique DNS subdomain for the cluster (for example, myenv in myenv.example.com). The value for spec.components.dns.external.suffix is the remainder of the DNS domain (for example, example.com in myenv.example.com). Under spec.components.ingress.nginxInstallArgs, the value for controller.service.externalIPs is the IP address of ingress-mgmt.\u003cmyenv\u003e.\u003cexample.com\u003e configured during DNS set up. Under spec.components.istio.istioInstallArgs, the value for gateways.istio-ingressgateway.externalIPs is the IP address of ingress-verrazzano.\u003cmyenv\u003e.\u003cexample.com\u003e configured during DNS set up.  You will install Verrazzano using the external DNS type (the example custom resource for Oracle Cloud Native Environment is already configured to use spec.components.dns.external).\nSet the following environment variable:\nThe value for \u003cpath to valid Kubernetes config\u003e is typically ${HOME}/.kube/config.\n$ export KUBECONFIG=$VERRAZZANO_KUBECONFIG Next steps To continue, see the Installation Guide.\n","categories":"","description":"Instructions for setting up an Oracle Cloud Native Environment cluster for Verrazzano","excerpt":"Instructions for setting up an Oracle Cloud Native Environment cluster for Verrazzano","ref":"/docs/setup/platforms/olcne/olcne/","tags":"","title":"Oracle Cloud Native Environment"},{"body":"Verrazzano supports Kubernetes Role-Based Access Control (RBAC) for Verrazzano resources, and integrates with Keycloak to enable Single Sign-On (SSO) across the Verrazzano Console and the Verrazzano Monitoring Instance (VMI) logging and metrics consoles. Verrazzano provides proxies that enable SSO and Kubernetes API access for Keycloak user accounts.\nFor information on how Verrazzano secures network traffic, see Network Security.\n","categories":"","description":"","excerpt":"Verrazzano supports Kubernetes Role-Based Access Control (RBAC) for Verrazzano resources, and integrates with Keycloak to enable Single Sign-On (SSO) across the Verrazzano Console and the Verrazzano …","ref":"/docs/security/","tags":"","title":"Security"},{"body":"You can install Verrazzano using a private Docker-compliant container registry. This requires the following:\n Loading all required Verrazzano container images into your own registry and repository. Installing the Verrazzano platform operator with the private registry and repository used to load the images.  To obtain the required Verrazzano images and install from your private registry, you must:\n Download the Verrazzano ZIP file from the Oracle Software Delivery Cloud:  In your browser, go to Oracle Software Delivery Cloud and log in with your credentials. In the drop-down menu next to the search bar, select Download Package. In the search bar, enter Verrazzano Enterprise Container Platform and click Search. Select the DLP: Oracle Verrazzano Enterprise Edition 1.2 link. This will add it to your download queue. Select the Continue link. Accept the license agreement and click Continue. Download the file:  To download the ZIP file directly, select the file link in the list. To download the ZIP file using Oracle Download Manager, click Download and run the Oracle Download Manager executable.     Prepare to do the private registry install:  Unzip the ZIP archive to a desired directory location. There will be two files, a compressed TAR file containing the product files and a checksum file. Go to the expanded archive directory. (Optional) Validate the checksum and the TAR file match. For example, $ shasum -c verrazzano_1.2.0.tar.gz.sha256 # Sample output verrazzano_1.2.0.tar.gz: OK  Expand the TAR file, for example, tar xvf verrazzano_1.2.0.tar.gz.   Load the product images into your private registry and install Verrazzano using the instructions in the README.md file that is packaged with the TAR file.  ","categories":"","description":"Instructions for setting up Verrazzano using a private container registry","excerpt":"Instructions for setting up Verrazzano using a private container registry","ref":"/docs/setup/private-registry/private-registry/","tags":"","title":"Use a Private Registry"},{"body":"Verrazzano provides a proxy that enables authentication and authorization for Keycloak users accessing Verrazzano resources. This proxy is automatically configured and deployed.\nKubernetes API The Verrazzano authentication proxy is used to authenticate and authorize Keycloak users, then impersonate them to the Kubernetes API, so that Keycloak users can access Kubernetes resources.\nThis capability is used primarily by the Verrazzano Console. The Console authenticates users against Keycloak, using the PKCE flow, obtains a bearer token, and then sends the token to the API along with the Kubernetes API request. The API proxy validates the token and, if valid, impersonates the user to the Kubernetes API server. This allows the Console to run Kubernetes API calls on behalf of Keycloak users, with Kubernetes enforcing role-based access control (RBAC) based on the impersonated identity.\nIn multicluster scenarios, the Console directs all Kubernetes API requests to the admin cluster’s authentication proxy. If a request refers to a resource in a different cluster, the authentication proxy forwards the request, along with the user’s authentication token, to the authentication proxy running in the remote cluster.\nSingle Sign-On (SSO) The Verrazzano authentication proxy provides SSO across the Verrazzano Console and the Verrazzano Monitoring Instance (VMI) logging and metrics consoles. When an unauthenticated request is received by the proxy, it runs the OIDC PKCE authentication flow to obtain tokens for the user. If the user is already authenticated to Keycloak (because they have already accessed either the Verrazzano Console or another VMI component), Keycloak returns tokens based on the existing user session, and the process is transparent to the user. If not, Keycloak will authenticate the user, establishing a session, before returning tokens.\n","categories":"","description":"Learn about the Verrazzano authentication proxy","excerpt":"Learn about the Verrazzano authentication proxy","ref":"/docs/security/proxies/proxies/","tags":"","title":"Verrazzano Authentication Proxy"},{"body":"During installation, Verrazzano generates several default accounts.\n   System Account Secret Secret Namespace Description     Keycloak keycloakadmin keycloak-http keycloak Keycloak root user: full administrative privileges for Keycloak.   Keycloak verrazzano verrazzano verrazzano-system Verrazzano root user: can manage the verrazzano-system realm in Keycloak, including managing users in that realm. This user is a member of the verrazzano-admins group, and, if default role bindings are used, has the verrazzano-admin role.   Rancher admin rancher-admin-secret cattle-system Rancher root user: full administrative privileges for Rancher.    ","categories":"","description":"Learn about default user accounts","excerpt":"Learn about default user accounts","ref":"/docs/security/accounts/accounts/","tags":"","title":"Default User Accounts"},{"body":"Kind is a tool for running local Kubernetes clusters using Docker container “nodes”. Follow these instructions to prepare a Kind cluster for running Verrazzano.\nNOTE Kind is not recommended for use on macOS and Windows because the Docker network is not directly exposed to the host.  Prerequisites  Install Docker Install Kind  Prepare the Kind cluster To prepare the Kind cluster for use with Verrazzano, you must create the cluster and then install and configure MetalLB in that cluster.\nYou can create the Kind cluster in two ways: with or without image caching; image caching can speed up your installation time.\nCreate a Kind cluster Kind images are prebuilt for each release. To find images suitable for a given release, check the release notes for your Kind version (check with kind version). There you’ll find a complete listing of images created for a Kind release.\nThe following example references a Kubernetes v1.21.1-based image built for Kind v0.11.1. Replace that image with one suitable for the Kind release you are using.\n$ kind create cluster --config - \u003c\u003cEOF kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane image: kindest/node:v1.21.1@sha256:69860bda5563ac81e3c0057d654b5253219618a22ec3a346306239bba8cfa1a6 kubeadmConfigPatches: - | kind: ClusterConfiguration apiServer: extraArgs: \"service-account-issuer\": \"kubernetes.default.svc\" \"service-account-signing-key-file\": \"/etc/kubernetes/pki/sa.key\" EOF Create a Kind cluster with image caching While developing or experimenting with Verrazzano, you might destroy and re-create your Kind cluster multiple times. To speed up Verrazzano installation, follow these steps to ensure that the image cache used by containerd inside a Kind cluster, is preserved across clusters. Subsequent installations will be faster because they will not need to pull the images again.\n1. Create a named Docker volume that will be used for the image cache, and note its Mountpoint path. In this example, the volume is named containerd.\n$ docker volume create containerd $ docker volume inspect containerd #Sample output { \"CreatedAt\": \"2021-01-11T16:27:47Z\", \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/containerd/_data\", \"Name\": \"containerd\", \"Options\": {}, \"Scope\": \"local\" } 2. Specify the Mountpoint path obtained, as the hostPath under extraMounts in your Kind configuration file, with a containerPath of /var/lib/containerd, which is the default containerd image caching location inside the Kind container. An example of the modified Kind configuration is shown in the following create cluster command:\n$ kind create cluster --config - \u003c\u003cEOF kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane image: kindest/node:v1.21.1@sha256:69860bda5563ac81e3c0057d654b5253219618a22ec3a346306239bba8cfa1a6 kubeadmConfigPatches: - | kind: ClusterConfiguration apiServer: extraArgs: \"service-account-issuer\": \"kubernetes.default.svc\" \"service-account-signing-key-file\": \"/etc/kubernetes/pki/sa.key\" extraMounts: - hostPath: /var/lib/docker/volumes/containerd/_data containerPath: /var/lib/containerd #This is the location of the image cache inside the Kind container EOF Install and configure MetalLB By default, Kind does not provide an implementation of network load balancers (Services of type LoadBalancer). MetalLB offers a network load balancer implementation.\nTo install MetalLB:\n$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.11.0/manifests/namespace.yaml $ kubectl create secret generic \\ -n metallb-system memberlist \\ --from-literal=secretkey=\"$(openssl rand -base64 128)\" $ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.11.0/manifests/metallb.yaml For further details, see the MetalLB installation guide.\nMetalLB is idle until configured. Configure MetalLB in Layer 2 mode and give it control over a range of IP addresses in the kind Docker network. In versions v0.7.0 and earlier, Kind uses Docker’s default bridge network; in versions v0.8.0 and later, it creates its own bridge network in Kind.\nTo determine the subnet of the kind Docker network in Kind v0.8.0 and later:\n$ docker inspect kind | jq '.[0].IPAM.Config[0].Subnet' -r # Sample output 172.18.0.0/16 To determine the subnet of the kind Docker network in Kind v0.7.0 and earlier:\n$ docker inspect bridge | jq '.[0].IPAM.Config[0].Subnet' -r # Sample output 172.17.0.0/16 For use by MetalLB, assign a range of IP addresses at the end of the kind network’s subnet CIDR range.\n$ kubectl apply -f - \u003c\u003c-EOF apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: my-ip-space protocol: layer2 addresses: - 172.18.0.230-172.18.0.250 EOF Next steps To continue, see the Installation Guide.\n","categories":"","description":"Instructions for setting up a Kind cluster for Verrazzano","excerpt":"Instructions for setting up a Kind cluster for Verrazzano","ref":"/docs/setup/platforms/kind/kind/","tags":"","title":"Kind"},{"body":"Prerequisites Prior to running the multicluster examples, complete the multicluster installation and managed cluster registration documented here.\n","categories":"","description":"Examples of using Verrazzano in a multicluster environment","excerpt":"Examples of using Verrazzano in a multicluster environment","ref":"/docs/samples/multicluster/","tags":"","title":"Multicluster"},{"body":"Verrazzano supports two cluster topologies for an OpenSearch cluster:\n A single-node cluster (master, ingest, and data roles performed by a single node). A multi-node cluster configuration with separate master, data, and ingest nodes.  Installation Profiles describes the default OpenSearch cluster configurations provided by Verrazzano.\nYou can customize the node characteristics of your OpenSearch cluster by using the spec.components.elasticsearch.installArgs field in the Verrazzano custom resource. When installing Verrazzano, you can use this field to specify a list of Helm value overrides for the OpenSearch configuration.\nThese Helm overrides let you customize the following node characteristics:\n Number of node replicas. Memory request size per node. Storage request size (data nodes only).  The following table lists the Helm values in the Verrazzano system chart related to OpenSearch nodes.\n   Name Description     nodes.master.replicas Number of master node replicas.   nodes.master.requests.memory Memory request amount expressed as a Quantity.   nodes.ingest.replicas Number of ingest node replicas.   nodes.ingest.requests.memory Memory request amount expressed as a Quantity.   nodes.data.replicas Number of data node replicas.   nodes.data.requests.memory Memory request amount expressed as a Quantity.   nodes.data.requests.storage Storage request amount expressed as a Quantity.    The following example overrides the dev installation profile, OpenSearch configuration (a single-node cluster with 1Gi of memory and ephemeral storage) to use a multi-node cluster with persistent storage. Note that the public API references Elasticsearch, the API will change to OpenSearch in an upcoming release.\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: custom-es-example spec: profile: dev components: elasticsearch: installArgs: - name: nodes.master.replicas value: \"1\" - name: nodes.master.requests.memory value: \"1G\" - name: nodes.ingest.replicas value: \"1\" - name: nodes.ingest.requests.memory value: \"1G\" - name: nodes.data.replicas value: \"3\" - name: nodes.data.requests.memory value: \"1.5G\" - name: nodes.data.requests.storage value: \"10Gi\" Listing the pods and persistent volumes in the verrazzano-system namespace for the previous configuration shows the expected nodes are running with the appropriate data volumes:\n$ kubectl get pvc,pod -n verrazzano-system # Sample output NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/elasticsearch-master-vmi-system-es-master-0 Bound pvc-8ffff457-4d72-4a72-89ba-2cdcb8eade38 10Gi RWO standard 6m51s persistentvolumeclaim/vmi-system-es-data Bound pvc-e32c2182-46ba-4789-b577-195874b3dd69 10Gi RWO standard 6m53s persistentvolumeclaim/vmi-system-es-data-1 Bound pvc-67789196-d688-4d06-b074-77655a913552 10Gi RWO standard 6m53s persistentvolumeclaim/vmi-system-es-data-2 Bound pvc-43e07e3e-0713-4ab1-ac3f-812069c35cbb 10Gi RWO standard 6m53s NAME READY STATUS RESTARTS AGE pod/coherence-operator-6986d6cf95-6b58p 1/1 Running 2 7m3s pod/fluentd-fn28c 2/2 Running 2 7m12s pod/oam-kubernetes-runtime-679c6f6775-79tvm 1/1 Running 0 5m11s pod/verrazzano-api-58c5f65c8-6zbpc 2/2 Running 0 7m12s pod/verrazzano-application-operator-5766b899fd-9fjhb 1/1 Running 0 4m55s pod/verrazzano-console-6599854544-pw56c 2/2 Running 0 7m12s pod/verrazzano-monitoring-operator-55877766d4-9ktvh 1/1 Running 0 7m12s pod/verrazzano-operator-75b5cd49fc-68cm4 1/1 Running 0 7m12s pod/vmi-system-es-data-0-5884cfb84d-hn8xg 2/2 Running 0 6m52s pod/vmi-system-es-data-1-679775494f-pdwzf 2/2 Running 0 6m52s pod/vmi-system-es-data-2-5886d745c5-6pscm 2/2 Running 0 6m52s pod/vmi-system-es-ingest-795749ddd8-cs4pc 3/3 Running 0 6m52s pod/vmi-system-es-master-0 2/2 Running 0 6m51s pod/vmi-system-grafana-b94fcbb67-ktwf8 3/3 Running 0 6m52s pod/vmi-system-kibana-6594cfccc-j8gp5 3/3 Running 0 6m51s pod/vmi-system-prometheus-0-75864fc668-s5xv8 4/4 Running 0 44s pod/weblogic-operator-5bd7bb6fb5-wz5cr 2/2 Running 0 6m30s Note that the master node uses the same amount of persistent storage as is configured for the data nodes.\nRunning the command kubectl describe pod -n verrazzano-system vmi-system-es-data-0-5884cfb84d-hn8xg shows the requested amount of memory:\nContainers: es-data: Container ID: containerd://cc01f24b107da0e1e90a05a49c7fd969761f59a81316fa01f7cc56a166684628 Image: ghcr.io/verrazzano/opensearch:1.2.3-20220207214930-833b159de83 Image ID: ghcr.io/verrazzano/elasticsearch@sha256:3d2cbb539f9ebba991c6f36db4fbaa9dc9c03e6192a28787869f7850cc2bd66c Ports: 9200/TCP, 9300/TCP Host Ports: 0/TCP, 0/TCP State: Running Started: Thu, 29 Jul 2021 06:04:17 +0000 Ready: True Restart Count: 0 Requests: memory: 1500M ","categories":"","description":"Learn how to customize your OpenSearch cluster configuration","excerpt":"Learn how to customize your OpenSearch cluster configuration","ref":"/docs/setup/customizing/opensearch/","tags":"","title":"Customize OpenSearch"},{"body":"Verrazzano uses NGINX for ingress to Verrazzano system components and Istio for application ingress. You can customize the NGINX and Istio ingress installation configurations using Helm overrides specified in the Verrazzano custom resource. For example, the following Verrazzano custom resource overrides the shape of an Oracle Cloud Infrastructure load balancer for both NGINX and Istio ingresses:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: custom-lb-settings spec: profile: prod components: ingress: type: LoadBalancer nginxInstallArgs: - name: controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-shape\" value: \"10Mbps\" istio: istioInstallArgs: - name: gateways.istio-ingressgateway.serviceAnnotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-shape\" value: \"10Mbps\" The previous entries use dot notation to represent YAML values.\nFor example:\n - name: controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/oci-load-balancer-shape\" value: \"10Mbps\" Is translated into:\ncontroler: service: annotations: service.beta.kubernetes.io/oci-load-balancer-shape: 10Mbps For more information about setting component overrides, see Customizing the Chart Before Installing.\n","categories":"","description":"Customize Verrazzano NGINX and Istio ingress installation settings","excerpt":"Customize Verrazzano NGINX and Istio ingress installation settings","ref":"/docs/setup/customizing/ingress/","tags":"","title":"Customize Ingress"},{"body":"The Verrazzano metrics stack automates metrics aggregation and consists of Prometheus and Grafana components. Metrics sources expose system and application metrics. The Prometheus components retrieve and store the metrics and Grafana provides dashboards to visualize them.\nMetrics sources The following sections describe metrics sources that Verrazzano provides for OAM and standard Kubernetes applications.\nOAM Metrics sources produce metrics and expose them to the Kubernetes Prometheus system using annotations in the pods. The metrics annotations may differ slightly depending on the resource type. The following is an example of the WebLogic Prometheus-related configuration specified in the todo-list application pod:\n$ kubectl describe pod tododomain-adminserver -n todo-list\nAnnotations: prometheus.io/path: /wls-exporter/metrics prometheus.io/port: 7001 prometheus.io/scrape: true For other resource types, such as Coherence or Helidon, the annotations would look similar to this:\nAnnotations: verrazzano.io/metricsEnabled: true verrazzano.io/metricsPath: /metrics verrazzano.io/metricsPort: 8080 To look directly at the metrics that are being made available by the metric source, map the port and then access the path.\nFor example, for the previous metric source:\n  Map the port being used to expose the metrics.\n$ kubectl port-forward tododomain-adminserver 7001:7001 -n todo-list   Get the user name and password used to access the metrics source from the corresponding secret.\n$ kubectl get secret \\ --namespace todo-list tododomain-weblogic-credentials \\ -o jsonpath={.data.username} | base64 \\ --decode; echo $ kubectl get secret \\ --namespace todo-list tododomain-weblogic-credentials \\ -o jsonpath={.data.password} | base64 \\ --decode; echo   Access the metrics at the exported path, using the user name and password retrieved in the previous step.\n$ curl -u USERNAME:PASSWORD localhost:7001/wls-exporter/metrics   Standard Kubernetes workloads Verrazzano enables metric sources for Kubernetes workloads deployed without OAM components. Verrazzano supports the following workload types: Deployments, ReplicaSets, StatefulSets, and Pods. To enable metrics for Kubernetes workloads, you must label the workload namespace with verrazzano-managed=true.\nMetrics Template A Metrics Template is a custom resource created by Verrazzano to manage metrics configurations for standard Kubernetes workloads. Metrics templates can be placed in the workload namespace or the verrazzano-system namespace. By default, Verrazzano installs a metrics template named standard-k8s-metrics-template in the verrazzano-system namespace. This metrics template handles all the aforementioned workload types. If the default metrics template does not meet your requirements, then you can create your own metrics templates to extend and alter its functionality.\nAs outlined in the API, the metrics template contains a workloadSelector field that specifies the resources for which the template applies. If you want to forgo the workload selection and manually specify a template, you can add the annotation app.verrazzano.io/metrics=\u003ctemplate-name\u003e to the namespace of the workload or to the workload itself. Additionally, you can opt out of metrics for your namespace or workload by setting the annotation app.verrazzano.io/metrics=none.\nThe template matching precedence is as follows:\n  A workload is annotated.\na. A template matching the annotation value is found in the workload namespace.\nb. A template matching the annotation value is found in the verrazzano-system namespace.\nc. No template is found, an error is recorded, and metrics are not processed for this workload.\n  A workload namespace is annotated.\na. A template matching the annotation value is found in the workload namespace.\nb. A template matching the annotation value is found in the verrazzano-system namespace.\nc. No template is found, an error is recorded, and metrics are not processed for this namespace.\n  No annotation is present.\na. A template in the workload namespace matches the workload through the workloadSelector field.\nb. A template in the verrazzano-system namespace matches the workload through the workloadSelector field.\nc. No templates match the workload and metrics are not processed for this workload.\n  If a workload with no annotations matches multiple templates in a namespace, there is no guaranteed precedence in template matching. If this is the case, it is more reliable to specify the template you require by using an annotation.\nTo verify that the metrics template process was successful, follow these steps:\n Access the Prometheus console. From the console, use the navigation bar to access Status/Targets. On this page, you will see a target name with this formatting: \u003cworkload-namespace\u003e_\u003cworkload-name\u003e_\u003cworkload-type\u003e. Copy this job name for use in future queries. Verify that the State of this target is UP. Next, use the navigation bar to access the Graph. Here, use the job name you copied to construct this expression: {job=\"\u003cjob_name\u003e\"} Use the graph to execute this expression and verify that you see application metrics appear.  Prometheus overrides The standard-k8s-metrics-template metrics template installed by Verrazzano uses the following pod annotations to populate the Prometheus configuration. If not specified, Verrazzano will use these default values:\nAnnotations: prometheus.io/path: /metrics prometheus.io/port: 8080 prometheus.io/scrape: true To alter these values, annotate the workload pod with the corresponding annotations. For example, if you want to change the metrics path, then add the following to a Deployment definition:\napiVersion:apps/v1kind:Deploymentmetadata:name:hello-helidon-deploymentnamespace:hello-helidonannotations:app.verrazzano.io/metrics:standard-k8s-metrics-templatespec:template:metadata:# add path annotation to the pod templateannotations:prometheus.io/path:\"/custom/metrics/path\"Prometheus configuration If you want to create your own metrics template, you will need to construct a Prometheus scrape config. The scrape config uses Go Templates to generate configuration values based on Kubernetes resources. You can reference values in the workload and namespace definitions for use in the scrape config. For example, the default scrape config references the workload namespace field through this reference: .workload.metadata.namespace. Do not include the job_name field in your scrape config as it will be generated by Verrazzano. For guidance on how to construct a Prometheus scrape config, reference the scrapeConfigTemplate section in the Metrics Template example.\nMetrics server  Single pod per cluster. Named vmi-system-prometheus-* in verrazzano-system namespace. Discovers exposed metrics source endpoints. Scrapes metrics from metrics sources. Responsible for exposing all metrics.  Grafana Grafana provides visualization for your Prometheus metric data.\n Single pod per cluster. Named vmi-system-grafana-* in verrazzano-system namespace. Provides dashboards for metrics visualization.  To access Grafana:\n  Get the hostname from the Grafana ingress.\n$ kubectl get ingress vmi-system-grafana -n verrazzano-system # Sample output NAME CLASS HOSTS ADDRESS PORTS AGE vmi-system-grafana \u003cnone\u003e grafana.vmi.system.default.123.456.789.10.nip.io 123.456.789.10 80, 443 26h   Get the password for the user verrazzano.\n$ kubectl get secret \\ --namespace verrazzano-system verrazzano \\ -o jsonpath={.data.password} | base64 \\ --decode; echo   Access Grafana in a browser using the previous hostname.\n  Log in using the verrazzano user and the previous password.\n  From here, you can select an existing dashboard or create a new dashboard. To select an existing dashboard, use the drop-down list in the top left corner. The initial value of this list is Home.\nTo view host level metrics, select Host Metrics. This will provide system metrics for all of the nodes in your cluster.\nTo view the application metrics for the todo-list example application, select WebLogic Server Dashboard because the todo-list application is a WebLogic application.\n","categories":"","description":"Understand Verrazzano metrics gathering and viewing","excerpt":"Understand Verrazzano metrics gathering and viewing","ref":"/docs/monitoring/metrics/metrics/","tags":"","title":"Metrics"},{"body":"","categories":"","description":"View monitoring and diagnostic metrics","excerpt":"View monitoring and diagnostic metrics","ref":"/docs/monitoring/","tags":"","title":"Monitoring \u0026 Logging"},{"body":"Verrazzano provides the following support.\nKeycloak Applications can use the Verrazzano Keycloak server as an Identity Provider. Keycloak supports SAML 2.0 and OpenID Connect (OIDC) authentication and authorization flows. Verrazzano does not provide any explicit integrations for applications.\nNOTE If using Keycloak for application authentication and authorization, create a new realm to contain application users and clients. Do not use the verrazzano-system realm, or the default (Keycloak system) realm. The Keycloak root user account (keycloakadmin) has privileges to create realms.  Network security Verrazzano uses Istio to authenticate and authorize incoming network connections for applications. Verrazzano also provides support for configuring Kubernetes NetworkPolicy on Verrazzano projects. NetworkPolicy rules control where network connections can be made.\nNOTE Enforcement of NetworkPolicy requires that a Kubernetes CNI provider, such as Calico, be configured for the cluster.  For more information on how Verrazzano secures network traffic, see Network Security.\n","categories":"","description":"Learn about securing applications in Verrazzano","excerpt":"Learn about securing applications in Verrazzano","ref":"/docs/security/appsec/appsec/","tags":"","title":"Application Security"},{"body":"Prepare for the generic install Verrazzano requires that your Kubernetes cluster provides an implementation of network load balancers (Services of type LoadBalancer) for a production environment. If your generic Kubernetes implementation provides this feature, then you can use a default configuration of the Verrazzano custom resource with no customizations and follow the Installation Guide.\nNOTE Remember to not overlap network Classless Inter-Domain Routing (CIDR) blocks when designing and implementing your Kubernetes cluster, proper routing relies on that.  You can install a load balancer, such as MetalLB. This setup requires knowledge of networking both inside and outside your Kubernetes cluster. This would include specifics of your Container Network Interface (CNI) implementation, IP address allocation schemes, and routing that go beyond the scope of this documentation. For a Kind implementation, see Install and configure MetalLB.\nIt is possible to use a Kubernetes Service of type NodePort to test aspects of Verrazzano. This requires a good working knowledge of networking and has limited use cases.\nCustomizations Verrazzano is highly customizable. If your Kubernetes implementation requires a custom configuration, see Customize Installations.\nNext steps To continue, see the Installation Guide.\n","categories":"","description":"Instructions for setting up a generic Kubernetes cluster for Verrazzano","excerpt":"Instructions for setting up a generic Kubernetes cluster for Verrazzano","ref":"/docs/setup/platforms/generic/generic/","tags":"","title":"Generic Kubernetes"},{"body":"The following components allow persistent storage:\n OpenSearch Prometheus Grafana Keycloak/MySQL  By default, each Verrazzano installation profile has different storage characteristics. The dev profile uses ephemeral storage only, but in all other profiles, each of the listed components use persistent storage. For more information, see Profile Configurations.\nNOTE Ephemeral storage is not recommended for use in production; Kubernetes pods can be restarted at any time, leading to a loss of data and system instability if non-persistent storage is used. Persistent storage is recommended for all use cases beyond evaluation or development.  While each profile has its own default persistence settings, in each case you have the option to override the profile defaults to customize your persistence settings.\nCustomize persistent storage The following components can use persistent storage:\n OpenSearch OpenSearch Dashboards Prometheus Grafana Keycloak  You can customize the persistence settings for these components through the VerrazzanoSpec, as follows:\n Overriding the persistence settings for all components (Keycloak, Grafana, Prometheus, OpenSearch, and OpenSearch Dashboards) by using the defaultVolumeSource field. Overriding the persistence settings for Keycloak by using the volumeSource field on that component’s configuration.  You can set the global defaultVolumeSource and component-level volumeSource fields to one of the following values:\n   Value Storage     emptyDir Ephemeral storage; should not be used for production scenarios.   persistentVolumeClaim A PersistentVolumeClaimVolumeSource where the claimSource field references a named volumeClaimSpecTemplate.    When you want to use a persistentVolumeClaim to override the storage settings for components, you must do the following:\n Create a volumeClaimSpecTemplate which identifies the desired persistence settings. Configure a persistentVolumeClaim for the component where the claimName field references the template you created previously.  This lets you create named persistence settings that can be shared across multiple components within a Verrazzano configuration. Note that the existence of a persistence template in the volumeClaimSpecTemplates list does not directly result in the creation of a persistent volume, or affect any component storage settings until it is referenced by either defaultVolumeSource or a specific component’s volumeSource.\nExamples Review the following customizing persistent storage examples:\n Customize persistence globally using defaultVolumeSource Customize PersistentVolumeClaim settings for Keycloak using volumeSource Use global and local persistence settings together  Customize persistence globally using defaultVolumeSource If defaultVolumeSource is configured, then that setting will be used for all components that require storage.\nFor example, the following Verrazzano configuration uses the prod profile, but disables persistent storage for all components:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: no-storage-prod spec: profile: prod defaultVolumeSource: emptyDir: {} The following example uses persistentVolumeClaim to override persistence settings globally for a prod profile, to use 100Gi volumes for all components, instead of the default of 50Gi:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: prod-global-override spec: profile: prod defaultVolumeSource: persistentVolumeClaim: claimName: globalOverride volumeClaimSpecTemplates: - metadata: name: globalOverride spec: resources: requests: storage: 100Gi The following example uses a managed-cluster profile but overrides the persistence settings to use ephemeral storage:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: mgdcluster-empty-storage-example spec: profile: managed-cluster defaultVolumeSource: emptyDir: {} # Use emphemeral storage for all Components unless overridden Customize PersistentVolumeClaim settings for Keycloak using volumeSource The following example Verrazzano configuration enables a 100Gi PersistentVolumeClaim for the MySQL component in Keycloak in a dev profile configuration. This overrides the default of ephemeral storage for Keycloak in that profile, while retaining the default storage settings for other components:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: dev-mysql-storage-example spec: profile: dev components: keycloak: mysql: volumeSource: persistentVolumeClaim: claimName: mysql # Use the \"mysql\" PVC template for the MySQL volume configuration volumeClaimSpecTemplates: - metadata: name: mysql spec: resources: requests: storage: 100Gi Use global and local persistence settings together The following example uses a dev installation profile, but overrides the profile persistence settings to:\n Use 200Gi volumes for all components by default. Use a 100Gi volume for the MySQL instance associated with Keycloak.  apiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: dev-storage-example spec: profile: dev defaultVolumeSource: persistentVolumeClaim: claimName: vmi # set storage globally for the metrics stack components: keycloak: mysql: volumeSource: persistentVolumeClaim: claimName: mysql # set storage separately for keycloak's MySql instance volumeClaimSpecTemplates: - metadata: name: mysql spec: resources: requests: storage: 100Gi - metadata: name: vmi spec: resources: requests: storage: 200Gi ","categories":"","description":"Customize persistent storage settings for Verrazzano","excerpt":"Customize persistent storage settings for Verrazzano","ref":"/docs/setup/customizing/storage/","tags":"","title":"Customize Persistent Storage"},{"body":"","categories":"","description":"Troubleshooting Verrazzano issues","excerpt":"Troubleshooting Verrazzano issues","ref":"/docs/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Enable Google Chrome to accept self-signed Verrazzano certificates There are some installation scenarios where Verrazzano generates SSL certificates that are not trusted by browsers:\n The development (dev) profile installation, which uses its own self-signed CA to issue certificates. Using the Let’s Encrypt Staging authority, which uses untrusted CAs to sign certificates.  These are typical development or testing scenarios, not recommended for production. When accessing Verrazzano sites using these certificates, some browsers like Firefox let you manually accept these certificates. However, Google Chrome now prevents users from being able to accept self-signed certificates by default. This will prevent you from accessing Verrazzano consoles that are using untrusted certificates.\nWhen this occurs, while trying to access Verrazzano services, you will see an error message like the following:\nelasticsearch.vmi.system.default.129.153.98.156.nip.io normally uses encryption to protect your information. When Chrome tried to connect to elasticsearch.vmi.system.default.129.153.98.156.nip.io this time, the website sent back unusual and incorrect credentials You can choose to import the certificate into your local trust chain, but this will have to be done for each Verrazzano instance. From a security perspective, this is not recommended.\nAs an alternative, you can enter a secret passphrase in Chrome to enable it to prompt you to accept these certificates, by doing the following:\n When you see an error such as the one shown previously, when the browser window has the keyboard focus, enter the phrase thisisunsafe. Reload the site. Chrome will prompt you to accept the certificate.  Note: This should be used only when accessing sites that are known to be safe, such as in this situation.\nRelated articles   https://stackoverflow.com/questions/35274659/when-you-use-badidea-or-thisisunsafe-to-bypass-a-chrome-certificate-hsts-err\n  https://miguelpiedrafita.com/chrome-thisisunsafe\n  ","categories":"","description":"Answers to commonly asked questions","excerpt":"Answers to commonly asked questions","ref":"/docs/faq/faq/","tags":"","title":"FAQ"},{"body":"","categories":"","description":"Guides for common tasks with Verrazzano","excerpt":"Guides for common tasks with Verrazzano","ref":"/docs/guides/","tags":"","title":"Guides"},{"body":"Using the Verrazzano Console, you can:\n Find general information about your Verrazzano environment, such as Status and Version. Access the installed Rancher and Keycloak consoles. Access OpenSearch Dashboards, Grafana, Prometheus, and OpenSearch telemetry data. View applications, projects, and Components resources. In a multicluster environment, view clusters. Use the breadcrumbs for quick and easy navigation. Use the Resources navigation to browse child resources associated with the Resource being displayed. Sort (order) and filter data according to certain attributes.  Selecting an application, project, or Component will open a detailed view of the associated child resources. In the central metadata section, you’ll find detailed information about the Verrazzano resource being displayed.\nFor information on how to access the Verrazzano Console, see Get the consoles URLs.\n","categories":"","description":"Use its intuitive user interface to interact with applications and configurations in your Verrazzano environment.","excerpt":"Use its intuitive user interface to interact with applications and configurations in your Verrazzano environment.","ref":"/docs/access/console/","tags":"","title":"Verrazzano Console"},{"body":"","categories":"","description":"Example applications that demonstrate Verrazzano use case scenarios","excerpt":"Example applications that demonstrate Verrazzano use case scenarios","ref":"/docs/samples/","tags":"","title":"Examples"},{"body":"","categories":"","description":"Verrazzano reference information","excerpt":"Verrazzano reference information","ref":"/docs/reference/","tags":"","title":"Reference"},{"body":"v1.2.2 Fixes:\n Resolved an issue with the Grafana Dashboards for Helidon in multicluster setup. Resolved an issue with naming the Istio Authorization Policy for the AuthProxy. Resolved an issue with AuthProxy pods being evicted due to ephemeral storage. Resolved an issue with the length of the cookie TTL in the ingress trait.  v1.2.1 Fixes:\n Resolved an issue with upgrade when configured to use a private registry. Resolved an issue with the public image of WebLogic Monitoring Exporter being used when a private registry is configured. Resolved an issue with intermittent upgrade failures while upgrading from Verrazzano v1.0.2 to v1.2.0. Resolved an issue with the console UI when viewing WebLogic applications. Resolved an issue with the console UI when displaying an application that is targeted to a managed cluster that has not completed the registration process. Resolved an issue with the console UI not displaying the traits for an OAM application. Resolved an issue with the verrazzano-application-operator pod continually crashing and restarting. Resolved an issue with the WebLogic workload logHome value being ignored and always using /scratch/log. Resolved an issue with Prometheus not scraping metrics from Verrazzano managed namespaces that do not have Istio injection enabled. The Verrazzano operators no longer have watches on resources in the kube-system namespace. Updated Keycloak image to address CVEs.  Known Issues:\n Importing a Kubernetes v1.21 cluster into Rancher might not work properly. Rancher does not currently support Kubernetes v1.21.  v1.2.0 Features:\n Logging enhancements:  Added support for Oracle Cloud Infrastructure Logging integration. Replaced Elasticsearch and Kibana with Opensearch and Opensearch Dashboard. Updated Opensearch prod profile data node configuration to 3 replicas. Enhanced Fluentd parsing/filtering rules for Verrazzano system logs.   Added support for using instance_principal authorization with using Oracle Cloud Infrastructure DNS. Added support for metrics integration with non-OAM applications. Added support for scaling Istio gateways and setting affinity. Added support for scaling Verrazzano AuthProxy and setting affinity. Component version updates:  External DNS v0.10.2. MySQL v8.0.28. Grafana v7.5.11. Prometheus v2.31.1. Opensearch v1.2.3 (replaces Elasticsearch). Opensearch Dashboards v1.2.0 (replaces Kibana). WebLogic Kubernetes Operator v3.3.7.    Fixes:\n Fixed Keycloak issue creating incorrect verrazzano-monitors group on installation. Fixed Verrazzano failing to uninstall in a private registry configuration due to a missing Rancher image. Fixed Rancher install when tls-ca-additional secret is not present. Fixed Opensearch parsing errors of trait field. Fixed Custom CA certificates support. Fixed issue requeuing unsupported traits in the Verrazzano Application Operator, and updated the OAM Operator. Aligned Helidon workload service port names with Istio conventions to avoid protocol defaulting to TCP in all cases. Added ability to set a DestinationRule with HTTP Cookie for session affinity.  Known Issues:\n Importing a Kubernetes v1.21 cluster into Rancher might not work properly. Rancher does not currently support Kubernetes v1.21.  v1.1.2 Fixes:\n Fixed installation to create verrazzano-monitors group correctly. Fixed installation to enable network access to Prometheus for Kiali. Updated Spring Boot example image to address CVEs. Updated Kibana image to address CVEs. Updated Elasticsearch image to address CVEs. Fixed Verrazzano failing to install when specifying a custom CA certificate. Updated Keycloak image to address CVEs. Fixed Verrazzano failing to install when the spec.components.certManager.certificate.acme.environment field was set to production in the Verrazzano CR. Added support for using private DNS and instance principals with Oracle Cloud Infrastructure DNS. Fixed Verrazzano failing to uninstall in a private registry configuration due to a missing Rancher image. Updated Verrazzano to use the Rancher v2.5.9 Helm chart.  Known Issues:\n Importing a Kubernetes v1.21 cluster into Rancher might not work properly. Rancher does not currently support Kubernetes v1.21.  v1.1.1 Fixes:\n Elasticsearch and Keycloak images were updated to address CVEs. Updated WebLogic Kubernetes Operator version to 3.3.7. Minor bug fixes including updating Elasticsearch logging to avoid type collisions. Improved cluster-dump behavior when capturing logs. Rancher namespace is now created by default.  Known Issues:\n Importing a Kubernetes v1.21 cluster into Rancher might not work properly. Rancher does not currently support Kubernetes v1.21.  v1.1.0 Fixes:\n Added support for Kiali. Simplified the placement of multicluster resources. Improved the performance of installing Verrazzano. Added support for external Elasticsearch. Improvements to system functions, including the authenticating proxy. Added support in the LoggingTrait to customize application logging. Fixed ability to register a managed cluster with Rancher when configured to use LetsEncrypt staging certificates. Fixed Elasticsearch status yellow due to unassigned shards. Added support for Kubernetes 1.21, dropped support of Kubernetes 1.18. Updated several installed and supported Software Versions.  Known Issues:\n Importing a Kubernetes v1.21 cluster into Rancher might not work properly. Rancher does not currently support Kubernetes v1.21.  v1.0.4 Fixes:\n Elasticsearch and Spring Boot images were updated to consume log4j 2.16, to address CVE-2021-44228/CVE-2021-45046. Keycloak image was updated to address vulnerabilities. Minor bug fixes including fixes for capitalization in user-visible messages.  v1.0.3 Fixes:\n Fix to use load balancer service external IP address for application ingress when using an external load balancer and wildcard DNS. Fixed scraping of Prometheus metrics for WebLogic workloads on managed clusters. Rebuilt several component images to address known issues. Updated to the following versions:  Grafana 6.7.4. WebLogic Kubernetes Operator 3.3.3.    v1.0.2 Fixes:\n Updated CoreDNS to version 1.6.2-1. Updated Keycloak to version 10.0.2. Updated WebLogic Kubernetes Operator to version 3.3.2. Updated Oracle Linux image to version 7.9. Rebuilt several component images to address known issues. Fixes/improvements for the analysis tool, including support for diagnosing load balancer limit reached issues. Fixes/improvements for the install/upgrade process, including:  Install/upgrade jobs now run in the verrazzano-install namespace. Added Rancher registration status to the VerrazzanoManagedCluster status. Updated OKE troubleshooting URL in install log. Fixed ExternalIP handling during Istio install.   Fixed Elasticsearch status yellow due to unassigned_shards. Webhook now disallows multicluster resources that are not in a VerrazzanoProject namespace.  v1.0.1 Fixes:\n Updated to the following versions:  WebLogic Kubernetes Operator v3.3.0. Coherence Operator v3.2.1. In the Analysis Tool, kubectl v1.20.6-2.   Ensured ConfigMaps are deleted during uninstall. Fixed logging pattern match issue for OKE Kubernetes v1.20.8 clusters. Fixed multicluster log collection for Verrazzano installations using LetsEncrypt certificates. Fixed console UI display bugs for multicluster applications. Fixed a bug where API keys generated by the Oracle Cloud Infrastructure Console were not working correctly.  v1.0.0 Features: Updated to Rancher v2.5.9.\nv0.17.0 Features:\n Allow Verrazzano Monitoring Instance (VMI) replicas and memory sizes to be changed during installation for both dev and prod profiles. When installing Verrazzano on OKE, the OKE-specific Fluentd extraVolumeMounts configuration is no longer required. Updated to WebLogic Kubernetes Operator v3.2.5.  Fixes:\n During uninstall, delete application resources only from namespaces which are managed by Verrazzano. During upgrade, honor the APP_OPERATOR_IMAGE override. Fixed Keycloak installation failure when Prometheus is disabled. Allow empty values for Helm overrides in config.json.  v0.16.0 Features:\n Provided options to configure log volume/mount of the log collector, Fluentd, and pre-configured profiles. Automatically enabled metrics and log capture for WebLogic domains deployed in Verrazzano. Added security-related data/project YAML files to the Verrazzano Console, under project details. Updated to WebLogic Kubernetes Operator v3.2.4.  Fixes:\n Added a fix for default metrics traits not always being injected into the appconfig. Updated the timestamp in WebLogic application logs so that the time filter can be used in Kibana. Corrected the incorrect podSelector in the node exporter network policy. Fixed the DNS resolution issue due to the missing cluster section of the coredns configmap. Stability improvements for the platform, tests, and examples. Renamed the Elasticsearch fields in a multicluster registration secret to be consistent.  v0.15.1 Features:\n Allow customization of Elasticsearch node sizes and topology during installation. If runtimeEncryptionSecret, specified in the WebLogic domain spec, does not already exist, then create it. Support overrides of persistent storage configuration for Elasticsearch, Kibana, Prometheus, Grafana, and Keycloak.  Known Issues:\n After upgrade to 0.15.1, for Verrazzano Custom Resource installed on Oracle Cloud Infrastructure Container Engine for Kubernetes (OKE), the Fluentd DaemonSet in the verrazzano-system namespace cannot access logs. Run following command to patch the Fluentd DaemonSet and correct the issue: kubectl patch -n verrazzano-system ds fluentd --patch '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\": \"fluentd\",\"volumeMounts\":[{\"mountPath\":\"/u01/data/\",\"name\":\"extravol0\",\"readOnly\":true}]}],\"volumes\":[{\"hostPath\":{\"path\":\"/u01/data/\",\"type\":\"\"},\"name\":\"extravol0\"}]}}}}'   v0.15.0 Features:\n Support for private container registries. Secured communication between Verrazzano resources using Istio. Updated to the following versions:  cert-manager v1.2.0. Coherence Operator v3.1.5. WebLogic Kubernetes Operator v3.2.3. Node Exporter v1.0.0. NGINX Ingress Controller v0.46. Fluentd v1.12.3.   Added network policies for Istio.  Fixes:\n Stability improvements for the platform, tests, and examples. Several fixes for scraping Prometheus metrics. Several fixes for logging and Elasticsearch. Replaced keycloak.json with dynamic realm creation. Removed the LoggingScope CRD from the Verrazzano API. Fixed issues related to multicluster resources being orphaned.  v0.14.0 Features:\n Multicluster support for Verrazzano. Now you can:  Register participating clusters as VerrazzanoManagedClusters. Deploy MutiClusterComponents and MultiClusterApplicationConfigurations. Organize multicluster namespaces as VerrazzanoProjects. Access MultiCluster Components and ApplicationConfigurations in the Verrazzano Console UI.   Changed default wildcard DNS from xip.io to nip.io. Support for OKE clusters with private endpoints. Support for network policies. Now you can:  Add ingress-NGINX network policies. Add Rancher network policies. Add NetworkPolicy support to Verrazzano projects. Add network policies for Keycloak. Add platform operator network policies. Add network policies for Elasticsearch and Kibana. Set network policies for Verrazzano operators, Console, and API proxy. Add network policies for WebLogic Kubernetes Operator.   Changes to allow magic DNS provider to be specified (xip.io, nip.io, sslip.io). Support service setup for multiple containers. Enabled use of self-signed certs with Oracle Cloud Infrastructure DNS. Support for setting DeploymentStrategy for VerrazzanoHelidonWorkload.  Fixes:\n Several stability improvements for the platform, tests, and examples. Added retries around lookup of Rancher admin user. Granted specific privileges instead of ALL for Keycloak user in MySQL. Disabled the installation of the Verrazzano Console UI on managed clusters.  v0.13.0 Features:\n IngressTrait support for explicit destination host and port. Experimental cluster diagnostic tooling. Grafana dashboards for VerrazzanoHelidonWorkload. Now you can update application Fluentd sidecar images following a Verrazzano update. Documented Verrazzano specific OAM workload resources. Documented Verrazzano hardware requirements and installed software versions.  Fixes:\n VerrazzanoWebLogicWorkload and VerrazzanoCoherenceWorkload resources now handle updates. Now VerrazzanoHelidonWorkload supports the use of the ManualScalarTrait. Now you can delete a Namespace containing an ApplicationConfiguration resource. Fixed frequent restarts of Prometheus during application deployment. Made verrazzano-application-operator logging more useful and use structured logging. Fixed Verrazzano uninstall issues.  v0.12.0 Features:\n Observability stack now uses Keycloak SSO for authentication. Istio sidecars now automatically injected when namespaces labeled istio-injection=enabled. Support for Helidon applications now defined using VerrazzanoHelidonWorkload type.  Fixes:\n Fixed issues where logs were not captured from all containers in workloads with multiple containers. Fixed issue where some resources were not cleaned up during uninstall.  v0.11.0 Features:\n OAM applications are optionally deployed into an Istio service mesh. Incremental improvements to user-facing roles.  Fixes:\n Fixed issue with logging when an application has multiple workload types. Fixed metrics configuration in Spring Boot example application.  v0.10.0 Breaking Changes:\n Model/binding files removed; now application deployment done exclusively by using Open Application Model (OAM). Syntax changes for WebLogic and Coherence OAM workloads, now defined using VerrazzanoCoherenceWorkload and VerrazzanoWebLogicWorkload types.  Features:\n By default, application endpoints now use HTTPs - when using magic DNS, certificates are issued by cluster issuer, when using Oracle Cloud Infrastructure DNS certificates are issued using Let’s Encrypt, or the end user can provide certificates. Updated to Coherence Operator v3.1.3. Updates for running Verrazzano on Kubernetes 1.19 and 1.20. RBAC roles and role bindings created at install time. Added instance information to status of Verrazzano custom resource; can be used to obtain instance URLs. Updated to Istio v1.7.3.  Fixes:\n Reduced log level of Elasticsearch; excessive logging could have resulted in filling up disks.  v0.9.0  Features:  Added platform support for installing Verrazzano on Kind clusters. Log records are indexed from the OAM appconfig and component definitions using the following pattern: namespace-appconfig-component. All system and curated components are now patchable. More updates to Open Application Model (OAM) support.    To enable OAM, when you install Verrazzano, specify the following in the Kubernetes manifest file for the Verrazzano custom resource:\nspec: oam: enabled: true v0.8.0  Features:  Support for two installation profiles, development (dev) and production (prod). The production profile, which is the default, provides a 3-node Elasticsearch and persistent storage for the Verrazzano Monitoring Instance (VMI). The development profile provides a single node Elasticsearch and no persistent storage for the VMI. The default behavior has been changed to use the system VMI for all monitoring (applications and Verrazzano components). It is still possible to customize one of the profiles to enable the original, non-shared VMI mode. Initial support for the Open Application Model (OAM).   Fixes:  Updated to Axios NPM package v0.21.1 to resolve a security vulnerability in the examples code.    v.0.7.0   Features:\n Ability to upgrade an existing Verrazzano installation. Added the Verrazzano Console. Enhanced the structure of the Verrazzano custom resource to allow more configurability. Streamlined the secret usage for Oracle Cloud Infrastructure DNS installations.    Fixes:\n Fixed bug where the Verrazzano CR Certificate.CA fields were being ignored. Removed secret used for hello-world; hello-world-application image is now public in ghcr so ImagePullSecrets is no longer needed. Fixed issue #339 (PRs #208 \u0026 #210.)    v0.6.0  Features:  In-cluster installer which replaces client-side install scripts. Added installation profiles; in this release, there are two: production and development. Verrazzano system components now emit JSON structured logs.   Fixes:  Updated Elasticsearch and Kibana versions (elasticsearch:7.6.1-20201130145440-5c76ab1) and (kibana:7.6.1-20201130145840-7717e73).    ","categories":"","description":"","excerpt":"v1.2.2 Fixes:\n Resolved an issue with the Grafana Dashboards for Helidon in multicluster setup. Resolved an issue with naming the Istio Authorization Policy for the AuthProxy. Resolved an issue with …","ref":"/docs/releasenotes/","tags":"","title":"Release Notes"},{"body":"Verrazzano is an end-to-end enterprise container platform for deploying cloud-native and traditional applications in multicloud and hybrid environments. It is made up of a curated set of open source components – many that you may already use and trust, and some that were written specifically to pull together all of the pieces that make Verrazzano a cohesive and easy to use platform.\nVerrazzano includes the following capabilities:\n Hybrid and multicluster workload management Special handling for WebLogic, Coherence, and Helidon applications Multicluster infrastructure management Integrated and pre-wired application monitoring Integrated security DevOps and GitOps enablement  Select Quick Start to get started.\nVerrazzano release versions and source code are available at https://github.com/verrazzano/verrazzano. This repository contains a Kubernetes operator for installing Verrazzano and example applications for use with Verrazzano. Find the Verrazzano release history here.\nFor documentation from all releases, use the Documentation version menu at the top of this page. For developers’ perspectives, check us out on Medium.\n","categories":"","description":"","excerpt":"Verrazzano is an end-to-end enterprise container platform for deploying cloud-native and traditional applications in multicloud and hybrid environments. It is made up of a curated set of open source …","ref":"/docs/","tags":"","title":"Welcome to Verrazzano"},{"body":"Hello Config World Helidon This example is a Helidon-based service that returns a “HelloConfig World” response when invoked. The application configuration uses a Kubernetes ConfigMap, instead of the default, microprofile properties file.\nBefore you begin Install Verrazzano by following the installation instructions.\nNOTE: The Hello World Helidon configuration example application deployment files are contained in the Verrazzano project located at \u003cVERRAZZANO_HOME\u003e/examples/helidon-config, where \u003cVERRAZZANO_HOME\u003e is the root of the Verrazzano project.\nDeploy the Hello Config World Helidon application NOTE To run this application in the default namespace:\n$kubectl label namespace default verrazzano-managed=true istio-injection=enabled If you chose the default namespace, you can skip Step 1. and ignore the -n option in the rest of the commands.\n   Create a namespace for the application and add a label identifying the namespace as managed by Verrazzano.\n$ kubectl create namespace helidon-config $ kubectl label namespace helidon-config verrazzano-managed=true istio-injection=enabled   To deploy the application, apply the helidon-config OAM resources.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/helidon-config/helidon-config-comp.yaml -n helidon-config $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/helidon-config/helidon-config-app.yaml -n helidon-config   Wait for the application to be ready.\n$ kubectl wait \\ --for=condition=Ready pods \\ --all -n helidon-config \\ --timeout=300s   Explore the application The Hello World Helidon configuration example implements a REST API endpoint, /config, which returns a message {\"message\":\"HelloConfig World!\"} when invoked.\nNOTE: The following instructions assume that you are using a Kubernetes environment such as OKE. Other environments or deployments may require alternative mechanisms for retrieving addresses, ports, and such.\nFollow these steps to test the endpoints:\n  Get the generated host name for the application.\n$ HOST=$(kubectl get gateways.networking.istio.io helidon-config-helidon-config-appconf-gw \\ -n helidon-config \\ -o jsonpath={.spec.servers[0].hosts[0]}) $ echo $HOST # Sample output helidon-config-appconf.helidon-config.11.22.33.44.nip.io   Get the EXTERNAL_IP address of the istio-ingressgateway service.\n$ ADDRESS=$(kubectl get service \\ -n istio-system istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo $ADDRESS # Sample output 11.22.33.44   Access the application:\n  Using the command line\n$ curl -sk \\ -X GET \\ https://${HOST}/config \\ --resolve ${HOST}:443:${ADDRESS} # Expected response output {\"message\":\"HelloConfig World!\"} If you are using nip.io, then you do not need to include --resolve.\n  Local testing with a browser\nTemporarily, modify the /etc/hosts file (on Mac or Linux) or c:\\Windows\\System32\\Drivers\\etc\\hosts file (on Windows 10), to add an entry mapping the host name to the ingress gateway’s EXTERNAL-IP address. For example:\n11.22.33.44 helidon-config.example.com Then you can access the application in a browser at https://\u003chost\u003e/config.\nIf you are using nip.io, then you can access the application in a browser using the HOST variable (for example, https://${HOST}/config). If you are going through a proxy, then you may need to add *.nip.io to the NO_PROXY list.\n  Using your own DNS name\n Point your own DNS name to the ingress gateway’s EXTERNAL-IP address. In this case, you would need to edit the helidon-config-app.yaml file to use the appropriate value under the hosts section (such as yourhost.your.domain), before deploying the helidon-config application. Then, you can use a browser to access the application at https://\u003cyourhost.your.domain\u003e/config.      A variety of endpoints associated with the deployed application, are available to further explore the logs, metrics, and such. You can access them according to the directions here.\n  Troubleshooting   Verify that the application configuration, domain, and ingress trait all exist.\n$ kubectl get ApplicationConfiguration -n helidon-config $ kubectl get IngressTrait -n helidon-config   Verify that the helidon-config service pods are successfully created and transition to the READY state. Note that this may take a few minutes and that you may see some of the services terminate and restart.\n $ kubectl get pods -n helidon-config # Sample output NAME READY STATUS RESTARTS AGE helidon-config-deployment-676d97c7d4-wkrj2 3/3 Running 0 5m39s   Undeploy the application   To undeploy the application, delete the Hello Config World Helidon OAM resources.\n$ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/helidon-config/helidon-config-app.yaml -n helidon-config $ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/helidon-config/helidon-config-comp.yaml -n helidon-config   Delete the namespace helidon-config after the application pod is terminated.\n$ kubectl delete namespace helidon-config   ","categories":"","description":"","excerpt":"Hello Config World Helidon This example is a Helidon-based service that returns a “HelloConfig World” response when invoked. The application configuration uses a Kubernetes ConfigMap, instead of the …","ref":"/docs/samples/helidon-config/","tags":"","title":""},{"body":"Hello World Helidon This example is a Helidon-based service that returns a “Hello World” response when invoked. The application configuration uses the default, microprofile properties file.\nBefore you begin Install Verrazzano by following the installation instructions.\nNOTE: The Hello World Helidon example application deployment files are contained in the Verrazzano project located at \u003cVERRAZZANO_HOME\u003e/examples/hello-helidon, where \u003cVERRAZZANO_HOME\u003e is the root of the Verrazzano project.\nDeploy the Hello World Helidon application NOTE To run this application in the default namespace:\n$kubectl label namespace default verrazzano-managed=true istio-injection=enabled If you chose the default namespace, you can skip Step 1. and ignore the -n option in the rest of the commands.\n   To run the application in a namespace other than default namespace, create a namespace for the application and add a label identifying the namespace as managed by Verrazzano.\n$ kubectl create namespace hello-helidon $ kubectl label namespace hello-helidon verrazzano-managed=true istio-injection=enabled   To deploy the application, apply the hello-helidon OAM resources.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/hello-helidon/hello-helidon-comp.yaml -n hello-helidon $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/hello-helidon/hello-helidon-app.yaml -n hello-helidon   Wait for the application to be ready.\n$ kubectl wait \\ --for=condition=Ready pods \\ --all \\ -n hello-helidon \\ --timeout=300s   Explore the application The Hello World Helidon microservices application implements a REST API endpoint, /greet, which returns a message {\"message\":\"Hello World!\"} when invoked.\nNOTE: The following instructions assume that you are using a Kubernetes environment such as OKE. Other environments or deployments may require alternative mechanisms for retrieving addresses, ports, and such.\nFollow these steps to test the endpoints:\n  Get the generated host name for the application.\n$ HOST=$(kubectl get gateways.networking.istio.io hello-helidon-hello-helidon-appconf-gw \\ -n hello-helidon \\ -o jsonpath='{.spec.servers[0].hosts[0]}') $ echo $HOST # Sample output hello-helidon-appconf.hello-helidon.11.22.33.44.nip.io   Get the EXTERNAL_IP address of the istio-ingressgateway service.\n$ ADDRESS=$(kubectl get service \\ -n istio-system istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo $ADDRESS # Sample output 11.22.33.44   Access the application:\n  Using the command line\n$ curl -sk \\ -X GET \\ https://${HOST}/greet \\ --resolve ${HOST}:443:${ADDRESS} # Expected response output {\"message\":\"Hello World!\"} If you are using nip.io, then you do not need to include --resolve.\n  Local testing with a browser\nTemporarily, modify the /etc/hosts file (on Mac or Linux) or c:\\Windows\\System32\\Drivers\\etc\\hosts file (on Windows 10), to add an entry mapping the host name to the ingress gateway’s EXTERNAL-IP address. For example:\n11.22.33.44 hello-helidon.example.com Then you can access the application in a browser at https://\u003chost\u003e/greet.\nIf you are using nip.io, then you can access the application in a browser using the HOST variable (for example, https://${HOST}/greet). If you are going through a proxy, then you may need to add *.nip.io to the NO_PROXY list.\n  Using your own DNS name\n Point your own DNS name to the ingress gateway’s EXTERNAL-IP address. In this case, you would need to edit the hello-helidon-app.yaml file to use the appropriate value under the hosts section (such as yourhost.your.domain), before deploying the hello-helidon application. Then, you can use a browser to access the application at https://\u003cyourhost.your.domain\u003e/greet.      A variety of endpoints associated with the deployed application, are available to further explore the logs, metrics, and such. You can access them according to the directions here.\n  Troubleshooting   Verify that the application configuration, domain, and ingress trait all exist.\n$ kubectl get ApplicationConfiguration -n hello-helidon $ kubectl get IngressTrait -n hello-helidon   Verify that the hello-helidon service pods are successfully created and transition to the READY state. Note that this may take a few minutes and that you may see some of the services terminate and restart.\n $ kubectl get pods -n hello-helidon # Sample output NAME READY STATUS RESTARTS AGE hello-helidon-workload-676d97c7d4-wkrj2 2/2 Running 0 5m39s   Undeploy the application   To undeploy the application, delete the Hello World Helidon OAM resources.\n$ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/hello-helidon/hello-helidon-app.yaml -n hello-helidon $ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.2.2/examples/hello-helidon/hello-helidon-comp.yaml -n hello-helidon   Delete the namespace hello-helidon after the application pod is terminated.\n$ kubectl delete namespace hello-helidon   ","categories":"","description":"","excerpt":"Hello World Helidon This example is a Helidon-based service that returns a “Hello World” response when invoked. The application configuration uses the default, microprofile properties file.\nBefore you …","ref":"/docs/samples/hello-helidon/","tags":"","title":""},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"},{"body":" Verrazzano Enterprise Container Platform A hybrid multicloud Kubernetes-based Enterprise Container Platform for running both cloud-native and traditional applications.\nLearn More  View Repository   --  #td-cover-block-1 { background-image: url(./featured-background_hu782cb335b85f9f9418d05341676d005b_196571_960x540_fill_catmullrom_bottom_3.png); } @media only screen and (min-width: 1200px) { #td-cover-block-1 { background-image: url(./featured-background_hu782cb335b85f9f9418d05341676d005b_196571_1920x1080_fill_catmullrom_bottom_3.png); } }  Verrazzano Enterprise Container Platform Learn More  View Repository   A hybrid multicloud Kubernetes-based Enterprise Container Platform for running both cloud-native and traditional applications.\n          Application Lifecycle Management Use Open Application Model constructs to describe, deploy, and update multicomponent application systems across clusters in multiple clouds, including clusters on premises.\n     Integrated Monitoring Verrazzano can provision a full monitoring stack for your application, including OpenSearch, OpenSearch Dashboards, Prometheus, and Grafana. Your applications are automatically wired up to send logs and metrics into the monitoring tools.\n     Security Built in security with encryption, certificate management, authentication and authorization services, and network policies.\n         Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\n Read more …\n    Connect with us on Slack! For project discussions.\n Read more …\n    Follow us on Twitter! For announcement of latest features etc.\n Read more …\n     ","categories":"","description":"","excerpt":" Verrazzano Enterprise Container Platform A hybrid multicloud Kubernetes-based Enterprise Container Platform for running both cloud-native and traditional applications.\nLearn More  View Repository …","ref":"/","tags":"","title":"Verrazzano Enterprise Container Platform"}]