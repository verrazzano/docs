<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Verrazzano Enterprise Container Platform â€“ Troubleshooting</title>
    <link>/docs/troubleshooting/</link>
    <description>Recent content in Troubleshooting on Verrazzano Enterprise Container Platform</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="/docs/troubleshooting/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Application Deployment</title>
      <link>/docs/troubleshooting/troubleshooting-application-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/troubleshooting/troubleshooting-application-deployment/</guid>
      <description>
        
        
        &lt;p&gt;During application deployment, the &lt;code&gt;oam-kubernetes-runtime&lt;/code&gt; and &lt;code&gt;verrazzano-application-operator&lt;/code&gt; cooperate through the generation and update of Kubernetes resources.
The &lt;code&gt;oam-kubernetes-runtime&lt;/code&gt; processes the ApplicationConfiguration and Component resources provided by the user and generates workload and Trait resources.
The &lt;code&gt;verrazzano-application-operator&lt;/code&gt; processes Verrazzano specific workload and Trait resources.
These are then used to generate additional child and related resources.&lt;/p&gt;
&lt;p&gt;Troubleshooting application deployments should follow three general steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Review the status of the &lt;code&gt;oam-kubernetes-runtime&lt;/code&gt; and &lt;code&gt;verrazzano-application-operator&lt;/code&gt; operator pods.&lt;/li&gt;
&lt;li&gt;Review the logs of the &lt;code&gt;oam-kubernetes-runtime&lt;/code&gt; and &lt;code&gt;verrazzano-application-operator&lt;/code&gt; operator pods.&lt;/li&gt;
&lt;li&gt;Review the resources generated by the &lt;code&gt;oam-kubernetes-runtime&lt;/code&gt; and the &lt;code&gt;verrazzano-application-operator&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;review-oam-kubernetes-runtime-operator-status&#34;&gt;Review &lt;code&gt;oam-kubernetes-runtime&lt;/code&gt; operator status&lt;/h3&gt;
&lt;p&gt;For application deployment to succeed, the &lt;code&gt;oam-kubernetes-runtime&lt;/code&gt; pod must have a status of &lt;code&gt;Running&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Use the following command to get the pod status:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl get pods \
    -n verrazzano-system \
    -l app.kubernetes.io/name=oam-kubernetes-runtime
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If the pod status is not &lt;code&gt;Running&lt;/code&gt;, then see the instructions for &lt;a href=&#34;#review-oam-kubernetes-runtime-operator-logs&#34;&gt;reviewing the &lt;code&gt;oam-kubernetes-runtime&lt;/code&gt;&lt;/a&gt; pod logs.&lt;/p&gt;
&lt;h3 id=&#34;review-verrazzano-application-operator-operator-status&#34;&gt;Review &lt;code&gt;verrazzano-application-operator&lt;/code&gt; operator status&lt;/h3&gt;
&lt;p&gt;For application deployment to succeed, the &lt;code&gt;verrazzano-application-operator&lt;/code&gt; pod must have a status of &lt;code&gt;Running&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Use the following command to get the pod status:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl get pods \
    -n verrazzano-system \
    -l app=verrazzano-application-operator
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If the pod status is not &lt;code&gt;Running&lt;/code&gt;, then see the instructions for &lt;a href=&#34;#review-verrazzano-application-operator-logs&#34;&gt;reviewing the &lt;code&gt;verrazzano-application-operator&lt;/code&gt;&lt;/a&gt; logs.&lt;/p&gt;
&lt;h3 id=&#34;review-oam-kubernetes-runtime-operator-logs&#34;&gt;Review &lt;code&gt;oam-kubernetes-runtime&lt;/code&gt; operator logs&lt;/h3&gt;
&lt;p&gt;Review the &lt;code&gt;oam-kubernetes-runtime&lt;/code&gt; pod logs for any indication that pod startup or the generation of workloads or traits has failed.&lt;/p&gt;
&lt;p&gt;Use the following command to get the logs:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl logs \
    -n verrazzano-system \
    -l app.kubernetes.io/name=oam-kubernetes-runtime
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;review-verrazzano-application-operator-logs&#34;&gt;Review &lt;code&gt;verrazzano-application-operator&lt;/code&gt; logs&lt;/h3&gt;
&lt;p&gt;Review the &lt;code&gt;verrazzano-application-operator&lt;/code&gt; logs for any indication that pod startup or resource generation has failed.&lt;/p&gt;
&lt;p&gt;Use the following command to get the logs:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl logs \
    -n verrazzano-system \
    -l app=verrazzano-application-operator
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;review-generated-workload-resources&#34;&gt;Review generated workload resources&lt;/h3&gt;
&lt;p&gt;The processing of a Component reference within an ApplicationConfiguration results in the generation of workloads.
For example, a referenced Component might result in the generation of a VerrazzanoHelidonWorkload workload resource.
In turn, the VerrazzanoHelidonWorkload workload resource will be processed and result in the generation of related Deployment and Service resources.&lt;/p&gt;
&lt;p&gt;If the expected workload resource, for example VerrazzanoHelidonWorkload, is missing, then review the &lt;code&gt;oam-kubernetes-runtime&lt;/code&gt; logs.
If the expected related resources, for example Deployment or Service, are missing, then review the &lt;code&gt;verrazzano-application-operator&lt;/code&gt; logs.&lt;/p&gt;
&lt;p&gt;The following commands are examples of checking for the resources related to a VerrazzanoHelidonWorkload deployment:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl get -n hello-helidon verrazzanohelidonworkload hello-helidon-workload
$ kubectl get -n hello-helidon deployment hello-helidon-deployment
$ kubectl get -n hello-helidon service hello-helidon-deployment
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;review-generated-trait-resources&#34;&gt;Review generated Trait resources&lt;/h3&gt;
&lt;p&gt;The processing of traits embedded with an ApplicationConfiguration results in the generation of Trait resources.
For example, an IngressTrait embedded within an ApplicationConfiguration will result in the generation of an IngressTrait resource.
In turn, the IngressTrait resource will be processed and result in the generation of related Certificate, Gateway, and VirtualService resources.&lt;/p&gt;
&lt;p&gt;If the expected Trait resource, for example IngressTrait, is missing, then review the &lt;code&gt;oam-kubernetes-runtime&lt;/code&gt; logs.
If the expected related resources, for example Certificate, Gateway, and VirtualService, are missing, then review the &lt;code&gt;verrazzano-application-operator&lt;/code&gt; logs.&lt;/p&gt;
&lt;p&gt;The following commands are examples of checking for the resources related to an IngressTrait:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl get -n hello-helidon ingresstrait hello-helidon-ingress
$ kubectl get -n istio-system Certificate hello-helidon-hello-helidon-appconf-cert
$ kubectl get -n hello-helidon gateway hello-helidon-hello-helidon-gw
$ kubectl get -n hello-helidon virtualservice hello-helidon-ingress-rule-0-vs
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;check-for-rbac-privilege-issues&#34;&gt;Check for RBAC privilege issues&lt;/h3&gt;
&lt;p&gt;The use of generic Kubernetes resources as workloads and traits can result in deployment failures if privileges are insufficient.
In this case, the &lt;code&gt;oam-kubernetes-runtime&lt;/code&gt; logs will contain errors containing the term &lt;code&gt;forbidden&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The following command shows how to query for this type of failure message:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl logs \
    -n verrazzano-system \
    -l app.kubernetes.io/name=oam-kubernetes-runtime | grep forbidden
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;check-resource-owners&#34;&gt;Check resource owners&lt;/h3&gt;
&lt;p&gt;Kubernetes maintains the child to parent relationship within metadata fields.&lt;/p&gt;
&lt;p&gt;The following example returns the parent of the IngressTrait, named &lt;code&gt;hello-helidon-ingress&lt;/code&gt;, in the &lt;code&gt;hello-helidon&lt;/code&gt; namespace:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl get IngressTrait \
    -n hello-helidon hello-helidon-ingress \
    -o jsonpath=&amp;#39;{range .metadata.ownerReferences[*]}{.name}{&amp;#34;\n&amp;#34;}{end}&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The results of this command can help identify the lineage of a given resource.&lt;/p&gt;
&lt;h3 id=&#34;check-related-resources&#34;&gt;Check related resources&lt;/h3&gt;
&lt;p&gt;Some resources also record the related resources affected during their processing.
For example, when processed, an IngressTrait will create related Gateway, VirtualService, and Certificate resources.&lt;/p&gt;
&lt;p&gt;The following command is an example of how to obtain the related resources of an IngressTraits:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl get IngressTrait \
    -n hello-helidon hello-helidon-ingress \
    -o jsonpath=&amp;#39;{range .status.resources[*]}{.kind}: {.name}{&amp;#34;\n&amp;#34;}{end}&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The results of this command can help identify which other resources, the given resource affected.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Diagnostic Tools</title>
      <link>/docs/troubleshooting/diagnostictools/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/troubleshooting/diagnostictools/</guid>
      <description>
        
        
        
      </description>
    </item>
    
    <item>
      <title>Docs: Multicluster Verrazzano</title>
      <link>/docs/troubleshooting/troubleshooting-multicluster/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/troubleshooting/troubleshooting-multicluster/</guid>
      <description>
        
        
        &lt;p&gt;This document describes some common problems you might encounter when using multicluster Verrazzano, and how to troubleshoot them.&lt;/p&gt;
&lt;p&gt;If you created multicluster resources in the admin cluster, and specified a &lt;code&gt;placement&lt;/code&gt; value in a managed cluster,
then those resources will get created in that managed cluster. If they do not get created in the managed cluster, then
use the following steps to troubleshoot:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that the managed cluster is registered correctly and can connect to the admin cluster.&lt;/li&gt;
&lt;li&gt;Verify that the VerrazzanoProject for the resource&amp;rsquo;s namespace, also has a &lt;code&gt;placement&lt;/code&gt; in that managed cluster.&lt;/li&gt;
&lt;li&gt;Check the multicluster resource&amp;rsquo;s status field on the admin cluster to know what the status of that resource is
on each managed cluster to which it is targeted.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you update the &lt;a href=&#34;../../docs/customize/dns/&#34;&gt;DNS&lt;/a&gt; of the admin cluster and notice that the
managed cluster status is unavailable in the Rancher console, along with the error &lt;code&gt;x509: certificate is valid for &amp;lt;rancher new url&amp;gt;, not &amp;lt;rancher old url&amp;gt;&lt;/code&gt; seen in the &lt;code&gt;cattle-cluster-agent&lt;/code&gt; (Rancher Agent) logs on the
managed cluster, then re-register the managed cluster, as described &lt;a href=&#34;#re-register-the-managed-cluster&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;verify-managed-cluster-registration-and-connectivity&#34;&gt;Verify managed cluster registration and connectivity&lt;/h2&gt;
&lt;p&gt;You can verify that a managed cluster was successfully registered with an admin cluster by viewing the
corresponding VerrazzanoManagedCluster (VMC) resource on the admin cluster. For example, to verify that a managed cluster
named &lt;code&gt;managed1&lt;/code&gt; was successfully registered:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# on the admin cluster
$ kubectl get verrazzanomanagedcluster managed1 \
    -n verrazzano-mc \
    -o yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Partial sample output from the previous command.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  status:
    conditions:
    - lastTransitionTime: &amp;#34;2021-06-22T21:03:27Z&amp;#34;
      message: Ready
      status: &amp;#34;True&amp;#34;
      type: Ready
    lastAgentConnectTime: &amp;#34;2021-06-22T21:06:04Z&amp;#34;
    ... other fields ...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Check the &lt;code&gt;lastAgentConnectTime&lt;/code&gt; in the status of the VMC resource. This is the last time at which the
managed cluster connected to the admin cluster. If this value is not present, or is not recent (within the last
three minutes), then the managed cluster named &lt;code&gt;managed1&lt;/code&gt; cannot successfully connect to the admin cluster.
This could be due to several reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The managed cluster registration process step of applying the registration YAML on the managed cluster,
was not completed. For the complete setup instructions, see &lt;a href=&#34;../../docs/setup/install/multicluster/#register-the-managed-cluster&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The managed cluster does not have network connectivity to the admin cluster. The managed cluster will attempt to
connect to the admin cluster at regular intervals, and any errors will be reported in the
&lt;code&gt;verrazzano-application-operator&lt;/code&gt; pod&amp;rsquo;s log on the &lt;em&gt;managed&lt;/em&gt; cluster. View the logs using the following command:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# on the managed cluster
$ kubectl logs \
    -n verrazzano-system \
    -l app=verrazzano-application-operator
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If these logs reveal that there is a connectivity issue, then in the case of an installation that includes Rancher on
the admin cluster, there may have been a problem with Verrazzano pushing registration details or updates to the managed
cluster. Try exporting and applying the registration manifest to the managed cluster as shown:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# on the admin cluster
       kubectl get secret \
       -n verrazzano-mc verrazzano-cluster-managed1-manifest \
       -o jsonpath={.data.yaml} | base64 --decode &amp;gt; register.yaml

# on the managed cluster
       kubectl apply -f register.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: If your installation disabled Rancher on the admin cluster, then check the admin cluster Kubernetes server
address that you provided during registration and ensure that it is correct, and that it is reachable from the managed
cluster. If it is incorrect, then you will need to repeat the managed cluster registration process described in the setup instructions
&lt;a href=&#34;../../docs/setup/install/multicluster/#register-the-managed-cluster&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;verify-verrazzanoproject-placement&#34;&gt;Verify VerrazzanoProject placement&lt;/h2&gt;
&lt;p&gt;For Verrazzano to create an application namespace in a managed cluster, that namespace must be part of a VerrazzanoProject
that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Includes that namespace.&lt;/li&gt;
&lt;li&gt;Has a &lt;code&gt;placement&lt;/code&gt; value that includes that managed cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;View the details of the project that corresponds to your application&amp;rsquo;s namespace. In the example command that follows, the
project name is assumed to be &lt;code&gt;myproject&lt;/code&gt;. All projects are expected to be created in the &lt;code&gt;verrazzano-mc&lt;/code&gt; namespace.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# on the admin cluster
$ kubectl get verrazzanoproject myproject \
    -n verrazzano-mc \
    -o yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The following partial sample output is for a project that will result in the namespace &lt;code&gt;mynamespace&lt;/code&gt; being created on the managed
cluster &lt;code&gt;managed1&lt;/code&gt;.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;spec:
  placement:
    clusters:
    - name: managed1
  template:
    namespaces:
    - metadata:
        name: mynamespace
....other fields....
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;check-the-multicluster-resource-status&#34;&gt;Check the multicluster resource status&lt;/h2&gt;
&lt;p&gt;On the admin cluster, each multicluster resource&amp;rsquo;s status field is updated with the status of the underlying resource
on each managed cluster in which it is placed.&lt;/p&gt;
&lt;p&gt;The following example command shows how to view the status of a MultiClusterApplicationConfiguration named &lt;code&gt;myapp&lt;/code&gt;, in
the namespace &lt;code&gt;mynamespace&lt;/code&gt;, that has a &lt;code&gt;placement&lt;/code&gt; value that includes the managed cluster &lt;code&gt;managed1&lt;/code&gt;.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl get multiclusterapplicationconfiguration myapp \
    -n mynamespace \
    -o yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The status of the underlying resource in each cluster specified in the placement is shown in the following partial sample
output.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  status:
    clusters:
    - lastUpdateTime: &amp;#34;2021-06-22T21:05:04Z&amp;#34;
      message: OAM Application Configuration created
      name: managed1
      state: Succeeded
    conditions:
    - lastTransitionTime: &amp;#34;2021-06-22T21:03:58Z&amp;#34;
      message: OAM Application Configuration created
      status: &amp;#34;True&amp;#34;
      type: DeployComplete
    state: Succeeded
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The status message contains additional information on the operation&amp;rsquo;s success or failure.&lt;/p&gt;
&lt;h2 id=&#34;re-register-the-managed-cluster&#34;&gt;Re-register the managed cluster&lt;/h2&gt;
&lt;p&gt;Perform the following steps to re-register the managed cluster with the admin cluster. The cluster against which to run
the command is indicated in each code block.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;On the admin cluster, export the register YAML file newly created on the admin cluster to re-register the
managed cluster.
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# On the admin cluster
$ kubectl --kubeconfig $KUBECONFIG_ADMIN --context $KUBECONTEXT_ADMIN \
    get secret verrazzano-cluster-managed1-manifest \
    -n verrazzano-mc \
    -o jsonpath={.data.yaml} | base64 --decode &amp;gt; register_new.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;On the managed cluster, apply the registration file exported in the previous step.
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# On the managed cluster
$ kubectl --kubeconfig $KUBECONFIG_MANAGED1 --context $KUBECONTEXT_MANAGED1 \
    apply -f register_new.yaml

# After the command succeeds, you may delete the register_new.yaml file
$ rm register_new.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;On the admin cluster, run &lt;code&gt;kubectl patch clusters.management.cattle.io&lt;/code&gt; to trigger redeployment of the Rancher agent
on the managed cluster.
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# On the admin cluster
$ kubectl --kubeconfig $KUBECONFIG_ADMIN --context $KUBECONTEXT_ADMIN \
    get clusters.management.cattle.io

# Sample output
NAME      AGE
c-mzb2h   4h48m
local     4h56m

$ kubectl --kubeconfig $KUBECONFIG_ADMIN --context $KUBECONTEXT_ADMIN \
    patch clusters.management.cattle.io &amp;lt;the managed cluster name from the above output&amp;gt; \
    -p &amp;#39;{&amp;#34;status&amp;#34;:{&amp;#34;agentImage&amp;#34;:&amp;#34;dummy&amp;#34;}}&amp;#39; --type merge

# Sample output
cluster.management.cattle.io/c-mzb2h patched
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: MySQL</title>
      <link>/docs/troubleshooting/troubleshooting-mysql/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/troubleshooting/troubleshooting-mysql/</guid>
      <description>
        
        
        &lt;p&gt;There are known issues that can occur with MySQL.  The Verrazzano platform operator will automatically detect each of the described issues and perform actions to repair them.  The operator initiates a repair within a few minutes of detecting an issue.&lt;/p&gt;
&lt;p&gt;The following sections are provided in the event that a manual repair of an issue is required.&lt;/p&gt;
&lt;h3 id=&#34;mysql-pod-stuck-terminating&#34;&gt;MySQL pod stuck &lt;code&gt;Terminating&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;A MySQL pod may get stuck in a terminating state.  This may occur while upgrading the nodes of a Kubernetes cluster.&lt;/p&gt;
&lt;p&gt;Here is an example of what this condition looks like.  All the pod containers are terminated, but the pod never finishes terminating.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl get pods -n keycloak -l component=mysqld
NAME      READY   STATUS        RESTARTS   AGE
mysql-0   0/3     Terminating   0          60m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can repair this issue by restarting the &lt;code&gt;mysql-operator&lt;/code&gt; pod.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl delete pod -l name=mysql-operator -n mysql-operator
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;mysql-pod-waiting-for-readiness-gates&#34;&gt;MySQL pod waiting for readiness gates&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;mysql&lt;/code&gt; StatefulSet may get stuck while waiting to reach the ready state.  This will occur when one or more MySQL pods not meeting its set of &lt;code&gt;ReadinessGates&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here is an example of what this condition looks like.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl describe pods -n keycloak -l component=mysqld

# Excerpt from the command output
Readiness Gates:
  Type                          Status
  mysql.oracle.com/configured   False 
  mysql.oracle.com/ready        True 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can repair this issue by restarting the &lt;code&gt;mysql-operator&lt;/code&gt; pod.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl delete pod -l name=mysql-operator -n mysql-operator
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;mysql-router-pod-in-crashloopbackoff-state&#34;&gt;MySQL router pod in &lt;code&gt;CrashLoopBackOff&lt;/code&gt; state&lt;/h3&gt;
&lt;p&gt;Here is an example of what this condition looks like.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl get pods -n keycloak -l component=mysqlrouter
NAME                            READY   STATUS             RESTARTS   AGE
mysql-router-757595f6c5-pdgxj   1/2     CrashLoopBackOff   0          109m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can repair this issue by deleting the pod that is in the &lt;code&gt;CrashLoopBackOff&lt;/code&gt; state.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl delete pod -n keycloak mysql-router-757595f6c5-pdgxj
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;innodbcluster-object-stuck-terminating&#34;&gt;InnoDBCluster object stuck &lt;code&gt;Terminating&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;This condition has been observed to occur on an uninstallation of Verrazzano.&lt;/p&gt;
&lt;p&gt;Here is an example of what this condition looks like.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl get InnoDBCluster -n keycloak
NAME    STATUS    ONLINE   INSTANCES   ROUTERS   AGE
mysql   OFFLINE   0        1           1         7m51s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can repair this issue by restarting the &lt;code&gt;mysql-operator&lt;/code&gt; pod.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl delete pod -l name=mysql-operator -n mysql-operator
&lt;/code&gt;&lt;/pre&gt;
      </description>
    </item>
    
    <item>
      <title>Docs: Prometheus</title>
      <link>/docs/troubleshooting/troubleshooting-prometheus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/troubleshooting/troubleshooting-prometheus/</guid>
      <description>
        
        
        &lt;h3 id=&#34;kubernetes-cluster-monitors-are-in-a-down-state&#34;&gt;Kubernetes cluster monitors are in a &lt;code&gt;DOWN&lt;/code&gt; state&lt;/h3&gt;
&lt;p&gt;When viewing targets in the Prometheus console, some Kubernetes cluster monitors may be down (&lt;code&gt;kube-etcd&lt;/code&gt;, &lt;code&gt;kube-proxy&lt;/code&gt;, and such). This is likely caused by the configuration of the Kubernetes cluster
itself. Depending on the type of cluster, certain metrics may be disabled by default. Enabling metrics is cluster dependent; for details, refer to the documentation for your cluster type.&lt;/p&gt;
&lt;p&gt;For example, to enable &lt;code&gt;kube-proxy&lt;/code&gt; metrics on Kind clusters, edit the &lt;code&gt;kube-proxy&lt;/code&gt; ConfigMap.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl edit cm/kube-proxy -n kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Replace the &lt;code&gt;metricsBindAddress&lt;/code&gt; value with the following and save the ConfigMap.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;metricsBindAddress: 0.0.0.0:10249
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then, restart the &lt;code&gt;kube-proxy&lt;/code&gt; pods.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl delete pod -l k8s-app=kube-proxy -n kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;For more information, see this GitHub &lt;a href=&#34;https://github.com/prometheus-community/helm-charts/issues/204&#34;&gt;issue&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;metrics-trait-service-monitor-not-discovered&#34;&gt;Metrics Trait Service Monitor not discovered&lt;/h3&gt;
&lt;p&gt;Metrics Traits use Service Monitors which require a Service to collect metrics.
If your OAM workload is created with a Metrics Trait and no Ingress Trait, a Service might not be generated for your workload and will need to be created manually.&lt;/p&gt;
&lt;p&gt;This troubleshooting example uses the &lt;code&gt;hello-helidon&lt;/code&gt; application.&lt;/p&gt;
&lt;p&gt;Verify a Service Monitor exists for your application workload.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl get servicemonitors -n hello-helidon
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Verify a Service exists for your application workload.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ kubectl get services -n hello-helidon
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If no Service exists, create one manually.
This example uses the default Prometheus port.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: hello-helidon-service
  namespace: hello-helidon
spec:
  selector:
    app: hello-helidon
  ports:
    - name: tcp-hello-helidon
      port: 8080
      protocol: TCP
      targetPort: 8080
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After you&amp;rsquo;ve completed these steps, you can &lt;a href=&#34;../../docs/monitoring/metrics/metrics/#verify-metrics-collection&#34;&gt;verify metrics collection&lt;/a&gt; has succeeded.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
